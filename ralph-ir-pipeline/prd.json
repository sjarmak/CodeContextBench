{
  "project": "IR Enterprise MCP Value Pipeline",
  "branchName": "ralph/ir-enterprise-pipeline",
  "description": "Analysis pipeline improvements for demonstrating MCP value in enterprise codebases: trajectory synthesis, retrieval-outcome correlation, composite MCP value score, cost-efficiency metrics, and stakeholder-ready report output.",
  "userStories": [
    {
      "id": "US-001",
      "title": "Synthesize trajectory.json from claude-code.txt",
      "description": "As an analyst, I want trajectory.json to be generated from claude-code.txt when Harbor fails to produce it, so that TTFR/TTAR metrics have complete coverage.",
      "acceptanceCriteria": [
        "New function synthesize_trajectory(transcript_path: Path) -> dict exists in scripts/ccb_metrics/ir_metrics.py",
        "Function parses claude-code.txt JSONL and extracts tool_use/tool_result blocks with tool_use_ids",
        "Timestamps estimated in real seconds using calibration from runs that have both trajectory.json and claude-code.txt. Calibration: compute median tokens-per-second from paired runs, apply to synthesized runs. Also track step count.",
        "extract_time_to_context() in ir_metrics.py falls back to synthesize_trajectory() when trajectory_path does not exist or is empty",
        "python3 scripts/ir_analysis.py runs without error and 'Skipped (no transcript)' count drops compared to before (ideally to 0)"
      ],
      "priority": 1,
      "passes": true,
      "notes": "~85% of runs have real trajectory.json. ~15% missing due to H3 subagent bug (fixed now, but older runs affected). claude-code.txt is always present. Calibrate token-to-seconds rate from the 85% that have both files."
    },
    {
      "id": "US-002",
      "title": "Add trajectory.json post-run warning to _common.sh",
      "description": "As a benchmark operator, I want the run scripts to warn if trajectory.json is missing after a task completes, so that coverage gaps are caught immediately.",
      "acceptanceCriteria": [
        "configs/_common.sh has a post-run check function that logs WARNING if trajectory.json is missing from the task output directory",
        "The warning is non-blocking (does not fail the run pipeline)",
        "The check is called after each task completes in run_tasks_parallel or run_paired_configs",
        "AGENTS.md documents trajectory.json generation: what produces it, why it might be missing, how to troubleshoot",
        "bash -n configs/_common.sh passes (syntax check)"
      ],
      "priority": 2,
      "passes": true,
      "notes": "trajectory.json is auto-generated by Harbor's ClaudeCode._convert_events_to_trajectory(). The H3 bug (subagent dirs confusing _get_session_dir()) is already fixed in claude_baseline_agent.py. This story adds a runtime check so operators notice if it fails again."
    },
    {
      "id": "US-003",
      "title": "Retrieval-to-outcome correlation analysis",
      "description": "As a stakeholder, I want to see that better retrieval quality leads to better task outcomes, not just better search metrics in isolation.",
      "acceptanceCriteria": [
        "New function compute_retrieval_outcome_correlation() in scripts/ir_analysis.py",
        "Function loads MANIFEST.json (at runs/official/MANIFEST.json, resolving through symlink) and joins rewards with IR scores by (task_id, config)",
        "Computes Spearman rank correlation between MRR and reward across all paired tasks",
        "Computes per-task scatter data: (task_id, suite, mrr_bl, mrr_sg, reward_bl, reward_sg, mrr_delta, reward_delta)",
        "Table output includes RETRIEVAL-OUTCOME CORRELATION section with: n_paired, Spearman r, p-value, interpretation text",
        "python3 scripts/ir_analysis.py runs and shows the new section with numeric results"
      ],
      "priority": 3,
      "passes": false,
      "notes": "Use scipy.stats.spearmanr for correlation. MANIFEST.json has rewards per (task_id, config). IR scores already have task_id and config_name. Join key is task_id match (IR uses task_id from ground truth, MANIFEST uses task_name from result.json — these should match). runs/official/ is a symlink to /home/stephanie_jarmak/evals/custom_agents/agents/claudecode/runs/official."
    },
    {
      "id": "US-004",
      "title": "Composite MCP value score with z-score normalization",
      "description": "As a stakeholder, I want a single composite score per task that combines retrieval quality, task outcome, and efficiency into one 'MCP value' number, z-scored across benchmarks.",
      "acceptanceCriteria": [
        "New dataclass MCPValueScore in scripts/ccb_metrics/ir_metrics.py with fields: task_id, suite, retrieval_lift, outcome_lift, efficiency_lift, cost_ratio, composite",
        "Components are z-score normalized across benchmarks before combining: (value - mean) / std for each component",
        "composite = configurable weighted sum of z-scored components. Default weights: retrieval_lift=0.3, outcome_lift=0.3, efficiency_lift=0.25, cost_ratio=0.15",
        "New function compute_mcp_value_scores() that joins IR scores, MANIFEST rewards, and token data (from task_metrics.json)",
        "Table output includes MCP VALUE SCORE section: per-suite mean composite, top 10 MCP-helped tasks, top 10 MCP-hurt tasks",
        "Weights are configurable via CLI args: --value-weights 0.3,0.3,0.25,0.15",
        "python3 scripts/ir_analysis.py runs and shows the new section"
      ],
      "priority": 4,
      "passes": false,
      "notes": "Depends on US-003 for reward data join. Z-scoring: compute mean/std of each component across ALL tasks, then normalize. Tasks missing components (e.g., no TTFR) get NaN for that component and composite uses available components only (re-weight). Token data in task_metrics.json at runs/official/{run_dir}/{config}/{batch}/{task}/task_metrics.json."
    },
    {
      "id": "US-005",
      "title": "Cost-efficiency metrics",
      "description": "As a stakeholder, I want to understand the ROI of MCP: how many tokens/dollars does it cost per relevant file found.",
      "acceptanceCriteria": [
        "New metrics computed in ir_analysis.py: tokens_per_relevant_file (total input tokens / n_overlap) and tokens_before_first_relevant (cumulative tokens up to TTFR step)",
        "Reads task_metrics.json for token counts (input_tokens field from agent task time, not wall clock)",
        "Aggregates by config and suite in table output: COST EFFICIENCY section",
        "Shows delta and percentage change columns (baseline vs SG_full) for each metric",
        "python3 scripts/ir_analysis.py runs and shows the new section with numeric results"
      ],
      "priority": 5,
      "passes": false,
      "notes": "task_metrics.json contains input_tokens, output_tokens, cache_creation_tokens, cache_read_tokens. Use input_tokens for cost-per-file. For tokens_before_first_relevant, need to sum tokens from trajectory steps 0..ttfr_step. If synthesized trajectory, estimate from claude-code.txt token counts per message block."
    },
    {
      "id": "US-008",
      "title": "Stakeholder-ready markdown report output",
      "description": "As a stakeholder, I want the IR analysis to produce a clean, formatted markdown report suitable for sharing in enterprise contexts.",
      "acceptanceCriteria": [
        "New --report flag on scripts/ir_analysis.py",
        "Generates structured markdown with sections: Executive Summary, Retrieval Quality, Time-to-Context, Cost Efficiency, MCP Value Rankings, Statistical Methodology",
        "Executive Summary has 3-4 bullet points with key findings and significance levels",
        "Each section includes: metric table (markdown), interpretation paragraph, per-suite breakdown",
        "Uses enterprise-friendly labels: baseline='IDE-native', sourcegraph_full='Context infrastructure'",
        "Report written to docs/ir_analysis_report.md",
        "--report flag works with --suite filter",
        "python3 scripts/ir_analysis.py --report produces a clean markdown file at docs/ir_analysis_report.md"
      ],
      "priority": 6,
      "passes": false,
      "notes": "Depends on US-003, US-004, US-005 for the data sections. The TIER_LABELS mapping already exists in compare_configs.py (baseline='IDE-native', sourcegraph_full='Context infra'). Reuse or import. format_table() in ir_analysis.py is the existing ASCII formatter — add a parallel format_report_markdown() function."
    }
  ]
}
