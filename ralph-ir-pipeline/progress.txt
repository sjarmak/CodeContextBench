## Codebase Patterns

### IR Metrics Module (scripts/ccb_metrics/ir_metrics.py)
- IRScores dataclass holds per-task metrics: mrr, map_score, file_recall, context_efficiency, ttfr, tt_all_r, n_steps_to_first, plus @k dicts (precision, recall, f1, ndcg)
- compute_ir_scores() takes (retrieved, ground_truth_files, task_id, config_name) → IRScores
- aggregate_ir_scores() takes list[IRScores] → dict with mean/std/median/n for each metric
- extract_time_to_context() joins trajectory.json timestamps with claude-code.txt file paths via tool_use_id
- _normalize() strips leading slashes, /workspace/ prefix for path comparison
- Trajectory regex: _TOOL_EXEC_RE = re.compile(r"Executed (\S+) (toolu_\S+)")

### IR Analysis Script (scripts/ir_analysis.py)
- run_ir_analysis() is the main orchestrator: loads ground truth, scans run dirs, computes scores, runs stats
- format_table() generates ASCII output; add format_report_markdown() for --report
- SKIP_PATTERNS = {"archive", "__broken_verifier", "validation_test", "preamble_test_"}
- DROPPED_BENCHMARKS = {"ccb_dependeval", "ccb_locobench"}
- _infer_suite() maps task_id patterns to suite names (15+ patterns)
- Statistical tests use ccb_metrics.statistics: welchs_t_test, cohens_d, bootstrap_ci

### Ground Truth (scripts/ccb_metrics/ground_truth.py)
- Per-benchmark extraction functions: _gt_swebenchpro, _gt_pytorch, _gt_k8s_docs, etc.
- TaskGroundTruth dataclass: task_id, benchmark, files, source, confidence
- K8s Docs and TAC use hardcoded dicts (manual ground truth)
- Cache at configs/ground_truth_files.json (rebuild with --build-ground-truth)

### Run Directory Structure
- runs/official/ is a symlink to /home/stephanie_jarmak/evals/custom_agents/agents/claudecode/runs/official
- Layout: runs/official/{run_dir}/{config}/{batch_timestamp}/{task__hash}/
- Files per task: result.json, agent/claude-code.txt, agent/trajectory.json, task_metrics.json
- MANIFEST.json at runs/official/MANIFEST.json — keyed by (run_name, task_name, config)

### Existing Statistics Module (scripts/ccb_metrics/statistics.py)
- welchs_t_test(a, b) → dict with t_stat, p_value, df, is_significant (alpha=0.05)
- cohens_d(a, b) → dict with d, magnitude, ci_lower, ci_upper
- bootstrap_ci(deltas) → dict with estimate, ci_lower, ci_upper
- mcnemar_test(pairs) → dict for pass/fail contingency

### Token Data
- task_metrics.json has: input_tokens, output_tokens, cache_creation_tokens, cache_read_tokens, agent_task_time_seconds
- Extracted by scripts/reextract_all_metrics.py from claude-code.txt JSONL
- Cost model: input at $15/M, output at $75/M tokens

### Config Labels (from compare_configs.py)
- TIER_LABELS = {"baseline": "IDE-native", "sourcegraph_base": "SG_base", "sourcegraph_full": "Context infra"}

## Progress

