diff --git a/test/inductor/test_pattern_matcher.py b/test/inductor/test_pattern_matcher.py
index 29c888e8bc383..a4a96637524f9 100644
--- a/test/inductor/test_pattern_matcher.py
+++ b/test/inductor/test_pattern_matcher.py
@@ -40,7 +40,6 @@
     instantiate_parametrized_tests,
     IS_LINUX,
     parametrize,
-    skipIfRocm,
 )
 from torch.testing._internal.inductor_utils import GPU_TYPE, HAS_GPU, IS_BIG_GPU
 from torch.testing._internal.logging_utils import LoggingTestCase, make_logging_test
@@ -1216,70 +1215,6 @@ def fn2(inp, a, b):
         _, (code) = run_and_get_code(fn2, args[0], args[1], args[2])
         FileCheck().check_not("extern_kernels.addmm(").run(code[0])
 
-    @skipIfRocm
-    def test_addmm_activation_fusion(self):
-        """
-        Test whether Activation(Addmm) implies _addmm_activation
-        """
-
-        b = torch.rand(4, device=GPU_TYPE)
-        m1 = torch.rand(3, 2, device=GPU_TYPE)
-        m2 = torch.rand(2, 4, device=GPU_TYPE)
-        alphas = ({"alpha": 0.8}, {})  # **{} -> alpha=1
-        betas = ({"beta": 1}, {})  # **{} -> beta=1
-
-        # Cases Activation(Addmm) -> _addmm_activation
-        fusable_activations = (
-            torch.nn.functional.relu,
-            # NOTE: only approximate="tanh" is fusable
-            lambda *args, **kwargs: torch.nn.functional.gelu(
-                *args, approximate="tanh", **kwargs
-            ),
-        )
-        for activation in fusable_activations:
-
-            def f(b, m1, m2, beta, alpha):
-                return activation(torch.addmm(b, m1, m2, **beta, **alpha))
-
-            fc = torch.compile(f)
-
-            for beta, alpha in itertools.product(betas, alphas):
-                expected = f(b, m1, m2, beta, alpha)
-                actual = fc(b, m1, m2, beta, alpha)
-                torch.testing.assert_close(expected, actual)
-
-                _, (code) = run_and_get_code(fc, b, m1, m2, beta, alpha)
-                self.assertIn("_addmm_activation", code[0])
-
-            # Check no disruptions in the gemm autotune process
-            _, (code) = run_and_get_code(
-                torch.compile(f, options={"max_autotune_gemm": True}),
-                b,
-                m1,
-                m2,
-                beta,
-                alpha,
-            )
-            self.assertNotIn("_addmm_activation", code[0])
-
-        # Cases Activation(Addmm) -> Activation(Addmm)
-        non_fusable_activations = (
-            torch.nn.functional.gelu,  # implies approximate="none"
-            lambda *args, **kwargs: torch.nn.functional.gelu(
-                *args, approximate="none", **kwargs
-            ),
-        )
-        for activation in non_fusable_activations:
-
-            def f(b, m1, m2, beta, alpha):
-                return activation(torch.addmm(b, m1, m2, **beta, **alpha))
-
-            fc = torch.compile(f)
-
-            for beta, alpha in itertools.product(betas, alphas):
-                _, (code) = run_and_get_code(fc, b, m1, m2, beta, alpha)
-                self.assertNotIn("_addmm_activation", code[0])
-
     def test_addmm_alpha_beta_with_pointwise(self):
         # Test that addmm with alpha/beta != 1 is unfused correctly with pointwise ops
         # See https://github.com/pytorch/pytorch/issues/167313
@@ -1288,7 +1223,7 @@ def test_addmm_alpha_beta_with_pointwise(self):
         b = torch.rand(3, 2, device=GPU_TYPE)
 
         def f(x, a, b):
-            return torch.abs(torch.addmm(x, a, b, alpha=0.8, beta=0.2))
+            return torch.nn.functional.relu(torch.addmm(x, a, b, alpha=0.8, beta=0.2))
 
         fc = torch.compile(f)
 
@@ -1305,7 +1240,7 @@ def f(x, a, b):
 
         # Test with alpha=1, beta=1 (default) - should also unfuse
         def f_default(x, a, b):
-            return torch.abs(torch.addmm(x, a, b))
+            return torch.nn.functional.relu(torch.addmm(x, a, b))
 
         fc_default = torch.compile(f_default)
         expected_default = f_default(x, a, b)
diff --git a/torch/_inductor/fx_passes/post_grad.py b/torch/_inductor/fx_passes/post_grad.py
index f084cf91ea0e8..4a350b81bbecb 100644
--- a/torch/_inductor/fx_passes/post_grad.py
+++ b/torch/_inductor/fx_passes/post_grad.py
@@ -34,7 +34,6 @@
     CallFunctionVarArgs,
     filter_nodes,
     fwd_only,
-    gen_register_replacement,
     get_arg_value,
     get_mutation_region_id,
     Ignored,
@@ -682,66 +681,6 @@ def body_fn(*flat_args):
         raise AssertionError("scan is not lowered to while_loop")
 
 
-@functools.cache
-def register_addmm_activation_fusions():
-    def is_valid_addmm_activation_fusion(match: Match) -> bool:
-        # Exclude ROCm
-        if torch.version.hip:
-            return False
-
-        if config.max_autotune or config.max_autotune_gemm:
-            return False
-
-        inp = match.kwargs["inp"].meta["val"]
-
-        if not inp.is_cuda:
-            return False
-
-        output = match.output_node()
-        return not all(
-            is_pointwise_use(use, lambda target: torch.Tag.reduction in target.tags)
-            for use in output.users
-        )
-
-    args = [torch.empty(3), torch.empty(4, 2), torch.empty(2, 3)]
-    beta_alpha_workaround = {"beta": 1.3, "alpha": 1.2}
-
-    def addmm_relu_pattern(inp, m1, m2, beta, alpha):
-        return aten.relu(aten.addmm(inp, m1, m2, beta=beta, alpha=alpha))
-
-    def addmm_gelu_pattern(inp, m1, m2, beta, alpha):
-        return aten.gelu(
-            aten.addmm(inp, m1, m2, beta=beta, alpha=alpha), approximate="tanh"
-        )
-
-    def addmm_relu_replacement(inp, m1, m2, beta, alpha):
-        return aten._addmm_activation(inp, m1, m2, beta=beta, alpha=alpha)
-
-    def addmm_gelu_replacement(inp, m1, m2, beta, alpha):
-        return aten._addmm_activation(
-            inp, m1, m2, beta=beta, alpha=alpha, use_gelu=True
-        )
-
-    patterns = (addmm_relu_pattern, addmm_gelu_pattern)
-    replacements = (addmm_relu_replacement, addmm_gelu_replacement)
-    for pattern, replacement in zip(patterns, replacements):
-        key = f"{pattern.__name__}"
-        gen_register_replacement(
-            key,
-            # pyrefly: ignore [bad-argument-type]
-            pattern,
-            # pyrefly: ignore [bad-argument-type]
-            replacement,
-            args,
-            # pyrefly: ignore [bad-argument-type]
-            trace_fn=fwd_only,
-            # pyrefly: ignore [bad-argument-type]
-            pass_dicts=pass_patterns[1],
-            extra_check=is_valid_addmm_activation_fusion,
-            scalar_workaround=beta_alpha_workaround,
-        )
-
-
 @init_once_fakemode
 def lazy_init():
     if torch._C._has_mkldnn:
@@ -767,8 +706,6 @@ def lazy_init():
         extra_check=prepare_softmax_extra_check,
     )
 
-    register_addmm_activation_fusions()
-
 
 def reorder_for_locality(graph: torch.fx.Graph):
     if torch.distributed.is_available():
diff --git a/torch/_inductor/fx_passes/serialized_patterns/addmm_gelu_pattern.py b/torch/_inductor/fx_passes/serialized_patterns/addmm_gelu_pattern.py
deleted file mode 100644
index f991015b4de69..0000000000000
--- a/torch/_inductor/fx_passes/serialized_patterns/addmm_gelu_pattern.py
+++ /dev/null
@@ -1,43 +0,0 @@
-# mypy: ignore-errors
-
-# noqa: F401, E501
-# This is an auto-generated file. Please do not modify it by hand.
-# To re-generate, run:
-# cd ~/pytorch && python torchgen/fuse/gen_patterns.py
-
-import torch
-import torch._inductor
-import operator
-
-aten = torch.ops.aten
-prims = torch.ops.prims
-
-from torch._inductor.pattern_matcher import (
-   Arg,
-   CallFunction,
-   CallFunctionVarArgs,
-   CallMethod,
-   CallMethodVarArgs,
-   CallModule,
-   CallModuleVarArgs,
-   ExclusiveKeywordArg,
-   Ignored,
-   KeywordArg,
-   ListOf,
-   MultiOutputPattern,
-   PatternExpr,
-   RepeatedExpr,
-   _TargetArgsExpr,
-   _TargetExpr,
-   _TargetExprVarArgs,
-)
-addmm_default = CallFunction(aten.addmm.default, KeywordArg('inp'), KeywordArg('m1'), KeywordArg('m2'), beta=KeywordArg('beta'), alpha=KeywordArg('alpha'), _users=4)
-mul_Tensor = CallFunction(aten.mul.Tensor, addmm_default, Ignored())
-mul_Tensor_1 = CallFunction(aten.mul.Tensor, addmm_default, addmm_default)
-mul_Tensor_2 = CallFunction(aten.mul.Tensor, mul_Tensor_1, addmm_default)
-mul_Tensor_3 = CallFunction(aten.mul.Tensor, mul_Tensor_2, Ignored())
-add_Tensor = CallFunction(aten.add.Tensor, addmm_default, mul_Tensor_3)
-mul_Tensor_4 = CallFunction(aten.mul.Tensor, add_Tensor, Ignored())
-tanh_default = CallFunction(aten.tanh.default, mul_Tensor_4)
-add_Tensor_1 = CallFunction(aten.add.Tensor, tanh_default, Ignored())
-addmm_gelu_pattern = CallFunction(aten.mul.Tensor, mul_Tensor, add_Tensor_1, _users=0)
diff --git a/torch/_inductor/fx_passes/serialized_patterns/addmm_relu_pattern.py b/torch/_inductor/fx_passes/serialized_patterns/addmm_relu_pattern.py
deleted file mode 100644
index e9729a7787131..0000000000000
--- a/torch/_inductor/fx_passes/serialized_patterns/addmm_relu_pattern.py
+++ /dev/null
@@ -1,35 +0,0 @@
-# mypy: ignore-errors
-
-# noqa: F401, E501
-# This is an auto-generated file. Please do not modify it by hand.
-# To re-generate, run:
-# cd ~/pytorch && python torchgen/fuse/gen_patterns.py
-
-import torch
-import torch._inductor
-import operator
-
-aten = torch.ops.aten
-prims = torch.ops.prims
-
-from torch._inductor.pattern_matcher import (
-   Arg,
-   CallFunction,
-   CallFunctionVarArgs,
-   CallMethod,
-   CallMethodVarArgs,
-   CallModule,
-   CallModuleVarArgs,
-   ExclusiveKeywordArg,
-   Ignored,
-   KeywordArg,
-   ListOf,
-   MultiOutputPattern,
-   PatternExpr,
-   RepeatedExpr,
-   _TargetArgsExpr,
-   _TargetExpr,
-   _TargetExprVarArgs,
-)
-addmm_default = CallFunction(aten.addmm.default, KeywordArg('inp'), KeywordArg('m1'), KeywordArg('m2'), beta=KeywordArg('beta'), alpha=KeywordArg('alpha'))
-addmm_relu_pattern = CallFunction(aten.relu.default, addmm_default, _users=0)
diff --git a/torchgen/fuse/gen_patterns.py b/torchgen/fuse/gen_patterns.py
index b4bdf022202ba..0861c882e3fff 100644
--- a/torchgen/fuse/gen_patterns.py
+++ b/torchgen/fuse/gen_patterns.py
@@ -2,7 +2,7 @@
 import os
 
 from torch._inductor import pattern_matcher
-from torch._inductor.fx_passes import joint_graph, post_grad
+from torch._inductor.fx_passes import joint_graph
 
 
 if __name__ == "__main__":
@@ -17,4 +17,3 @@
     # to serialize the patterns as it goes.
     os.environ["PYTORCH_GEN_PATTERNS"] = "1"
     joint_graph.lazy_init()
-    post_grad.lazy_init()
