diff --git a/.ci/docker/common/install_inductor_benchmark_deps.sh b/.ci/docker/common/install_inductor_benchmark_deps.sh
index 8b2a3f3ac96c6..6ed81aea66c51 100644
--- a/.ci/docker/common/install_inductor_benchmark_deps.sh
+++ b/.ci/docker/common/install_inductor_benchmark_deps.sh
@@ -55,3 +55,8 @@ install_timm
 
 # Clean up
 conda_run pip uninstall -y torch torchvision torchaudio triton torchao
+if [[ "${DESIRED_CUDA}" == 13.* ]]; then
+  conda_run pip uninstall -y nvidia-nccl-cu13
+else
+  conda_run pip uninstall -y nvidia-nccl-cu12
+fi
diff --git a/.ci/manywheel/build_common.sh b/.ci/manywheel/build_common.sh
index 29dbc3822ed5c..d50bd623dace0 100644
--- a/.ci/manywheel/build_common.sh
+++ b/.ci/manywheel/build_common.sh
@@ -417,6 +417,11 @@ for pkg in /$WHEELHOUSE_DIR/torch_no_python*.whl /$WHEELHOUSE_DIR/torch*linux*.w
         zip -rq $pkg_name $PREIX*
         rm -f $pkg
         mv $pkg_name $(dirname $pkg)/$pkg_name
+    elif [[ $PLATFORM == "manylinux_2_28_aarch64" ]]; then
+        pkg_name=$(echo $(basename $pkg) | sed -e s#linux_aarch64#"${PLATFORM}"#)
+        zip -rq $pkg_name $PREIX*
+        rm -f $pkg
+        mv $pkg_name $(dirname $pkg)/$pkg_name
     else
         # zip up the wheel back
         zip -rq $(basename $pkg) $PREIX*
diff --git a/Dockerfile b/Dockerfile
index 099c0f82efe84..b524752f3541c 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -19,26 +19,26 @@ RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-ins
         libpng-dev \
         python3 \
         python3-pip \
-        python3-venv \
+        python-is-python3 \
         python3-dev && \
     rm -rf /var/lib/apt/lists/*
+# Remove PEP 668 restriction (safe in containers)
+RUN rm -f /usr/lib/python*/EXTERNALLY-MANAGED
 RUN /usr/sbin/update-ccache-symlinks
 RUN mkdir /opt/ccache && ccache --set-config=cache_dir=/opt/ccache
-ENV PATH /opt/pytorch-venv/bin:$PATH
 
-FROM dev-base as python-venv
+FROM dev-base as python-deps
 COPY requirements.txt requirements-build.txt .
-# Create virtual environment and install packages
-RUN python3 -m venv /opt/pytorch-venv && \
-    /opt/pytorch-venv/bin/pip install --upgrade pip setuptools wheel && \
-    /opt/pytorch-venv/bin/pip install cmake pyyaml numpy ipython -r requirements.txt
+# Install Python packages to system Python
+RUN pip3 install --upgrade --ignore-installed pip setuptools wheel && \
+    pip3 install cmake pyyaml numpy ipython -r requirements.txt
 
 FROM dev-base as submodule-update
 WORKDIR /opt/pytorch
 COPY . .
 RUN git submodule update --init --recursive
 
-FROM python-venv as pytorch-installs
+FROM python-deps as pytorch-installs
 ARG CUDA_PATH=cu121
 ARG INSTALL_CHANNEL=whl/nightly
 # Automatically set by buildx
@@ -46,11 +46,11 @@ ARG TARGETPLATFORM
 
 # INSTALL_CHANNEL whl - release, whl/nightly - nightly, whl/test - test channels
 RUN case ${TARGETPLATFORM} in \
-         "linux/arm64")  /opt/pytorch-venv/bin/pip install --extra-index-url https://download.pytorch.org/whl/cpu/ torch torchvision torchaudio ;; \
-         *)              /opt/pytorch-venv/bin/pip install --index-url https://download.pytorch.org/${INSTALL_CHANNEL}/${CUDA_PATH#.}/ torch torchvision torchaudio ;; \
+         "linux/arm64")  pip3 install --extra-index-url https://download.pytorch.org/whl/cpu/ torch torchvision torchaudio ;; \
+         *)              pip3 install --index-url https://download.pytorch.org/${INSTALL_CHANNEL}/${CUDA_PATH#.}/ torch torchvision torchaudio ;; \
     esac
-RUN /opt/pytorch-venv/bin/pip install torchelastic
-RUN IS_CUDA=$(python -c 'import torch ; print(torch.cuda._is_compiled())'); \
+RUN pip3 install torchelastic
+RUN IS_CUDA=$(python3 -c 'import torch ; print(torch.cuda._is_compiled())'); \
     echo "Is torch compiled with cuda: ${IS_CUDA}"; \
     if test "${IS_CUDA}" != "True" -a ! -z "${CUDA_VERSION}"; then \
         exit 1; \
@@ -66,13 +66,17 @@ RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-ins
         ca-certificates \
         libjpeg-dev \
         libpng-dev \
+        python-is-python3 \
+        python3 \
+        python3-pip \
         && rm -rf /var/lib/apt/lists/*
-COPY --from=pytorch-installs /opt/pytorch-venv /opt/pytorch-venv
+# Copy Python packages from pytorch-installs stage
+COPY --from=pytorch-installs /usr/local/lib/python3.12 /usr/local/lib/python3.12
+COPY --from=pytorch-installs /usr/local/bin /usr/local/bin
 RUN if test -n "${TRITON_VERSION}" -a "${TARGETPLATFORM}" != "linux/arm64"; then \
         DEBIAN_FRONTEND=noninteractive apt install -y --no-install-recommends gcc; \
         rm -rf /var/lib/apt/lists/*; \
     fi
-ENV PATH /opt/pytorch-venv/bin:$PATH
 ENV NVIDIA_VISIBLE_DEVICES all
 ENV NVIDIA_DRIVER_CAPABILITIES compute,utility
 ENV LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64
@@ -82,5 +86,6 @@ WORKDIR /workspace
 
 FROM official as dev
 # Should override the already installed version from the official-image stage
-COPY --from=python-venv /opt/pytorch-venv /opt/pytorch-venv
+COPY --from=python-deps /usr/local/lib/python3.12 /usr/local/lib/python3.12
+COPY --from=python-deps /usr/local/bin /usr/local/bin
 COPY --from=submodule-update /opt/pytorch /opt/pytorch
diff --git a/test/inductor/test_triton_heuristics.py b/test/inductor/test_triton_heuristics.py
index c5a5b96e896a0..d607b33df5901 100644
--- a/test/inductor/test_triton_heuristics.py
+++ b/test/inductor/test_triton_heuristics.py
@@ -15,7 +15,6 @@
     IS_LINUX,
     parametrize,
     runOnRocm,
-    skipIfRocm,
     skipIfXpu,
 )
 from torch.testing._internal.inductor_utils import (
@@ -273,13 +272,13 @@ def fn(x):
         self.assertEqual(ref, res)
 
     @skipIfXpu(msg="https://github.com/intel/torch-xpu-ops/issues/2331")
-    @skipIfRocm
     @skipUnless(HAS_GPU_AND_TRITON, "requires gpu and triton")
     @parametrize("do_pruning", [False, True])
     def test_prune_configs_over_shared_memory_limit(self, do_pruning):
         from torch._inductor.template_heuristics.triton import (
             CUDAConfigHeuristic,
             GemmConfig,
+            ROCmConfigHeuristic,
         )
 
         expected_count = 1 if do_pruning else 2
@@ -292,7 +291,10 @@ def test_prune_configs_over_shared_memory_limit(self, do_pruning):
         with config.patch(
             {"max_autotune_prune_choices_based_on_shared_mem": do_pruning}
         ):
-            config_heuristic = CUDAConfigHeuristic()
+            if torch.version.hip:
+                config_heuristic = ROCmConfigHeuristic()
+            else:
+                config_heuristic = CUDAConfigHeuristic()
             config_heuristic.should_scale_configs = False
             config_heuristic.mm_configs = mm_configs
             configs = list(
diff --git a/torch/_C/__init__.pyi.in b/torch/_C/__init__.pyi.in
index 231b827e548f6..3cfe372b8ea1c 100644
--- a/torch/_C/__init__.pyi.in
+++ b/torch/_C/__init__.pyi.in
@@ -2248,6 +2248,7 @@ class _CudaDeviceProperties:
     clock_rate: _int
     memory_clock_rate: _int
     memory_bus_width: _int
+    shared_memory_per_block: _int
 
 # Functions related to SDPA
 class _SDPAParams:
diff --git a/torch/_inductor/template_heuristics/triton.py b/torch/_inductor/template_heuristics/triton.py
index 1e9aa1cdba97f..21deda557346b 100644
--- a/torch/_inductor/template_heuristics/triton.py
+++ b/torch/_inductor/template_heuristics/triton.py
@@ -651,9 +651,13 @@ def _get_exceeding_shared_memory_checker(
         try:
             device = torch.cuda.current_device()
             props = torch.cuda.get_device_properties(device)
-            if not hasattr(props, "shared_memory_per_block_optin"):  # for NVidia GPUs
+            if hasattr(props, "shared_memory_per_block_optin"):  # for NVidia GPUs
+                sm_available = int(props.shared_memory_per_block_optin)
+            elif hasattr(props, "shared_memory_per_block"):  # for ROCm
+                sm_available = int(props.shared_memory_per_block)
+            else:
                 return None
-            sm_available = int(props.shared_memory_per_block_optin)
+
         except Exception:
             # If CUDA is not available or properties cannot be queried, return None
             return None
diff --git a/torch/csrc/cuda/Module.cpp b/torch/csrc/cuda/Module.cpp
index ec7e5be7eefe7..6a8bbe9904e5f 100644
--- a/torch/csrc/cuda/Module.cpp
+++ b/torch/csrc/cuda/Module.cpp
@@ -1093,6 +1093,11 @@ static void registerCudaDeviceProperties(PyObject* module) {
           "shared_memory_per_multiprocessor",
           &cudaDeviceProp::sharedMemPerMultiprocessor)
 #endif
+#if USE_ROCM
+      // ROCm: expose shared_memory_per_block for shared memory based pruning
+      .def_readonly(
+          "shared_memory_per_block", &cudaDeviceProp::sharedMemPerBlock)
+#endif
 #if (defined(USE_ROCM) && ROCM_VERSION >= 60100) || !USE_ROCM
       .def_readonly(
           "regs_per_multiprocessor", &cudaDeviceProp::regsPerMultiprocessor)
diff --git a/torch/csrc/cuda/nccl.cpp b/torch/csrc/cuda/nccl.cpp
index ee80c8b13f19f..477c560afede0 100644
--- a/torch/csrc/cuda/nccl.cpp
+++ b/torch/csrc/cuda/nccl.cpp
@@ -837,6 +837,8 @@ void all2all_single_equal_split(
   // operations issued as a part of the collective (e.g. alltoall) vs those
   // inside traditional p2p operations.
   NCCL_CHECK(ncclAllToAll(sendbuff, recvbuff, count, type, comm, stream));
+#elif NCCL_VERSION_CODE >= NCCL_VERSION(2, 28, 0)
+  NCCL_CHECK(ncclAlltoAll(sendbuff, recvbuff, count, type, comm, stream));
 #else
   int numranks = 0;
   NCCL_CHECK(ncclCommCount(comm, &numranks));
