diff --git a/test/inductor/test_flex_attention.py b/test/inductor/test_flex_attention.py
index fcf0cc5171d8b..f9e7914d42147 100644
--- a/test/inductor/test_flex_attention.py
+++ b/test/inductor/test_flex_attention.py
@@ -5113,6 +5113,31 @@ def simple_score_mod(score, b, h, q_idx, kv_idx):
             fa._FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = original_flag
             fa._WARNINGS_SHOWN = original_warnings_shown
 
+    @supported_platform
+    def test_mask_mod_functools_partial(self, device):
+        """Test that functools.partial wrapped mask_mod works with flex_attention.
+
+        Regression test for https://github.com/pytorch/pytorch/issues/170489
+        """
+        B, H, S, D = 1, 8, 64, 64
+
+        def causal_mask_with_offset(b, h, q_idx, kv_idx, offset):
+            return (q_idx + offset) >= kv_idx
+
+        offset = 31
+        mask_mod = functools.partial(causal_mask_with_offset, offset=offset)
+
+        query = torch.randn(B, H, S, D, device=device, dtype=torch.float16)
+        key = torch.randn(B, H, S, D, device=device, dtype=torch.float16)
+        value = torch.randn(B, H, S, D, device=device, dtype=torch.float16)
+
+        block_mask = create_block_mask(mask_mod, B, H, S, S, device=device)
+
+        flex_compiled = torch.compile(flex_attention)
+        out = flex_compiled(query, key, value, block_mask=block_mask)
+
+        self.assertEqual(out.shape, (B, H, S, D))
+
 
 class TestBlockMask(InductorTestCase):
     def setUp(self):
diff --git a/torch/_dynamo/variables/higher_order_ops.py b/torch/_dynamo/variables/higher_order_ops.py
index 4e29b9b0e0a0e..253386a94eeee 100644
--- a/torch/_dynamo/variables/higher_order_ops.py
+++ b/torch/_dynamo/variables/higher_order_ops.py
@@ -3899,13 +3899,10 @@ def _call_function(
         score_mod_node, score_mod_lifted_args = self.create_wrapped_node(
             tx, query, score_mod, "score_mod"
         )
-        mask_fn = block_mask.items[-1]
-        if mask_fn.is_python_constant():
-            mask_callable = mask_fn.as_python_constant()
-            if mask_callable is None:
-                mask_callable = torch.nn.attention.flex_attention.noop_mask
+        mask_fn = block_mask.items[-1]  # type: ignore[attr-defined]
+        if mask_fn.is_python_constant() and mask_fn.as_python_constant() is None:
             mask_fn = UserFunctionVariable(
-                mask_callable,
+                torch.nn.attention.flex_attention.noop_mask,
                 source=mask_fn.source,
             )
         mask_fn_node, mask_fn_lifted_args = self.create_wrapped_node(
