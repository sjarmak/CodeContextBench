diff --git a/test/dynamo/test_ctx_manager.py b/test/dynamo/test_ctx_manager.py
index 5e188e76dc56e..74f4daa895907 100644
--- a/test/dynamo/test_ctx_manager.py
+++ b/test/dynamo/test_ctx_manager.py
@@ -1835,6 +1835,59 @@ def fn(x):
         self.assertEqual(ref, res)
         self.assertEqual(len(counters["graph_break"]), 1)
 
+    def test_311_resume_block_keyerror(self):
+        # https://github.com/pytorch/pytorch/issues/162313
+        flag = True
+
+        def fn(x):
+            x = x + 1
+            torch._dynamo.graph_break()
+            x = x + 2
+            if flag:
+                with torch.no_grad():
+                    torch._dynamo.graph_break()
+                x = x + 4
+            else:
+                with torch.no_grad():
+                    torch._dynamo.graph_break()
+                x = x + 8
+            return x + 16
+
+        inp = torch.ones(3)
+        opt_fn = torch.compile(fn, backend="eager")
+        self.assertEqual(fn(inp), opt_fn(inp))
+        flag = False
+        self.assertEqual(fn(inp), opt_fn(inp))
+
+    def test_311_resume_block_keyerror2(self):
+        # https://github.com/pytorch/pytorch/issues/166176
+        def fn(x):
+            torch._dynamo.graph_break()
+            with torch.no_grad():
+                with torch.no_grad():
+                    torch._dynamo.graph_break()
+            return x + 1
+
+        inp = torch.ones(3)
+        opt_fn = torch.compile(fn, backend="eager")
+        self.assertEqual(fn(inp), opt_fn(inp))
+
+    def test_store_attr_graph_break_key_error(self):
+        # STORE_ATTR on dummy should result in graph break
+        def dummy():
+            pass
+
+        def fn(x):
+            x = x + 2
+            with torch.no_grad():
+                dummy.attr1 = x
+            return x + 4
+
+        inp = torch.ones(3)
+        opt_fn = torch.compile(fn, backend="eager")
+        self.assertEqual(fn(inp), opt_fn(inp))
+        self.assertGreater(len(counters["graph_break"]), 0)
+
 
 class ContextlibContextManagerTests(torch._dynamo.test_case.TestCase):
     def setUp(self):
diff --git a/test/dynamo/test_repros.py b/test/dynamo/test_repros.py
index 5e23e818f8eb0..1470315c95b30 100644
--- a/test/dynamo/test_repros.py
+++ b/test/dynamo/test_repros.py
@@ -7150,48 +7150,6 @@ def fn(x):
                 0, sys.monitoring.events.PY_START, old_callback
             )
 
-    def test_312_local_cell_overlap(self):
-        keys = range(10)
-        allowed = [0, 1, 2, 3]
-
-        def fn(x):
-            x = x + 1
-            torch._dynamo.graph_break()
-            key = [key for key in keys if key in allowed]
-
-            def inner():
-                nonlocal key
-
-            return x + key[0]
-
-        self.assertEqual(
-            fn(torch.ones(3)), torch.compile(fn, backend="eager")(torch.ones(3))
-        )
-
-    def test_311_resume_block_keyerror(self):
-        # https://github.com/pytorch/pytorch/issues/162313
-        flag = True
-
-        def fn(x):
-            x = x + 1
-            torch._dynamo.graph_break()
-            x = x + 2
-            if flag:
-                with torch.no_grad():
-                    torch._dynamo.graph_break()
-                x = x + 4
-            else:
-                with torch.no_grad():
-                    torch._dynamo.graph_break()
-                x = x + 8
-            return x + 16
-
-        inp = torch.ones(3)
-        opt_fn = torch.compile(fn, backend="eager")
-        self.assertEqual(fn(inp), opt_fn(inp))
-        flag = False
-        self.assertEqual(fn(inp), opt_fn(inp))
-
     def test_unbind_copy_out(self):
         def f(eye, out):
             torch.unbind_copy(eye, out=out)
diff --git a/torch/_dynamo/resume_execution.py b/torch/_dynamo/resume_execution.py
index 840e02a9cdb80..1041ba0ce2e38 100644
--- a/torch/_dynamo/resume_execution.py
+++ b/torch/_dynamo/resume_execution.py
@@ -250,8 +250,8 @@ class ResumeFunctionMetadata:
         default_factory=list
     )
     # per-offset map from new block target offsets to original block target offsets
-    block_target_offset_remap: dict[int, dict[int, int]] = dataclasses.field(
-        default_factory=dict
+    block_target_offset_remap: dict[tuple[int, int], dict[int, int]] = (
+        dataclasses.field(default_factory=dict)
     )
 
 
@@ -291,12 +291,14 @@ class ContinueExecutionCache:
     generated_code_metadata = ExactWeakKeyDictionary()
 
     @classmethod
-    def lookup(cls, code: types.CodeType, lineno: int, *key: Any) -> types.CodeType:
+    def lookup(
+        cls, code: types.CodeType, lineno: int, init_offset: int, *key: Any
+    ) -> types.CodeType:
         if code not in cls.cache:
             cls.cache[code] = {}
         key = tuple(key)
         if key not in cls.cache[code]:
-            cls.cache[code][key] = cls.generate(code, lineno, *key)
+            cls.cache[code][key] = cls.generate(code, lineno, init_offset, *key)
         return cls.cache[code][key]
 
     @classmethod
@@ -304,7 +306,8 @@ def generate(
         cls,
         code: types.CodeType,
         lineno: int,
-        offset: int,
+        init_offset: int,
+        resume_offset: int,
         setup_fn_target_offsets: tuple[int, ...],  # only used in Python 3.11+
         nstack: int,
         argnames: tuple[str, ...],
@@ -317,7 +320,7 @@ def generate(
         # which prevents excessive recompilation of inner frames
         nested_code_objs: tuple[types.CodeType],
     ) -> types.CodeType:
-        assert offset is not None
+        assert resume_offset is not None
         assert not (
             code.co_flags
             & (CO_GENERATOR | CO_COROUTINE | CO_ITERABLE_COROUTINE | CO_ASYNC_GENERATOR)
@@ -327,7 +330,8 @@ def generate(
             return cls.generate_based_on_original_code_object(
                 code,
                 lineno,
-                offset,
+                init_offset,
+                resume_offset,
                 setup_fn_target_offsets,
                 nstack,
                 argnames,
@@ -382,7 +386,7 @@ def update(
             code_options["co_flags"] = code_options["co_flags"] & ~(
                 CO_VARARGS | CO_VARKEYWORDS
             )
-            target = next(i for i in instructions if i.offset == offset)
+            target = next(i for i in instructions if i.offset == resume_offset)
 
             prefix = []
             if is_py311_plus:
@@ -575,7 +579,8 @@ def generate_based_on_original_code_object(
         cls,
         code: types.CodeType,
         lineno: int,
-        offset: int,
+        init_offset: int,
+        resume_offset: int,
         setup_fn_target_offsets: tuple[int, ...],
         *args: Any,
     ) -> types.CodeType:
@@ -590,34 +595,63 @@ def generate_based_on_original_code_object(
         meta: ResumeFunctionMetadata = ContinueExecutionCache.generated_code_metadata[
             code
         ]
-        new_offset = -1
 
-        def find_new_offset(
-            instructions: list[Instruction], code_options: dict[str, Any]
-        ) -> None:
-            nonlocal new_offset
-            (target,) = (i for i in instructions if i.offset == offset)
-            # match the functions starting at the last instruction as we have added a prefix
-            (new_target,) = (
-                i2
-                for i1, i2 in zip(reversed(instructions), reversed(meta.instructions))
-                if i1 is target
-            )
-            assert target.opcode == new_target.opcode
-            assert new_target.offset is not None
-            new_offset = new_target.offset
+        def find_orig_offset(cur_offset: int) -> int:
+            orig_offset = -1
+
+            def find_orig_offset_transform(
+                instructions: list[Instruction], code_options: dict[str, Any]
+            ) -> None:
+                nonlocal orig_offset
+                (target,) = (i for i in instructions if i.offset == cur_offset)
+                # match the functions starting at the last instruction as we have added a prefix
+                new_target_tuple = tuple(
+                    i2
+                    for i1, i2 in zip(
+                        reversed(instructions), reversed(meta.instructions)
+                    )
+                    if i1 is target
+                )
 
-        transform_code_object(code, find_new_offset)
-        assert new_offset >= 0
+                if not new_target_tuple:
+                    # Instruction with cur_offset in instructions was not found
+                    # in the original code - orig_offset left as -1.
+                    # Caller expected to handle this case.
+                    return
+
+                assert len(new_target_tuple) == 1
+                new_target = new_target_tuple[0]
+
+                assert target.opcode == new_target.opcode
+                assert new_target.offset is not None
+                orig_offset = new_target.offset
+
+            transform_code_object(code, find_orig_offset_transform)
+            return orig_offset
+
+        orig_init_offset = find_orig_offset(init_offset)
+        # It is fine if the initial instruction is not found in the original code;
+        # this means we graph broke in the prefix, which only happens with nested graph breaks.
+        # We should not be running into ambiguous graph break issues here.
+        orig_resume_offset = find_orig_offset(resume_offset)
+        assert orig_resume_offset > -1, (
+            "resume instruction not found in original code - this is a bug."
+        )
 
         if sys.version_info >= (3, 11):
             # setup_fn_target_offsets currently contains the target offset of
             # each setup_fn, based on `code`. When we codegen the resume function
             # based on the original code object, `meta.code`, the offsets in
             # setup_fn_target_offsets must be based on `meta.code` instead.
-            if new_offset not in meta.block_target_offset_remap:
+            offset_key = (orig_init_offset, orig_resume_offset)
+            # NOTE: we key by offset_key since the same resume function may graph
+            # break in multiple places and we need different block_target_offset_remap's
+            # for each graph break location. Keying by orig_resume_offset may not be enough
+            # if 2 graph breaks on different initial offsets resume on the same instruction
+            # (although this is rare and not tested anywhere).
+            if offset_key not in meta.block_target_offset_remap:
                 block_target_offset_remap = meta.block_target_offset_remap[
-                    new_offset
+                    offset_key
                 ] = {}
 
                 def remap_block_offsets(
@@ -625,11 +659,15 @@ def remap_block_offsets(
                 ) -> None:
                     # NOTE: each prefix block generates exactly one PUSH_EXC_INFO,
                     # so we can tell which block a prefix PUSH_EXC_INFO belongs to,
-                    # by counting. Then we can use meta.prefix_block-target_offset_remap
+                    # by counting. Then we can use meta.prefix_block_target_offset_remap
                     # to determine where in the original code the PUSH_EXC_INFO offset
                     # replaced.
                     prefix_blocks: list[Instruction] = []
                     for inst in instructions:
+                        # NOTE meta.prefix_block_target_offset_remap is based off of how we codegen'd
+                        # context managers at the prefix/prologue of the resume function. It is the same for
+                        # every graph break in the same resume function, so we do not need to recompute
+                        # for each graph break (unlike for meta.block_target_offset_remap)
                         if len(prefix_blocks) == len(
                             meta.prefix_block_target_offset_remap
                         ):
@@ -637,38 +675,49 @@ def remap_block_offsets(
                         if inst.opname == "PUSH_EXC_INFO":
                             prefix_blocks.append(inst)
 
-                    # offsets into prefix
+                    # remap block target offsets for blocks generated in the resume prefix
                     for inst, o in zip(
                         prefix_blocks, meta.prefix_block_target_offset_remap
                     ):
                         block_target_offset_remap[cast(int, inst.offset)] = o
 
-                    # old bytecode targets are after the prefix PUSH_EXC_INFO's
-                    old_start_offset = (
+                    # current bytecode targets are after the prefix PUSH_EXC_INFO's
+                    cur_start_offset = (
                         cast(int, prefix_blocks[-1].offset) if prefix_blocks else -1
                     )
-                    # offsets into old bytecode
-                    old_inst_offsets = sorted(
-                        n for n in setup_fn_target_offsets if n > old_start_offset
+                    # get the remaining block target offsets of the current bytecode
+                    cur_inst_offsets = sorted(
+                        n for n in setup_fn_target_offsets if n > cur_start_offset
                     )
                     targets = _filter_iter(
-                        instructions, old_inst_offsets, lambda inst, o: inst.offset == o
+                        instructions, cur_inst_offsets, lambda inst, o: inst.offset == o
                     )
-                    new_targets = _filter_iter(
-                        zip(reversed(instructions), reversed(meta.instructions)),
-                        targets,
-                        lambda v1, v2: v1[0] is v2,
+                    # The original code and resume code should have matching suffixes.
+                    # Match the post-prefix block target offsets of the current resume code
+                    # and the original code.
+                    orig_targets = reversed(
+                        _filter_iter(
+                            zip(reversed(instructions), reversed(meta.instructions)),
+                            reversed(targets),
+                            lambda v1, v2: v1[0] is v2,
+                        )
                     )
-                    for new, old in zip(new_targets, targets):
-                        block_target_offset_remap[old.offset] = new[1].offset
+                    for orig, cur in zip(orig_targets, targets):
+                        block_target_offset_remap[cur.offset] = orig[1].offset
 
                 transform_code_object(code, remap_block_offsets)
 
-            # if offset is not in setup_fn_target_offsets, it is an error
+            # if offset_key or offset is not in setup_fn_target_offsets, it is an error
+            # that needs to be fixed
             setup_fn_target_offsets = tuple(
-                meta.block_target_offset_remap[new_offset][n]
+                meta.block_target_offset_remap[offset_key][n]
                 for n in setup_fn_target_offsets
             )
         return ContinueExecutionCache.lookup(
-            meta.code, lineno, new_offset, setup_fn_target_offsets, *args
+            meta.code,
+            lineno,
+            orig_init_offset,
+            orig_resume_offset,
+            setup_fn_target_offsets,
+            *args,
         )
diff --git a/torch/_dynamo/symbolic_convert.py b/torch/_dynamo/symbolic_convert.py
index 4dd1321a5057d..26588dee2674d 100644
--- a/torch/_dynamo/symbolic_convert.py
+++ b/torch/_dynamo/symbolic_convert.py
@@ -2479,7 +2479,9 @@ def store_attr_graph_break(self, inst: Instruction) -> None:
             reason=GraphCompileReason("store_attr", [self.frame_summary()]),
             stack_pops=2,
         )
-        self.output.add_output_instructions([copy.copy(inst)])
+        inst_copy = copy.copy(inst)
+        inst_copy.exn_tab_entry = None
+        self.output.add_output_instructions([inst_copy])
         self.popn(2)
         self.output.add_output_instructions(
             self.create_call_resume_at(
@@ -2679,6 +2681,7 @@ def create_call_resume_at(
             if sys.version_info < (3, 12):
                 assert len(argnames_null) == 0, "variables should not be NULL in < 3.12"
 
+            assert cur_tx.current_instruction.offset is not None
             # compile_subgraph did not codegen any NULLs,
             # so we should not count NullVariables
             stack_len = len(cur_tx.stack) - len(meta.stack_null_idxes)
@@ -2686,7 +2689,8 @@ def create_call_resume_at(
             new_code: types.CodeType = ContinueExecutionCache.lookup(
                 cur_tx.f_code,
                 cur_tx.lineno,
-                resume_inst.offset,
+                cur_tx.current_instruction.offset,
+                resume_inst.offset,  # type: ignore[arg-type]
                 tuple(b.target.offset for b in cur_tx.block_stack),
                 stack_len,
                 argnames,
