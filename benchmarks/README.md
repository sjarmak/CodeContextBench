# CodeContextBench Benchmarks

This directory contains all benchmark task definitions for evaluating coding agents with and without Sourcegraph MCP. The canonical task selection is defined in [`selected_benchmark_tasks.json`](../configs/selected_benchmark_tasks.json) (116 tasks across 10 benchmarks).

See [`docs/TASK_SELECTION.md`](../docs/TASK_SELECTION.md) for the selection methodology.

---

## Active Benchmarks

### 1. [ccb_swebenchpro/](ccb_swebenchpro/) - Multi-Language Bug Fixing
**Tasks**: 36
**Languages**: Go, TypeScript, Python
**SDLC Phase**: Implementation (bug fix)
**Focus**: Long-horizon software engineering on production codebases
**Repositories**: flipt-io/flipt, tutao/tutanota, internetarchive/openlibrary, ansible/ansible, and more
**Task Format**: Harbor (via adapter, pre-generated)

---

### 2. [ccb_locobench/](ccb_locobench/) - Long-Context Agent Tasks
**Tasks**: 25
**Languages**: Rust, C#, C, C++, Python, Java, JavaScript, TypeScript, Go
**SDLC Phases**: Architecture & Design, Implementation (refactoring), Implementation (bug fix)
**Focus**: Architectural understanding, cross-file refactoring, bug investigation on synthetic codebases
**Task Format**: Harbor (via adapter, pre-generated)

---

### 3. [ccb_pytorch/](ccb_pytorch/) - Real PyTorch Pull Requests
**Tasks**: 12
**Languages**: C++ (PyTorch)
**SDLC Phase**: Implementation (bug fix)
**Focus**: Multi-file code changes on real production codebase
**Repository**: PyTorch (pytorch/pytorch)
**Task Format**: Harbor (task.toml, instruction.md, tests/)

---

### 4. [ccb_largerepo/](ccb_largerepo/) - Large Codebase Navigation
**Tasks**: 4
**Languages**: Go, Rust, C++, TypeScript
**SDLC Phase**: Implementation (feature)
**Focus**: Feature implementation in very large codebases
**Repositories**: Kubernetes, Servo, TensorRT-LLM, VS Code
**Task Format**: Harbor (task.toml, instruction.md, tests/)

---

### 5. [ccb_k8sdocs/](ccb_k8sdocs/) - Documentation Generation
**Tasks**: 5
**Languages**: Go
**SDLC Phase**: Documentation
**Focus**: Reconstruct doc.go/README content for stripped Kubernetes packages
**Repositories**: kubernetes/kubernetes, kubernetes/enhancements
**Task Format**: Harbor (task.toml, instruction.md, tests/)

---

### 6. [ccb_tac/](ccb_tac/) - TheAgentCompany Tasks
**Tasks**: 8
**Languages**: C++, Python
**SDLC Phases**: Requirements & Discovery, Implementation (feature), Testing & QA, Maintenance
**Focus**: Diverse SDE tasks (codebase search, implementation, unit testing, troubleshooting)
**Task Format**: Harbor (task.toml, instruction.md, tests/)

---

### 7. [ccb_sweperf/](ccb_sweperf/) - Performance Testing
**Tasks**: 3
**Languages**: Python
**SDLC Phase**: Testing & QA
**Focus**: Performance-oriented software engineering tasks
**Task Format**: Harbor (via adapter, pre-generated)

---

### 8. [ccb_repoqa/](ccb_repoqa/) - Semantic Code Navigation
**Tasks**: 10
**Languages**: Python, C++, Java, Rust, TypeScript
**SDLC Phase**: Requirements & Discovery
**Focus**: Find a function by behavioral description (no name provided)
**Repositories**: psf/black, python-poetry/poetry, google/gson, square/retrofit, and more
**Task Format**: Harbor (via adapter, pre-generated)

---

### 9. [ccb_crossrepo/](ccb_crossrepo/) - Enterprise Codebase Challenges
**Tasks**: 5
**Languages**: Go
**SDLC Phases**: Architecture & Design, Implementation (bug fix), Implementation (refactoring), Testing & QA
**Focus**: API migration, bug localization, cross-file reasoning, symbol rename, and smoke testing on Kubernetes
**Repository**: kubernetes/kubernetes
**Task Format**: Harbor (task.toml, instruction.md, tests/)
**Note**: Requires `harbor-ccb_crossrepo:base` Docker image built from `base/` directory.

---

### 10. [ccb_dibench/](ccb_dibench/) - Dependency Inference
**Tasks**: 8
**Languages**: Python, Rust, JavaScript, C#
**SDLC Phase**: Implementation (feature)
**Focus**: Infer and configure missing dependencies in build files by analyzing source code
**Source**: Microsoft DI-Bench (https://github.com/microsoft/DI-Bench)
**Task Format**: Harbor (task.toml, instruction.md, tests/)
**Note**: Each task includes the full project repo with dependencies stripped from build files.

---

## Benchmark Summary

| Benchmark | Tasks | Languages | SDLC Phase |
|-----------|------:|-----------|------------|
| ccb_swebenchpro | 36 | Go, TypeScript, Python | Bug fixing |
| ccb_locobench | 25 | 9 languages | Architecture, Refactoring |
| ccb_pytorch | 12 | C++ | Bug fixing |
| ccb_repoqa | 10 | Python, C++, Java, Rust, TypeScript | Code navigation |
| ccb_tac | 8 | C++, Python | Mixed (4 phases) |
| ccb_dibench | 8 | Python, Rust, JavaScript, C# | Dependency inference |
| ccb_k8sdocs | 5 | Go | Documentation |
| ccb_largerepo | 4 | Go, Rust, C++, TypeScript | Feature implementation |
| ccb_crossrepo | 5 | Go | Architecture, Bug fix, Refactoring, Testing |
| ccb_sweperf | 3 | Python | Testing & QA |
| **Total** | **116** | | |

---

## Running Benchmarks

### 3-Config Comparison (Recommended)

Each benchmark has a shell runner in [`configs/`](../configs/) that executes selected tasks across the 3-config matrix (Baseline, MCP-Base, MCP-Full):

```bash
# Run all selected tasks for a benchmark
bash configs/locobench_3config.sh
bash configs/swebenchpro_3config.sh
bash configs/bigcode_3config.sh
bash configs/k8s_docs_3config.sh

# Run all benchmarks from the unified runner
bash configs/run_selected_tasks.sh

# Run only baseline config
bash configs/locobench_3config.sh --baseline-only
```

### Single Task Run

```bash
harbor run --path benchmarks/ccb_largerepo/big-code-vsc-001 \
  --agent-import-path agents.claude_baseline_agent:BaselineClaudeCodeAgent \
  --model anthropic/claude-haiku-4-5-20251001 \
  -n 1
```

See [`docs/CONFIGS.md`](../docs/CONFIGS.md) for the full tool-by-tool breakdown of each config.

---

## Results & Analysis

After running benchmarks, generate evaluation reports:

```bash
python3 scripts/generate_eval_report.py \
  --runs-dir /path/to/runs/official/ \
  --output-dir ./eval_reports/
```

See the root [README.md](../README.md) for the full metrics extraction pipeline.
