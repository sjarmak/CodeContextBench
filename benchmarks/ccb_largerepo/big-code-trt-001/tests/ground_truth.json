{
  "task_id": "big-code-trt-001",
  "category": "feature_implementation",
  "repo": "NVIDIA/TensorRT-LLM",
  "language": "python,cpp",
  "files": [
    "cpp/include/tensorrt_llm/common/quantization.h",
    "tensorrt_llm/quantization/mode.py",
    "tensorrt_llm/_torch/modules/fused_moe/fused_moe_trtllm_gen.py",
    "tensorrt_llm/_torch/modules/fused_moe/interface.py",
    "tensorrt_llm/_torch/modules/fused_moe/fused_moe_cutlass.py",
    "tensorrt_llm/quantization/utils/fp4_utils.py"
  ],
  "dependency_chain": [
    "tensorrt_llm/quantization/mode.py",
    "cpp/include/tensorrt_llm/common/quantization.h",
    "tensorrt_llm/_torch/modules/fused_moe/interface.py",
    "tensorrt_llm/_torch/modules/fused_moe/fused_moe_cutlass.py",
    "tensorrt_llm/_torch/modules/fused_moe/fused_moe_trtllm_gen.py"
  ],
  "confidence": "high",
  "methodology": "Trajectory mining from 2 independent passing runs (baseline + SG_full, both reward=1.0). Common files modified across runs. W4A8_MXFP4_INT8 follows W4A8_MXFP4_FP8 pattern across Python enums, C++ enums, and kernel routing."
}
