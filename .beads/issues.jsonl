{"id":"CodeContextBench-05n","title":"Run DependEval benchmark: 32 tasks x 3 configs","description":"Execute first runs for DependEval benchmark. 32 tasks (16 DR + 16 ME) across 3 configs. Blocked by: dependeval_3config.sh creation. Large run — will need parallel execution and multi-account setup.","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:08.542946806Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:08.542946806Z","dependencies":[{"issue_id":"CodeContextBench-05n","depends_on_id":"CodeContextBench-1fa","type":"blocks","created_at":"2026-02-06T14:50:47.341118378Z","created_by":"LoCoBench Bot"}]}
{"id":"CodeContextBench-17e","title":"Rerun LoCoBench 079 SG_base + SG_full after mirror fix","description":"The sg-benchmarks/locobench-c_api_graphql_expert_079 GitHub mirror was empty (never pushed). Fixed 2026-02-06 (commit 259e038f pushed to GitHub, pending Sourcegraph re-index). Two tasks affected: c_api_graphql_expert_079_architectural_understanding_expert_01 and c_api_graphql_expert_079_cross_file_refactoring_expert_01. Previous SG_base results archived to archive/locobench_079_empty_repo_20260206/. Need: (1) Verify SG indexing complete (2) Rerun SG_base for both tasks (3) Rerun SG_full for both tasks (with DS retry preamble)","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T21:09:22.554182955Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T21:09:22.554182955Z"}
{"id":"CodeContextBench-1bz","title":"Update README.md with 13 benchmarks, 156 tasks, QA/operational sections","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:25:58.700321335Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:30:46.241540394Z","closed_at":"2026-02-06T14:30:46.241540394Z","close_reason":"Closed"}
{"id":"CodeContextBench-1fa","title":"Create configs/dependeval_3config.sh run script","description":"DependEval (32 tasks) has no 3config run script. Create configs/dependeval_3config.sh following the pattern of other *_3config.sh files. Should run all 32 tasks across 3 configs (baseline, sourcegraph_base, sourcegraph_full). Note: DependEval tasks are dependency ordering — may not benefit much from code search MCP.","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:57.19161103Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:42:45.917823776Z","closed_at":"2026-02-06T15:42:45.917823776Z","close_reason":"Created configs/dependeval_3config.sh with 32-task SG repo mappings, parallel execution support"}
{"id":"CodeContextBench-23c","title":"Update LoCoBench verify.py weights in 25 task files","description":"Template weights were updated but 25 individual task verify.py files still reference old weight values. Need to propagate new weights to all LoCoBench tasks. Low priority since current results already use the tasks.","status":"closed","priority":3,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:28.002219633Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T18:12:57.754096612Z","closed_at":"2026-02-06T18:12:57.754096612Z","close_reason":"Updated 23 LoCoBench verify.py files from old weights (0.5/0.2/0.2/0.1) to new weights (0.35/0.30/0.25/0.10). All 50 task verify.py files now match the template. 7 of the 23 were in the active selected-25 benchmark set."}
{"id":"CodeContextBench-33o","title":"US-011: Migrate results and remove score fallback","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T21:42:35.481643658Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T21:44:33.467729962Z","closed_at":"2026-02-06T21:44:33.467729962Z","close_reason":"Migrated 30 files, removed score fallback from generate_manifest.py"}
{"id":"CodeContextBench-3c9","title":"Archive 12 stale run batches","description":"QA audit M1: 12 stale batches (~325 results) sitting in runs/official/ that predate current verified results. Move to archive/ to reduce scan noise. Identify by checking timestamps against known good run dirs.","notes":"CORRECTED: 8 of 10 stale batches were restored from archive because they contained unique task results not present in newer batches. Only 3 batches correctly archived: pytorch_gapfill broken-verifier, linuxflbench incomplete run. The 8 restored: bigcode(2), k8s_docs(2), swebenchpro(2), sweperf(1), tac(1).","status":"closed","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:47.10922058Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T19:13:23.623047344Z","closed_at":"2026-02-06T18:00:02.35526543Z","close_reason":"Archived 10 stale batches (171 results) + 1 broken-verifier PyTorch gapfill batch. Total 11 batches moved to runs/official/archive/."}
{"id":"CodeContextBench-3e0","title":"Reclassify context window errors in TAC and CrossRepo","description":"QA audit C6: 5 context window errors misclassified as task failures (TAC find-in-codebase tasks, CrossRepo). The context_window_exceeded fingerprint exists in status_fingerprints.py but historical runs need reclassification in MANIFEST. May need to mark these tasks as infra-limited rather than agent-failed.","status":"closed","priority":3,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:20.850762526Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T18:12:29.617496653Z","closed_at":"2026-02-06T18:12:29.617496653Z","close_reason":"Investigation found NO context window exceeded errors in TAC or CrossRepo. Original C6 audit finding was misidentified. TAC find-in-codebase failures are RocketChat network unreachable (infra issue). CrossRepo failures are genuine task difficulty (0% solve rate). High token counts reflect heavy context usage, not window overflow."}
{"id":"CodeContextBench-3j8","title":"Fix CrossRepo verifier: COPY expected_changes.json into Docker image","description":"CrossRepo test.sh references /tests/expected_changes.json but the Dockerfile never COPYs it into the image. All 4 tasks fail even when agent succeeds. Fix: add COPY tests/expected_changes.json /tests/ to each CrossRepo Dockerfile. Affects: api_upgrade_01, bug_localization_01, cross_file_reasoning_01, refactor_rename_01.","status":"closed","priority":0,"issue_type":"bug","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:33.859688777Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:24:45.679075805Z","closed_at":"2026-02-06T15:24:45.679075805Z","close_reason":"Already fixed: test.sh path corrected from /task/tests/ to /tests/ in commit 0483b714. Harbor uploads tests/ to /tests/ correctly. Old runs used wrong path. Need reruns, not code changes."}
{"id":"CodeContextBench-3ri","title":"Fix TAC verifier: test.sh score/final_score key mismatch","description":"TAC test.sh checks for 'score' key first, but some evaluators write 'final_score'. The fallback logic exists but RocketChat tasks (tac-troubleshoot-dev-setup) also fail because RocketChat service is unreachable in Docker. Fix: verify evaluator output key consistency across all 8 TAC tasks. Root causes: (1) score key mismatch, (2) RocketChat unreachable, (3) arg mismatch in older runs, (4) find-in-codebase tasks hit context window limits.","status":"closed","priority":0,"issue_type":"bug","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:38.285701683Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:24:50.780685598Z","closed_at":"2026-02-06T15:24:50.780685598Z","close_reason":"Already fixed: all 8 TAC test.sh files now handle both score/final_score keys AND use correct --result_path arg (not --output_path). Old runs used --output_path which eval.py rejects. find-in-codebase-1/2 fail due to network unreachable (RocketChat infra issue, not verifier bug). Need reruns, not code changes."}
{"id":"CodeContextBench-4v8","title":"Add missing time_limit_sec to PyTorch sgt-008 and sgt-025","description":"QA audit C3: PyTorch tasks sgt-008 (808 LOC, critical difficulty) and sgt-025 have no time_limit_sec in task.toml. These are the largest PyTorch tasks and will timeout without proper limits. Add appropriate time_limit_sec values based on task size.","status":"closed","priority":1,"issue_type":"bug","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:42.21947973Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:42:45.800948321Z","closed_at":"2026-02-06T15:42:45.800948321Z","close_reason":"Added time_limit_sec=600 to sgt-008 and sgt-025 task.toml files"}
{"id":"CodeContextBench-98m","title":"Create docs/QA_PROCESS.md","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:26:04.643507515Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:30:46.258974528Z","closed_at":"2026-02-06T14:30:46.258974528Z","close_reason":"Closed"}
{"id":"CodeContextBench-99h","title":"Run TAC SG_base and SG_full configs","description":"QA audit H2: TAC has only baseline runs (all failing). Need SG_base and SG_full runs. Note: tac-find-in-codebase-1 and tac-find-in-codebase-2 consistently hit context window limits (5-12M tokens) across all configs — consider excluding from aggregate. Must fix TAC verifier first.","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:12.712525021Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:12.712525021Z","dependencies":[{"issue_id":"CodeContextBench-99h","depends_on_id":"CodeContextBench-3ri","type":"blocks","created_at":"2026-02-06T14:50:47.285413902Z","created_by":"LoCoBench Bot"}]}
{"id":"CodeContextBench-9r9","title":"Run LinuxFLBench benchmark: 5 tasks x 3 configs","description":"Execute first runs for LinuxFLBench benchmark. 5 Linux kernel fault localization tasks across 3 configs. Config script exists: configs/linuxflbench_3config.sh. PREREQUISITE: Base Docker images (ccb-linux-base) must be built first — these are ~5-6GB each.","notes":"Baseline: 5/5 done (1.0, 1.0, 0.3, 1.0, 1.0). MCP-base launched (sg-benchmarks mirrors). MCP-full will auto-launch after base. Orphaned MCP run archived.","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:04.95903591Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T18:55:45.566852369Z","closed_at":"2026-02-06T18:55:45.566852369Z","close_reason":"LinuxFLBench 5x3 complete. Baseline avg=0.86 (4/5 perfect, 1 partial). SG_base avg=0.74 (2/5 perfect, 2 partial, 1 fail). SG_full avg=0.86 (4/5 perfect, 1 partial — matches baseline, beats SG_base). SG_full recovered wifi and sound tasks where SG_base regressed. Results in linuxflbench_opus_20260206_155043 (baseline), 164001 (SG_base), 180131 (SG_full)."}
{"id":"CodeContextBench-a5e","title":"Update CLAUDE.md benchmarks table to 13","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:26:04.68755366Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:30:46.276437033Z","closed_at":"2026-02-06T14:30:46.276437033Z","close_reason":"Closed"}
{"id":"CodeContextBench-aot","title":"Regenerate MANIFEST.json after cleanup","description":"After archiving ghost runs, stale batches, and resolving duplicates, regenerate MANIFEST.json with python3 scripts/generate_manifest.py. Verify entry count is accurate and no archived results appear.","notes":"CORRECTED: Initial archival removed 8 batches that had unique task results. Restored those 8 batches. MANIFEST corrected to 277 tasks (34 runs). Difference from 329: SG_full tasks in sgfull_ds_compromised archive (valid - DS polling broken pre-fix), LoCoBench 5 tasks missing per config.","status":"closed","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:57.030129233Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T19:13:23.588709734Z","closed_at":"2026-02-06T18:01:37.584924816Z","close_reason":"MANIFEST regenerated with 329 tasks across 36 runs. Added linuxflbench_ prefix mapping to both generate_manifest.py and aggregate_status.py. LinuxFLBench now appears in MANIFEST. 11 batches archived before regen.","dependencies":[{"issue_id":"CodeContextBench-aot","depends_on_id":"CodeContextBench-nvw","type":"blocks","created_at":"2026-02-06T14:50:47.399305869Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-aot","depends_on_id":"CodeContextBench-3c9","type":"blocks","created_at":"2026-02-06T14:50:47.455213176Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-aot","depends_on_id":"CodeContextBench-yeo","type":"blocks","created_at":"2026-02-06T14:50:47.510339213Z","created_by":"LoCoBench Bot"}]}
{"id":"CodeContextBench-b41","title":"Add DependEval and LinuxFLBench to TASK_CATALOG.md","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:26:04.732271812Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:30:46.293910463Z","closed_at":"2026-02-06T14:30:46.293910463Z","close_reason":"Closed"}
{"id":"CodeContextBench-dfp","title":"Run LoCoBench baseline and SG_full configs","description":"QA audit H2: LoCoBench only has SG_base results in MANIFEST (25/25 tasks). Need baseline and SG_full runs for complete 3-config comparison. SG_full should use the updated Deep Search preamble.","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:17.265852053Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:17.265852053Z","dependencies":[{"issue_id":"CodeContextBench-dfp","depends_on_id":"CodeContextBench-17e","type":"blocks","created_at":"2026-02-06T21:09:35.481295416Z","created_by":"LoCoBench Bot"}]}
{"id":"CodeContextBench-gzm","title":"US-006: Write Agent Interface Specification","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T21:31:45.77218156Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T21:33:17.586508516Z","closed_at":"2026-02-06T21:33:17.586508516Z","close_reason":"US-006 complete: docs/AGENT_INTERFACE.md created"}
{"id":"CodeContextBench-h70","title":"US-001: Create result.json JSON Schema","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T21:11:04.541986753Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T21:12:25.714058684Z","closed_at":"2026-02-06T21:12:25.714058684Z","close_reason":"Schema created and validated against 11 real result.json files"}
{"id":"CodeContextBench-jz5","title":"US-010: Create submission packaging script","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T21:40:44.70660195Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T21:42:30.047823099Z","closed_at":"2026-02-06T21:42:30.047823099Z","close_reason":"Implemented package_submission.py"}
{"id":"CodeContextBench-kph","title":"Rerun SG_full tasks with Deep Search retry preamble","description":"Deep Search retry fix has been applied to claude_baseline_agent.py preamble. Need to rerun SG_full configs for benchmarks where old runs had \u003e30% polling-only DS responses: K8s Docs (40% success), PyTorch (50% success), SWE-bench Pro (67% success). Also rerun LoCoBench and RepoQA SG_full which used old DS instruction format (H1: LoCoBench 2/23, RepoQA 0/10 compliance).","status":"open","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:13.685838976Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:13.685838976Z"}
{"id":"CodeContextBench-m9g","title":"US-003: Fix RepoQA test.sh to write reward.txt","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T21:15:44.916868917Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T21:18:19.719362713Z","closed_at":"2026-02-06T21:18:19.719362713Z","close_reason":"US-003 complete: all 10 RepoQA test.sh + template write reward.txt"}
{"id":"CodeContextBench-nvw","title":"Archive ghost runs and false positives from MANIFEST","description":"QA audit C4-C5: Archive 24 protonmail ghost runs (0 tokens), 3 tutanota false positives (reward=1.0 with 0 tokens), and 5 internetarchive abandoned runs. These inflate pass rates and must be moved to runs/official/archive/. Use scripts/archive_run.py or manual mv.","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:43.688264315Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:42:45.859971148Z","closed_at":"2026-02-06T15:42:45.859971148Z","close_reason":"Archived 23 ghost runs (17 internetarchive, 6 tutanota) to __archived_invalid. 24 protonmail were already archived."}
{"id":"CodeContextBench-p3k","title":"Investigate baseline token logging bug (52 runs report 0 tokens)","description":"QA audit H3: 52 baseline runs report 0 tokens despite valid execution and results. May be a Harbor issue or Claude Code CLI issue. Need to determine if this affects cost reporting and whether reruns are needed.","status":"closed","priority":3,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:24.47470115Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T18:22:48.887098564Z","closed_at":"2026-02-06T18:22:48.887098564Z","close_reason":"Root cause identified: Harbor's _get_session_dir fails when Claude Code spawns subagents (Task tool), creating subagents/ subdir that confuses multi-dir detection. Affects 18 runs across 7 benchmarks. Scoring NOT affected (rewards are correct). Cost data recoverable from claude-code.txt transcripts. No reruns needed. Optional Harbor upstream fix to prefer top-level session dir over subagent dirs."}
{"id":"CodeContextBench-rej","title":"Generate aggregate CCB evaluation report","description":"After all benchmark runs complete and MANIFEST is clean, generate the aggregate evaluation report using python3 scripts/generate_report.py. Should cover all 13 benchmarks with 3-config comparison (baseline vs SG_base vs SG_full), MCP impact analysis, and per-benchmark breakdowns.","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:31.544649793Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:31.544649793Z","dependencies":[{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-aot","type":"blocks","created_at":"2026-02-06T14:50:47.565065613Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-kph","type":"blocks","created_at":"2026-02-06T14:50:47.632620141Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-yk3","type":"blocks","created_at":"2026-02-06T14:50:47.689660185Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-9r9","type":"blocks","created_at":"2026-02-06T14:50:47.744576933Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-05n","type":"blocks","created_at":"2026-02-06T14:50:47.799295655Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-99h","type":"blocks","created_at":"2026-02-06T14:50:47.854278452Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-dfp","type":"blocks","created_at":"2026-02-06T14:50:47.909843823Z","created_by":"LoCoBench Bot"}]}
{"id":"CodeContextBench-tcg","title":"Pin TAC OpenHands sg-benchmarks mirror to specific commit","description":"TAC tasks tac-write-unit-test and tac-dependency-change use sg-benchmarks/OpenHands--latest mirror which is NOT pinned to a specific commit. The TAC GitLab commit bfd78f9 was not found in upstream GitHub repo. Risk: SG searches different code than local Docker env. Either fork at the GitLab commit or document the limitation. See docs/INSTRUCTION_RUN_AUDIT_2026-02-06.md issue P-1.","status":"closed","priority":3,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T16:40:22.665590732Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T18:26:14.758883934Z","closed_at":"2026-02-06T18:26:14.758883934Z","close_reason":"Confirmed bfd78f9 does not exist in upstream All-Hands-AI/OpenHands GitHub repo. This is a TAC GitLab-only commit from pre-built Docker images. Cannot pin mirror — --latest fallback is the best option. Already documented in instance_to_mirror.json with commit=null and explicit notes. These 2 tasks (tac-write-unit-test, tac-dependency-change) should be flagged in evaluation report as having potential SG code divergence."}
{"id":"CodeContextBench-wb4","title":"US-009: Build leaderboard generator script","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T21:37:57.932739647Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T21:40:00.689194173Z","closed_at":"2026-02-06T21:40:00.689194173Z","close_reason":"US-009 complete: generate_leaderboard.py implemented"}
{"id":"CodeContextBench-yeo","title":"Resolve 4 duplicate runs with divergent rewards","description":"QA audit C7: 4 tasks have duplicate runs with different rewards (qutebrowser, tutanota). Establish policy (keep latest? keep best?) and archive the duplicates. Currently MANIFEST uses latest-wins which may not always be correct.","status":"closed","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:53.545999492Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T18:00:04.770387086Z","closed_at":"2026-02-06T18:00:04.770387086Z","close_reason":"Resolved: 23 divergent duplicate pairs traced to 2 root causes. (1) PyTorch gapfill batch used old broken verifier — archived, main batch has correct results. (2) SWE-Perf old batch already archived. (3) Remaining duplicates (qutebrowser, LargeRepo) are genuine non-determinism — latest-wins MANIFEST policy is appropriate."}
{"id":"CodeContextBench-yk3","title":"Run CodeReview benchmark: 3 tasks x 3 configs","description":"Execute first runs for CodeReview benchmark. 3 tasks (cr-ghost-001, cr-aspnetcore-001, cr-calcom-001) across baseline, sourcegraph_base, sourcegraph_full. Config script exists: configs/codereview_3config.sh. Verify Docker builds succeed first with a quick single-task test.","notes":"Baseline: 3/3 done (1.0, 1.0, 0.8). MCP-base launched (sg-benchmarks mirrors). MCP-full will auto-launch after base. Invalid MCP runs archived.","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:01.091878411Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T17:05:14.773296031Z","closed_at":"2026-02-06T17:05:14.773296031Z","close_reason":"CodeReview 3x3 complete. Baseline avg=0.93, SG_base avg=0.89, SG_full avg=0.93. Results in codereview_opus_20260206_155036 (baseline), codereview_opus_20260206_163958 (SG_base), codereview_opus_20260206_164838 (SG_full)."}
