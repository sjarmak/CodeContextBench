{"id":"CodeContextBench-05n","title":"Run DependEval benchmark: 32 tasks x 3 configs","description":"Execute first runs for DependEval benchmark. 32 tasks (16 DR + 16 ME) across 3 configs. Blocked by: dependeval_3config.sh creation. Large run — will need parallel execution and multi-account setup.","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:08.542946806Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:08.542946806Z","dependencies":[{"issue_id":"CodeContextBench-05n","depends_on_id":"CodeContextBench-1fa","type":"blocks","created_at":"2026-02-06T14:50:47.341118378Z","created_by":"LoCoBench Bot"}]}
{"id":"CodeContextBench-1bz","title":"Update README.md with 13 benchmarks, 156 tasks, QA/operational sections","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:25:58.700321335Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:30:46.241540394Z","closed_at":"2026-02-06T14:30:46.241540394Z","close_reason":"Closed"}
{"id":"CodeContextBench-1fa","title":"Create configs/dependeval_3config.sh run script","description":"DependEval (32 tasks) has no 3config run script. Create configs/dependeval_3config.sh following the pattern of other *_3config.sh files. Should run all 32 tasks across 3 configs (baseline, sourcegraph_base, sourcegraph_full). Note: DependEval tasks are dependency ordering — may not benefit much from code search MCP.","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:57.19161103Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:42:45.917823776Z","closed_at":"2026-02-06T15:42:45.917823776Z","close_reason":"Created configs/dependeval_3config.sh with 32-task SG repo mappings, parallel execution support"}
{"id":"CodeContextBench-23c","title":"Update LoCoBench verify.py weights in 25 task files","description":"Template weights were updated but 25 individual task verify.py files still reference old weight values. Need to propagate new weights to all LoCoBench tasks. Low priority since current results already use the tasks.","status":"open","priority":3,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:28.002219633Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:28.002219633Z"}
{"id":"CodeContextBench-3c9","title":"Archive 12 stale run batches","description":"QA audit M1: 12 stale batches (~325 results) sitting in runs/official/ that predate current verified results. Move to archive/ to reduce scan noise. Identify by checking timestamps against known good run dirs.","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:47.10922058Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:49:47.10922058Z"}
{"id":"CodeContextBench-3e0","title":"Reclassify context window errors in TAC and CrossRepo","description":"QA audit C6: 5 context window errors misclassified as task failures (TAC find-in-codebase tasks, CrossRepo). The context_window_exceeded fingerprint exists in status_fingerprints.py but historical runs need reclassification in MANIFEST. May need to mark these tasks as infra-limited rather than agent-failed.","status":"open","priority":3,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:20.850762526Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:20.850762526Z"}
{"id":"CodeContextBench-3j8","title":"Fix CrossRepo verifier: COPY expected_changes.json into Docker image","description":"CrossRepo test.sh references /tests/expected_changes.json but the Dockerfile never COPYs it into the image. All 4 tasks fail even when agent succeeds. Fix: add COPY tests/expected_changes.json /tests/ to each CrossRepo Dockerfile. Affects: api_upgrade_01, bug_localization_01, cross_file_reasoning_01, refactor_rename_01.","status":"closed","priority":0,"issue_type":"bug","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:33.859688777Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:24:45.679075805Z","closed_at":"2026-02-06T15:24:45.679075805Z","close_reason":"Already fixed: test.sh path corrected from /task/tests/ to /tests/ in commit 0483b714. Harbor uploads tests/ to /tests/ correctly. Old runs used wrong path. Need reruns, not code changes."}
{"id":"CodeContextBench-3ri","title":"Fix TAC verifier: test.sh score/final_score key mismatch","description":"TAC test.sh checks for 'score' key first, but some evaluators write 'final_score'. The fallback logic exists but RocketChat tasks (tac-troubleshoot-dev-setup) also fail because RocketChat service is unreachable in Docker. Fix: verify evaluator output key consistency across all 8 TAC tasks. Root causes: (1) score key mismatch, (2) RocketChat unreachable, (3) arg mismatch in older runs, (4) find-in-codebase tasks hit context window limits.","status":"closed","priority":0,"issue_type":"bug","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:38.285701683Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:24:50.780685598Z","closed_at":"2026-02-06T15:24:50.780685598Z","close_reason":"Already fixed: all 8 TAC test.sh files now handle both score/final_score keys AND use correct --result_path arg (not --output_path). Old runs used --output_path which eval.py rejects. find-in-codebase-1/2 fail due to network unreachable (RocketChat infra issue, not verifier bug). Need reruns, not code changes."}
{"id":"CodeContextBench-4v8","title":"Add missing time_limit_sec to PyTorch sgt-008 and sgt-025","description":"QA audit C3: PyTorch tasks sgt-008 (808 LOC, critical difficulty) and sgt-025 have no time_limit_sec in task.toml. These are the largest PyTorch tasks and will timeout without proper limits. Add appropriate time_limit_sec values based on task size.","status":"closed","priority":1,"issue_type":"bug","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:42.21947973Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:42:45.800948321Z","closed_at":"2026-02-06T15:42:45.800948321Z","close_reason":"Added time_limit_sec=600 to sgt-008 and sgt-025 task.toml files"}
{"id":"CodeContextBench-98m","title":"Create docs/QA_PROCESS.md","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:26:04.643507515Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:30:46.258974528Z","closed_at":"2026-02-06T14:30:46.258974528Z","close_reason":"Closed"}
{"id":"CodeContextBench-99h","title":"Run TAC SG_base and SG_full configs","description":"QA audit H2: TAC has only baseline runs (all failing). Need SG_base and SG_full runs. Note: tac-find-in-codebase-1 and tac-find-in-codebase-2 consistently hit context window limits (5-12M tokens) across all configs — consider excluding from aggregate. Must fix TAC verifier first.","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:12.712525021Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:12.712525021Z","dependencies":[{"issue_id":"CodeContextBench-99h","depends_on_id":"CodeContextBench-3ri","type":"blocks","created_at":"2026-02-06T14:50:47.285413902Z","created_by":"LoCoBench Bot"}]}
{"id":"CodeContextBench-9r9","title":"Run LinuxFLBench benchmark: 5 tasks x 3 configs","description":"Execute first runs for LinuxFLBench benchmark. 5 Linux kernel fault localization tasks across 3 configs. Config script exists: configs/linuxflbench_3config.sh. PREREQUISITE: Base Docker images (ccb-linux-base) must be built first — these are ~5-6GB each.","status":"in_progress","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:04.95903591Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:42:51.635222759Z"}
{"id":"CodeContextBench-a5e","title":"Update CLAUDE.md benchmarks table to 13","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:26:04.68755366Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:30:46.276437033Z","closed_at":"2026-02-06T14:30:46.276437033Z","close_reason":"Closed"}
{"id":"CodeContextBench-aot","title":"Regenerate MANIFEST.json after cleanup","description":"After archiving ghost runs, stale batches, and resolving duplicates, regenerate MANIFEST.json with python3 scripts/generate_manifest.py. Verify entry count is accurate and no archived results appear.","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:57.030129233Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:49:57.030129233Z","dependencies":[{"issue_id":"CodeContextBench-aot","depends_on_id":"CodeContextBench-nvw","type":"blocks","created_at":"2026-02-06T14:50:47.399305869Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-aot","depends_on_id":"CodeContextBench-3c9","type":"blocks","created_at":"2026-02-06T14:50:47.455213176Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-aot","depends_on_id":"CodeContextBench-yeo","type":"blocks","created_at":"2026-02-06T14:50:47.510339213Z","created_by":"LoCoBench Bot"}]}
{"id":"CodeContextBench-b41","title":"Add DependEval and LinuxFLBench to TASK_CATALOG.md","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:26:04.732271812Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:30:46.293910463Z","closed_at":"2026-02-06T14:30:46.293910463Z","close_reason":"Closed"}
{"id":"CodeContextBench-dfp","title":"Run LoCoBench baseline and SG_full configs","description":"QA audit H2: LoCoBench only has SG_base results in MANIFEST (25/25 tasks). Need baseline and SG_full runs for complete 3-config comparison. SG_full should use the updated Deep Search preamble.","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:17.265852053Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:17.265852053Z"}
{"id":"CodeContextBench-kph","title":"Rerun SG_full tasks with Deep Search retry preamble","description":"Deep Search retry fix has been applied to claude_baseline_agent.py preamble. Need to rerun SG_full configs for benchmarks where old runs had \u003e30% polling-only DS responses: K8s Docs (40% success), PyTorch (50% success), SWE-bench Pro (67% success). Also rerun LoCoBench and RepoQA SG_full which used old DS instruction format (H1: LoCoBench 2/23, RepoQA 0/10 compliance).","status":"open","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:13.685838976Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:13.685838976Z"}
{"id":"CodeContextBench-nvw","title":"Archive ghost runs and false positives from MANIFEST","description":"QA audit C4-C5: Archive 24 protonmail ghost runs (0 tokens), 3 tutanota false positives (reward=1.0 with 0 tokens), and 5 internetarchive abandoned runs. These inflate pass rates and must be moved to runs/official/archive/. Use scripts/archive_run.py or manual mv.","status":"closed","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:43.688264315Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:42:45.859971148Z","closed_at":"2026-02-06T15:42:45.859971148Z","close_reason":"Archived 23 ghost runs (17 internetarchive, 6 tutanota) to __archived_invalid. 24 protonmail were already archived."}
{"id":"CodeContextBench-p3k","title":"Investigate baseline token logging bug (52 runs report 0 tokens)","description":"QA audit H3: 52 baseline runs report 0 tokens despite valid execution and results. May be a Harbor issue or Claude Code CLI issue. Need to determine if this affects cost reporting and whether reruns are needed.","status":"open","priority":3,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:24.47470115Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:24.47470115Z"}
{"id":"CodeContextBench-rej","title":"Generate aggregate CCB evaluation report","description":"After all benchmark runs complete and MANIFEST is clean, generate the aggregate evaluation report using python3 scripts/generate_report.py. Should cover all 13 benchmarks with 3-config comparison (baseline vs SG_base vs SG_full), MCP impact analysis, and per-benchmark breakdowns.","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:31.544649793Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:50:31.544649793Z","dependencies":[{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-aot","type":"blocks","created_at":"2026-02-06T14:50:47.565065613Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-kph","type":"blocks","created_at":"2026-02-06T14:50:47.632620141Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-yk3","type":"blocks","created_at":"2026-02-06T14:50:47.689660185Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-9r9","type":"blocks","created_at":"2026-02-06T14:50:47.744576933Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-05n","type":"blocks","created_at":"2026-02-06T14:50:47.799295655Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-99h","type":"blocks","created_at":"2026-02-06T14:50:47.854278452Z","created_by":"LoCoBench Bot"},{"issue_id":"CodeContextBench-rej","depends_on_id":"CodeContextBench-dfp","type":"blocks","created_at":"2026-02-06T14:50:47.909843823Z","created_by":"LoCoBench Bot"}]}
{"id":"CodeContextBench-tcg","title":"Pin TAC OpenHands sg-benchmarks mirror to specific commit","description":"TAC tasks tac-write-unit-test and tac-dependency-change use sg-benchmarks/OpenHands--latest mirror which is NOT pinned to a specific commit. The TAC GitLab commit bfd78f9 was not found in upstream GitHub repo. Risk: SG searches different code than local Docker env. Either fork at the GitLab commit or document the limitation. See docs/INSTRUCTION_RUN_AUDIT_2026-02-06.md issue P-1.","status":"open","priority":3,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T16:40:22.665590732Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T16:40:28.718156761Z"}
{"id":"CodeContextBench-yeo","title":"Resolve 4 duplicate runs with divergent rewards","description":"QA audit C7: 4 tasks have duplicate runs with different rewards (qutebrowser, tutanota). Establish policy (keep latest? keep best?) and archive the duplicates. Currently MANIFEST uses latest-wins which may not always be correct.","status":"open","priority":2,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:49:53.545999492Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T14:49:53.545999492Z"}
{"id":"CodeContextBench-yk3","title":"Run CodeReview benchmark: 3 tasks x 3 configs","description":"Execute first runs for CodeReview benchmark. 3 tasks (cr-ghost-001, cr-aspnetcore-001, cr-calcom-001) across baseline, sourcegraph_base, sourcegraph_full. Config script exists: configs/codereview_3config.sh. Verify Docker builds succeed first with a quick single-task test.","status":"in_progress","priority":1,"issue_type":"task","owner":"locobench@anthropic.com","created_at":"2026-02-06T14:50:01.091878411Z","created_by":"LoCoBench Bot","updated_at":"2026-02-06T15:42:51.584964376Z"}
