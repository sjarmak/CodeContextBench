## Codebase Patterns

### Project structure
- Benchmarks live in benchmarks/ccb_{name}/{task_id}/ with: task.toml, instruction.md, CLAUDE.md, tests/test.sh, environment/Dockerfile
- Task templates live in benchmarks/ccb_largerepo/templates/{category}/ — 5 categories: architectural_understanding, cross_file_refactoring, bug_investigation, security_analysis, feature_implementation
- Shared verifier library: benchmarks/ccb_largerepo/templates/verifier_lib.sh — MUST be copied into each task's tests/ directory
- Config scripts in configs/ follow _common.sh conventions (source it, load .env.local, use run_harbor_task function)
- Task selection registry: configs/selected_benchmark_tasks.json (array of task objects)
- Ground truth extraction: scripts/ccb_metrics/ground_truth.py — largerepo handler reads tests/ground_truth.json

### Task naming conventions
- Task IDs: big-code-{repo}-{category_abbrev}-{num} where abbrevs are: arch, refac, bug, sec, feat
- Cross-repo: big-code-cross-{ecosystem}-{category_abbrev}-{num}
- Repos abbreviated: k8s, django, pg, vsc, servo, trt, camel, flink, kafka, strata, quantlib, hazelcast, legend, linux, rust

### Pinned commits (from configs/sg_indexing_list.json)
- kubernetes/kubernetes: v1.30.0 (existing), also have new entry for v1.31.0
- torvalds/linux: v6.19 (05f7e89ab9731565d8a62e3b5d1ec206485eeb0b)
- rust-lang/rust: 1.93.1 (01f6ddf7588f42ae2d7eb0a2f21d44e8e96674cf)
- postgres/postgres: REL_18_2 (5a461dc4dbf72a1ec281394a76eb36d68cbdd935)
- django/django: 5.2 (9e7cc2b628fe8fd3895986af9b7fc9525034c1b0)
- apache/camel: camel-4.18.0 (dd22f752e240bd1343af15c775ef02d754f6df38)
- hazelcast/hazelcast: v5.6.0 (a9ce2a02ac17f88fcd38869ac698e56e613dc40c)
- apache/flink: release-2.2.0 (5a336892424a9458653ead89610bf60d771ab8d7)
- apache/kafka: 4.1.1 (be816b82d25370ceac697ccf7c88cea873e9b4e3)
- finos/legend-engine: legend-engine-4.120.1 (20cca27326b19c265bb580e97659420bd33e1ac5)
- OpenGamma/Strata: v2.12.65 (a608d3da56725bc089dfca50697e2c4426683021)
- lballabio/QuantLib: v1.41 (367ce80ae4f285835ceb3bba97746bea239984ad)
- microsoft/vscode: 1.91.1 (existing)

### Scoring approach
- Composite score: 0.4 * task_quality + 0.3 * file_recall + 0.2 * file_precision + 0.1 * dep_accuracy
- test.sh writes score to /logs/verifier/reward.txt AND ir_metrics.json to /logs/verifier/ir_metrics.json
- verifier_lib.sh handles: solution.md parsing, IR metrics computation, dep chain accuracy, root cause MRR
- Avoid bc in bash — use awk for floating point

### Dockerfile patterns
- Use --filter=blob:none --no-checkout for shallow clone
- Always set git config user.email/name
- Always install npm and @anthropic-ai/claude-code
- COPY tests/ into /workspace/tests/
- Base images: golang:1.23-bookworm, eclipse-temurin:17-jdk, python:3.12-bookworm, rust:1.82-bookworm, gcc:14-bookworm, node:22-bookworm

### Judge context
- scripts/ccb_metrics/judge_context.py handles per-task context generation
- benchmark=='bigcode' maps to ccb_largerepo directory
- `_locobench_dimensions()` returns ACS, DTA, CFRD rubrics with 1-5 scale; called for bigcode tasks
- Function reads ir_metrics.json from run dir + ground_truth.json from benchmarks dir

### Run config patterns
- Source _common.sh first
- Load ~/evals/.env.local for credentials
- Filter tasks from selected_benchmark_tasks.json by benchmark name
- Support --parallel, --baseline-only, --full-only flags
- Use run_harbor_task function from _common.sh

### Stdlib-only constraint
- ALL Python scripts must use stdlib only (no requests, no external packages)
- Use urllib.request for HTTP, json for parsing, argparse for CLI

## Completed Foundation (on main)
- US-001: 11 repos pinned in configs/sg_indexing_list.json
- US-002: Templates created for all 5 categories (instruction.md, task.toml, test.sh, CLAUDE.md)
- US-004: verifier_lib.sh with IR metrics pipeline (precision/recall/F1/dep_accuracy/MRR)
- US-012: ground_truth.json backfilled for 4 existing tasks, ground_truth.py updated to read them
- US-015: ground_truth.py reads tests/ground_truth.json for ccb_largerepo tasks

### GitHub API patterns
- The issues endpoint (`/repos/{owner}/{repo}/issues`) returns BOTH issues and PRs. Filter by checking `"pull_request" in item`.
- Many repos label PRs directly with `bug` instead of maintaining separate bug issues. Script must handle both patterns: (1) standalone issues → find linked PR, (2) PRs directly labeled as bugs.
- Unauthenticated API gives only 60 req/hour. Set GITHUB_TOKEN for 5000/hour.
- Issues endpoint `pull_request` sub-object includes `merged_at` when merged, avoiding an extra API call.

## Progress

## 2026-02-15 - US-017
- Implemented `scripts/mine_bug_tasks.py` — mines candidate bug investigation tasks from GitHub
- Handles two patterns: issues with linked PRs AND PRs directly labeled 'bug' (more common)
- CLI flags: --repo (required), --min-files (5), --max-files (30), --limit (20)
- Rate limiting with 3 retries and configurable sleep
- Pagination via Link header parsing
- Files changed: scripts/mine_bug_tasks.py (new)
- **Learnings for future iterations:**
  - Django and many repos label PRs (not issues) with `bug` — filtering out PRs from the issues endpoint drops all candidates
  - GitHub issues endpoint returns PRs too — use `"pull_request" in item` to distinguish
  - Timeline events API requires `Accept: application/vnd.github+json` header
  - Without GITHUB_TOKEN, the API returns very few results per page due to rate limits
---

## 2026-02-15 - US-005
- Added `_locobench_dimensions()` function to `scripts/ccb_metrics/judge_context.py`
- Returns ACS (Architectural Coherence Score), DTA (Dependency Traversal Accuracy), CFRD (Cross-File Reasoning Depth) rubrics
- Each rubric has dimension, definition, scoring_scale='1-5', observable_evidence list
- Enriches evidence with IR metrics (precision/recall/f1/dep_accuracy) from run output and ground truth (files/dependency_chain) from benchmarks
- Integrated into `generate_judge_contexts` — called for `benchmark=='bigcode'` tasks, output stored in `locobench_dimensions` key
- Files changed: scripts/ccb_metrics/judge_context.py (modified)
- **Learnings for future iterations:**
  - IR metrics live at `task_dir / "verifier" / "ir_metrics.json"` in run output
  - Ground truth for largerepo lives at `benchmarks/ccb_largerepo/{task_id}/tests/ground_truth.json`
  - The `_locobench_dimensions` function signature accepts optional `benchmarks_dir` and `task_id` in addition to `task_dir` and `ir_metrics_path` since ground truth is in the benchmarks tree, not the run tree
---

## 2026-02-15 - US-016
- Updated `configs/largerepo_2config.sh` to follow crossrepo_2config.sh conventions
- Fixed stale header comments (was referencing `3config`)
- Renamed `TASK_DIRS` → `TASK_IDS` for consistency across all 2config scripts
- Changed JOBS_BASE prefix from `bigcode_mcp_` → `largerepo_` to match MANIFEST conventions
- Cleaned up verbose summary section to match crossrepo's concise pattern
- Added comments explaining new task SG repo name handling (public repos vs sg-benchmarks mirrors)
- Files changed: configs/largerepo_2config.sh (modified)
- **Learnings for future iterations:**
  - The JOBS_BASE prefix matters for MANIFEST generation — `generate_manifest.py` uses `DIR_PREFIX_TO_SUITE` to map run dirs to benchmark suites. `largerepo_` prefix maps to `ccb_largerepo`.
  - Existing 4 tasks use sg-benchmarks mirrors; new tasks will use public repos indexed in Sourcegraph. The preamble injection handles repo context for new tasks, so SOURCEGRAPH_REPO_NAME can be left unset.
  - Use `TASK_IDS` (not `TASK_DIRS`) as the variable name for task ID arrays — this is the convention in crossrepo and other 2config scripts.
---

## 2026-02-15 - US-018
- Created `benchmarks/ccb_largerepo/docs/LOCOBENCH_ADAPTATION.md` documenting the LoCoBench methodology adaptation
- Created `benchmarks/ccb_largerepo/docs/REPO_SELECTION.md` with repo selection rationale
- LOCOBENCH_ADAPTATION.md covers: adopted principles (5 task categories, SE metrics), adapted principles (codebase scale, IR metrics verifier, research-derived ground truth, multi-project diversity), excluded principles (synthetic code gen, context window stress testing, 3 categories)
- Includes comparison table (LoCoBench original vs our adaptation across 12 dimensions)
- Maps LCBS dimensions: SE→ACS/DTA/CFRD rubrics, FC→compilation+test verifiers, CQ→static analysis (planned), LCU→IR metrics
- References arXiv:2509.09614
- REPO_SELECTION.md covers: Tier A (8 systems infra repos) and Tier B (7 capital markets repos), LOC/language/domain/indexing status
- Files changed: benchmarks/ccb_largerepo/docs/LOCOBENCH_ADAPTATION.md (new), benchmarks/ccb_largerepo/docs/REPO_SELECTION.md (new)
- **Learnings for future iterations:**
  - The LoCoBench paper (arXiv:2509.09614) defines 8 categories; we adopted 5 and excluded code_comprehension, integration_testing, and multi_session
  - LCBS has 4 evaluation dimensions: SE, FC, CQ, LCU — all mapped to our implementation except CQ (static analysis, planned but not implemented)
  - Tier B capital markets repos were selected to represent the trade lifecycle: Kafka (capture) → Flink (enrichment) → Strata/QuantLib (pricing) → Legend (governance)
---

