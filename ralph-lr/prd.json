{
  "project": "Large-Repo LoCoBench Expansion",
  "branchName": "ralph/largerepo-locobench-expansion",
  "description": "Expand ccb_largerepo benchmark from 4 to 30+ tasks across 5 LoCoBench categories and 10+ repos. Build infrastructure scripts, enrich judge context, scaffold tasks with ground truth, create Dockerfiles, and register everything. Foundation work (US-001, US-002, US-004, US-012, US-015) is already complete on main.",
  "userStories": [
    {
      "id": "US-017",
      "title": "Build bug task sourcing script",
      "description": "As a benchmark engineer, I want scripts/mine_bug_tasks.py that queries GitHub API for candidate bug investigation tasks so that task sourcing scales beyond manual curation.",
      "acceptanceCriteria": [
        "scripts/mine_bug_tasks.py exists and is executable",
        "Script accepts --repo flag (e.g., --repo kubernetes/kubernetes) to target specific repos",
        "Script accepts --min-files (default 5) and --max-files (default 30) flags",
        "Script queries GitHub API for closed issues labeled 'bug' with linked merged PRs that touch min-files+ files spanning 2+ directories",
        "Script outputs JSON to stdout: [{issue_url, pr_url, files_changed: [...], file_count, directory_spread, description}]",
        "Script handles GitHub API pagination and rate limiting (uses GITHUB_TOKEN env var if available)",
        "python3 scripts/mine_bug_tasks.py --repo django/django --min-files 5 --max-files 20 runs without error (may return empty if no matching issues)",
        "Uses only stdlib (no requests library) - use urllib.request for HTTP calls",
        "Script has --limit N flag to cap number of candidates returned (default 20)"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Standalone script, no dependencies on other stories. GitHub API: api.github.com/repos/{owner}/{repo}/issues?labels=bug&state=closed"
    },
    {
      "id": "US-005",
      "title": "Enrich judge context with LoCoBench SE dimensions",
      "description": "As a benchmark engineer, I want judge_context.py to include LoCoBench-inspired evaluation dimensions (ACS, DTA, CFRD) for ccb_largerepo tasks so that LLM-as-judge can score them.",
      "acceptanceCriteria": [
        "scripts/ccb_metrics/judge_context.py has a new function _locobench_dimensions(task_dir, ir_metrics_path) that returns a dict",
        "The returned dict contains rubrics for: architectural_coherence_score (ACS), dependency_traversal_accuracy (DTA), cross_file_reasoning_depth (CFRD)",
        "Each rubric has: dimension (string), definition (string), scoring_scale '1-5', observable_evidence (list of strings)",
        "The function reads ir_metrics.json from the task run directory (if present) and includes precision/recall/f1/dep_accuracy as evidence",
        "The function reads tests/ground_truth.json from the benchmarks directory (if present) and includes files/dependency_chain as reference",
        "The generate_judge_context function calls _locobench_dimensions for benchmark=='bigcode' tasks and includes it as 'locobench_dimensions' key in the output",
        "python3 -c \"from scripts.ccb_metrics.judge_context import _locobench_dimensions\" imports without error"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Edits scripts/ccb_metrics/judge_context.py only. The existing _read_instruction function already handles benchmark=='bigcode' mapping to ccb_largerepo."
    },
    {
      "id": "US-016",
      "title": "Update largerepo 2-config run script",
      "description": "As a benchmark engineer, I want configs/largerepo_2config.sh updated to run all largerepo tasks (existing + new) in both baseline and SG_full configs.",
      "acceptanceCriteria": [
        "configs/largerepo_2config.sh exists and follows the same pattern as configs/crossrepo_2config.sh",
        "Script sources configs/_common.sh for shared infrastructure",
        "Script runs tasks from configs/selected_benchmark_tasks.json filtered by benchmark=ccb_largerepo",
        "Script supports --parallel N flag for concurrent execution",
        "Script supports --baseline-only and --full-only flags",
        "Script uses 'sourcegraph_full' as the MCP config name (matching existing convention)",
        "bash -n configs/largerepo_2config.sh passes syntax check"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Follow configs/crossrepo_2config.sh as the reference pattern. The existing configs/largerepo_2config.sh may need updating to match current conventions."
    },
    {
      "id": "US-018",
      "title": "Document the LoCoBench adaptation",
      "description": "As a benchmark engineer, I want documentation explaining how LoCoBench methodology was adapted to large repos.",
      "acceptanceCriteria": [
        "benchmarks/ccb_largerepo/docs/LOCOBENCH_ADAPTATION.md exists",
        "Document explains which LoCoBench design principles were adopted (task categories, SE metrics framework)",
        "Document explains which were adapted (verifier approach from keyword overlap to IR metrics, codebase scale from 5.8K to 1M+ LOC)",
        "Document explains which were excluded (synthetic code generation, context window stress testing, code_comprehension/integration_testing/multi_session categories)",
        "Document includes comparison table: LoCoBench original vs our adaptation (codebase size, task count, evaluation approach)",
        "Document maps LCBS dimensions to our implementation: SE->ACS/DTA/CFRD rubrics; FC->compilation+test verifiers; CQ->static analysis; LCU->IR metrics",
        "Document references the paper (arXiv:2509.09614)",
        "benchmarks/ccb_largerepo/docs/REPO_SELECTION.md exists with repo selection rationale (LOC, language, domain, capital markets alignment)"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Creates benchmarks/ccb_largerepo/docs/ directory. Two files: LOCOBENCH_ADAPTATION.md and REPO_SELECTION.md."
    },
    {
      "id": "US-006a",
      "title": "Scaffold 3 architectural understanding tasks (Tier A repos)",
      "description": "As a benchmark engineer, I want 3 architectural understanding tasks for Tier A repos (K8s, Django, PostgreSQL) with ground truth from Sourcegraph research.",
      "acceptanceCriteria": [
        "Directory benchmarks/ccb_largerepo/big-code-k8s-arch-001/ created with task.toml, instruction.md, CLAUDE.md, tests/test.sh, tests/ground_truth.json, environment/Dockerfile",
        "Directory benchmarks/ccb_largerepo/big-code-django-arch-001/ created with same file set",
        "Directory benchmarks/ccb_largerepo/big-code-pg-arch-001/ created with same file set",
        "K8s task: 'Explain the Kubernetes scheduler architecture and how a Pod gets assigned to a Node' - ground truth has 10-15 files spanning pkg/scheduler/",
        "Django task: 'Map the Django ORM query compilation pipeline from QuerySet to SQL' - ground truth has 8-12 files spanning django/db/models/",
        "PostgreSQL task: 'Trace the PostgreSQL query execution pipeline from parse to execute' - ground truth has 10-15 files spanning src/backend/",
        "All task.toml files have category='architectural_understanding', difficulty='hard', time_limit_sec=1200, reward_type='ir_checklist'",
        "All test.sh files source /workspace/tests/verifier_lib.sh (copy from templates/verifier_lib.sh during Dockerfile build)",
        "All ground_truth.json files have confidence='medium' and methodology noting 'LLM-generated via Sourcegraph, requires manual validation'",
        "All ground_truth.json files have 8+ entries in the files array"
      ],
      "priority": 5,
      "passes": true,
      "notes": "Use templates from benchmarks/ccb_largerepo/templates/architectural_understanding/. Research repos using Sourcegraph MCP tools (keyword_search, nls_search, list_files) to identify ground truth files. K8s commit: v1.30.0, Django: 5.2, PostgreSQL: REL_18_2. Copy verifier_lib.sh into each task's tests/ directory."
    },
    {
      "id": "US-006b",
      "title": "Scaffold 3 architectural understanding tasks (Tier B repos)",
      "description": "As a benchmark engineer, I want 3 architectural understanding tasks for capital markets repos (Camel, Flink, QuantLib).",
      "acceptanceCriteria": [
        "Directory benchmarks/ccb_largerepo/big-code-camel-arch-001/ created with full file set",
        "Directory benchmarks/ccb_largerepo/big-code-flink-arch-001/ created with full file set",
        "Directory benchmarks/ccb_largerepo/big-code-quantlib-arch-001/ created with full file set",
        "Camel task: 'Trace how Apache Camel routes a message from endpoint reception through the EIP pipeline to a destination - map the Component->Endpoint->Consumer->Processor->Producer hierarchy'",
        "Flink task: 'Map the Flink checkpoint coordination architecture: how the JobManager triggers a checkpoint, propagates barriers through the task graph, and coordinates acknowledgments'",
        "QuantLib task: 'Trace the QuantLib pricing chain for a barrier option: instrument->pricing engine->term structure->stochastic process->path generator'",
        "All ground_truth.json files have 8+ entries in files array and confidence='medium'",
        "All task.toml files follow architectural_understanding template with correct repo names and language='java' or 'cpp'"
      ],
      "priority": 6,
      "passes": true,
      "notes": "Capital markets repos: apache/camel (camel-4.18.0), apache/flink (release-2.2.0, already indexed), lballabio/QuantLib (v1.41). Use Sourcegraph to research file structures."
    },
    {
      "id": "US-007a",
      "title": "Scaffold 2 cross-file refactoring tasks (Tier A)",
      "description": "As a benchmark engineer, I want 2 cross-file refactoring tasks for large repos where agents must find all affected files.",
      "acceptanceCriteria": [
        "Directory benchmarks/ccb_largerepo/big-code-k8s-refac-001/ created with full file set",
        "Directory benchmarks/ccb_largerepo/big-code-rust-refac-001/ created with full file set",
        "K8s task: refactoring related to a widely-used type or API in the scheduler/controller subsystem",
        "Rust task: refactoring in the Rust compiler related to a type system or diagnostic concept",
        "Ground truth files array has 8-20 entries each (the complete set of files needing modification)",
        "test.sh sources verifier_lib.sh and checks for compilation after changes",
        "All task.toml files have category='cross_file_refactoring'"
      ],
      "priority": 7,
      "passes": true,
      "notes": "K8s: v1.30.0, rust-lang/rust: 1.93.1. Research specific refactoring targets using Sourcegraph find_references to count usages."
    },
    {
      "id": "US-007b",
      "title": "Scaffold 2 cross-file refactoring tasks (Tier B capital markets)",
      "description": "As a benchmark engineer, I want 2 cross-file refactoring tasks targeting capital markets repos with deep inheritance.",
      "acceptanceCriteria": [
        "Directory benchmarks/ccb_largerepo/big-code-strata-refac-001/ created with full file set",
        "Directory benchmarks/ccb_largerepo/big-code-kafka-refac-001/ created with full file set",
        "Strata task: rename or restructure a widely-used financial domain type (exercises Joda-Beans code generation chain, 100+ subclasses)",
        "Kafka task: extract or restructure a widely-used component in the consumer/producer subsystem",
        "Ground truth has 10-25 entries each",
        "All task.toml have category='cross_file_refactoring', language='java'"
      ],
      "priority": 8,
      "passes": true,
      "notes": "OpenGamma/Strata v2.12.65, apache/kafka 4.1.1. Use Sourcegraph find_references to identify widely-referenced types."
    },
    {
      "id": "US-008",
      "title": "Scaffold 3 bug investigation tasks",
      "description": "As a benchmark engineer, I want 3 bug investigation tasks sourced from real closed issues/PRs where agents must trace from symptom to root cause.",
      "acceptanceCriteria": [
        "3 task directories created: big-code-{repo}-bug-001 for 3 different repos",
        "At least 1 task targets a capital markets repo",
        "Each task has entry_point specified in instruction.md",
        "Each ground_truth.json has root_cause_files array identifying the bug's root cause file(s)",
        "Each ground_truth.json has dependency_chain showing the call path from symptom to root cause",
        "Ground truth sourced from real GitHub issues/PRs (methodology field references the issue/PR URL)",
        "test.sh sources verifier_lib.sh and uses root cause MRR scoring",
        "All task.toml have category='bug_investigation'"
      ],
      "priority": 9,
      "passes": true,
      "notes": "Use scripts/mine_bug_tasks.py (US-017) output as candidates, or manually find good bug issues with 5+ file PRs. Good repos: django/django (many labeled bug issues), kubernetes/kubernetes, apache/kafka."
    },
    {
      "id": "US-009",
      "title": "Scaffold 3 security analysis tasks",
      "description": "As a benchmark engineer, I want 3 security analysis tasks where agents must trace data flows and identify attack surfaces.",
      "acceptanceCriteria": [
        "3 task directories created: big-code-{repo}-sec-001 for 3 different repos",
        "At least 1 task targets a capital markets repo (Kafka SASL, Camel JNDI, or PostgreSQL auth)",
        "Each ground_truth.json has entry_points array and data_flow array",
        "Each instruction.md specifies the subsystem to analyze and the type of vulnerability to look for",
        "Suggested tasks: PostgreSQL auth chain (auth.c->crypt.c->role validation), Kafka SASL authentication flow, Django CSRF/session handling",
        "test.sh sources verifier_lib.sh and checks for vulnerability class identification",
        "All task.toml have category='security_analysis'"
      ],
      "priority": 10,
      "passes": true,
      "notes": "Security analysis tasks are analysis-only (no code changes expected). Focus on tracing data flows through the codebase."
    },
    {
      "id": "US-010",
      "title": "Scaffold 3 feature implementation tasks (capital markets focus)",
      "description": "As a benchmark engineer, I want 3 new feature implementation tasks for capital markets repos to supplement existing 4.",
      "acceptanceCriteria": [
        "3 task directories created, at least 2 targeting capital markets repos",
        "Suggested tasks: CreditDefaultSwap in Strata, FIX protocol component in Camel, custom windowing in Flink",
        "Each follows existing largerepo test.sh pattern: compilation check + keyword detection + IR metrics",
        "test.sh includes COMPILE_CMD and FEATURE_KEYWORDS env vars appropriate for each task's language",
        "All task.toml have category='feature_implementation'",
        "Each has ground_truth.json with files array of 5-15 target files",
        "Dockerfiles include appropriate language toolchain (Java JDK 17+ for all three)"
      ],
      "priority": 11,
      "passes": true,
      "notes": "Follow existing big-code-k8s-001 pattern but for Java repos. Strata: v2.12.65, Camel: camel-4.18.0, Flink: release-2.2.0."
    },
    {
      "id": "US-011",
      "title": "Scaffold 2 cross-repo dependency tasks",
      "description": "As a benchmark engineer, I want 2 cross-repo tasks requiring agents to trace dependencies across multiple repositories.",
      "acceptanceCriteria": [
        "2 task directories created following pattern big-code-cross-{ecosystem}-{category}-001",
        "Task 1: K8s ecosystem cross-repo (trace feature through kubernetes/kubernetes + kubernetes/client-go + kubernetes/api) - architectural or refactoring category",
        "Task 2: Capital markets integration (trace data flow through Kafka + Flink or Kafka + Camel) - architectural or security category",
        "Each Dockerfile clones all involved repos into /workspace/{repo-name}/",
        "Each ground_truth.json includes files from ALL involved repos with cross-repo dependency chain",
        "Each CLAUDE.md lists all Sourcegraph repo names the agent should search",
        "task.toml time_limit_sec=1500 (25 min, extra time for cross-repo navigation)"
      ],
      "priority": 12,
      "passes": true,
      "notes": "Cross-repo tasks are the highest MCP-benefit scenario (0.95+). The K8s sub-repos (client-go, api) may need to be added to sg_indexing_list.json."
    },
    {
      "id": "US-013",
      "title": "Ensure all new tasks have Dockerfiles",
      "description": "As a benchmark engineer, I want all scaffolded tasks to have working Dockerfiles that clone repos at pinned commits.",
      "acceptanceCriteria": [
        "Every task directory created by US-006 through US-011 has an environment/Dockerfile",
        "Each Dockerfile uses the appropriate base image: golang:1.23-bookworm (Go), eclipse-temurin:17-jdk (Java), python:3.12-bookworm (Python), rust:1.82-bookworm (Rust), gcc:14-bookworm (C/C++), node:22-bookworm (TypeScript)",
        "Each Dockerfile clones the repo with git clone --filter=blob:none --no-checkout, then git checkout {pinned_commit}",
        "Each Dockerfile copies tests/ directory into /workspace/tests/ (including verifier_lib.sh and ground_truth.json)",
        "Each Dockerfile installs npm and @anthropic-ai/claude-code",
        "Each Dockerfile sets WORKDIR /workspace",
        "bash -n on all test.sh files passes"
      ],
      "priority": 13,
      "passes": true,
      "notes": "Verified: All 23 tasks have Dockerfiles, all test.sh pass syntax check."
    },
    {
      "id": "US-014",
      "title": "Register all new tasks in selected_benchmark_tasks.json and MANIFEST.json",
      "description": "As a benchmark engineer, I want all new tasks registered in the global registries.",
      "acceptanceCriteria": [
        "All new task IDs added to configs/selected_benchmark_tasks.json under benchmark='ccb_largerepo'",
        "Each entry has: task_id, benchmark='ccb_largerepo', sdlc_phase (varies by category), language, difficulty='hard', category (LoCoBench category name), repo, mcp_benefit_score (0.85-0.95), mcp_breakdown, task_dir",
        "MCP breakdown follows existing pattern: context_complexity=0.95, cross_file_deps=0.8, semantic_search_potential=0.9, task_category_weight varies by category",
        "Cross-repo tasks have mcp_benefit_score=0.95 (highest)",
        "benchmarks/ccb_largerepo/MANIFEST.json updated with new task_ids, repos, languages, and task_count",
        "python3 -c \"import json; d=json.load(open('configs/selected_benchmark_tasks.json')); lr=[t for t in d if t['benchmark']=='ccb_largerepo']; print(f'{len(lr)} largerepo tasks')\" shows 20+ tasks"
      ],
      "priority": 14,
      "passes": true,
      "notes": "Completed: 19 new tasks registered (23 total). 5 repos need SG mirror push (kafka, camel, Strata, QuantLib, rust) before SG_full runs."
    }
  ]
}
