{
  "project": "CodeContextBench",
  "branchName": "ralph/vm-docker-fixes-reruns",
  "description": "VM Operations: Diagnose and fix Docker environments for 29 errored SWE-bench Pro tasks, re-run fixed tasks, then run 3x trials across all configs for variance estimation. Beads: CodeContextBench-yvl, CodeContextBench-lrs, CodeContextBench-c5c.",
  "userStories": [
    {
      "id": "US-001",
      "title": "Diagnose SWE-bench Pro Docker failures by error group",
      "description": "As a benchmark operator, I need to understand exactly why each group of tasks errored so I can write targeted fixes.",
      "acceptanceCriteria": [
        "Examine result.json for each of the 29 errored task runs at ~/evals/custom_agents/agents/claudecode/runs/official/",
        "Group by root cause: protonmail (10 runs: 4 BL + 4 SB + 2 SF), qutebrowser (6 runs: 3 BL + 3 SB), Teleport (7 runs: 4 BL + 3 SB), vuls (3 runs: 1 BL + 2 SB), element-hq (1 SB), tutao (2 runs: 1 BL + 1 SB), nodebb (1 SB)",
        "For each group: extract the exact error message from result.json 'error' field",
        "Check Docker build logs for environment setup failures",
        "Classify each as: (a) Node.js version incompatibility, (b) Docker build timeout, (c) rate limiting, (d) missing system dependency, (e) other",
        "Document findings in a diagnosis file: task_name, config(s), error message, root cause, proposed fix",
        "Note which errors are shared across configs (same Docker issue) vs config-specific"
      ],
      "priority": 1,
      "passes": true,
      "notes": "COMPLETED 2026-02-11. Found 42 infra failures (not 29): 25 rate-limit, 10 Node.js version conflict (protonmail), 4 setup timeout, 3 transcript corruption. Only protonmail needs Docker fix. Others just need re-run with fresh subscription. See docs/swebenchpro_docker_diagnosis.md."
    },
    {
      "id": "US-002",
      "title": "Fix protonmail Docker environment (10 errored runs)",
      "description": "As a benchmark operator, I need the protonmail/webclients Docker environment to build and run successfully so 10 errored runs can be recovered.",
      "acceptanceCriteria": [
        "Identify the Node.js version required by protonmail/webclients (likely Node 18+ based on package.json)",
        "Update the Dockerfile in the protonmail task's environment/ directory",
        "Ensure npm install / yarn install completes without errors",
        "Test the Docker build locally: docker build -f environment/Dockerfile -t protonmail-test .",
        "Run a smoke test: harbor run with nop agent to verify environment starts",
        "Document the fix applied"
      ],
      "priority": 2,
      "passes": false,
      "notes": "Protonmail tasks are TypeScript/JavaScript. The webclients monorepo likely needs Node 18+ and specific system dependencies. Check if the base image provides the right Node version. Harbor runs Docker build from the task root directory (not environment/). Use --timeout-multiplier 10 for first build."
    },
    {
      "id": "US-003",
      "title": "Fix remaining Docker environments (qutebrowser, Teleport, vuls, others)",
      "description": "As a benchmark operator, I need all remaining errored task environments fixed so the full SWE-bench Pro suite can run cleanly.",
      "acceptanceCriteria": [
        "Fix qutebrowser Docker environment (likely Qt/Python dependency issue)",
        "Fix Teleport environment (likely rate limiting — may need auth token or mirror)",
        "Fix vuls environment",
        "Fix element-hq, tutao, nodebb environments",
        "Test each fix with nop agent smoke test via Harbor",
        "For rate-limiting issues (Teleport): document workaround (retry, auth token, or mirror repo)",
        "Track which fixes are environment-only vs require task.yaml changes"
      ],
      "priority": 3,
      "passes": false,
      "notes": "Prioritize by run count: qutebrowser (6 runs) > Teleport (7 runs) > vuls (3 runs) > others (1-2 runs each). Some may not be fixable (e.g., if Teleport requires enterprise auth). Document unfixable tasks separately. These are SWE-bench Pro tasks so the Dockerfile comes from the SWE-bench dataset — may need to patch cached tasks at ~/.cache/harbor/tasks/."
    },
    {
      "id": "US-004",
      "title": "Re-run fixed errored tasks with all 3 configs",
      "description": "As a benchmark operator, I need to re-run all previously-errored tasks that now have fixed environments to fill in the missing data.",
      "acceptanceCriteria": [
        "For each fixed task: run with BL (baseline), SG_base (sourcegraph_base), SG_full (strategic_deep_search)",
        "Use the same model: anthropic/claude-haiku-4-5-20251001",
        "Use Daytona environment: --env daytona",
        "Store results in the official runs directory alongside existing results",
        "Verify each run completes (agent_task_seconds > 30s, no error in result.json)",
        "Update analysis.csv with new results",
        "Run scripts/recompute_analysis.py to regenerate corrected aggregations",
        "Harbor command: harbor run --path benchmarks/swebenchpro/<task-id> --agent-import-path agents.mcp_variants:<AgentClass> --model anthropic/claude-haiku-4-5-20251001 --env daytona -n 1"
      ],
      "priority": 4,
      "passes": false,
      "notes": "Source env first: source .env.local && export ANTHROPIC_API_KEY DAYTONA_API_KEY SOURCEGRAPH_ACCESS_TOKEN SOURCEGRAPH_URL. Agent classes: BaselineClaudeCodeAgent (BL), SourcegraphBaseAgent (SG_base), StrategicDeepSearchAgent (SG_full). Run one task at a time to catch failures early. Skip tasks whose environments couldn't be fixed in US-003."
    },
    {
      "id": "US-005",
      "title": "Run 3x SWE-bench Pro trials for variance estimation",
      "description": "As an analyst, I need 3 independent trials of SWE-bench Pro with each config to compute confidence intervals and assess claim reliability.",
      "acceptanceCriteria": [
        "Run 3 complete trials of all non-errored SWE-bench Pro tasks with each of the 3 configs (BL, SG_base, SG_full)",
        "That's 9 total trial runs (3 configs x 3 trials)",
        "Each trial must use the same model and environment settings as the official run",
        "Store trial results separately from official results (e.g., runs/trials/swebenchpro_trial_{1,2,3}/)",
        "After all trials complete: compute per-task reward variance across trials",
        "Compute bootstrap 95% CIs for aggregate deltas using trial data",
        "Document which tasks show high variance (reward differs across trials) vs stable tasks"
      ],
      "priority": 5,
      "passes": false,
      "notes": "This is the most expensive operation (~9 * 15-30 tasks * $0.50-2.00 per task). Consider starting with a pilot of 5 representative tasks x 3 trials to estimate variance before committing to full runs. SWE-bench Pro has 30 tasks (after error exclusion). Expected total: ~270 task runs. Depends on US-004 completing first so the task set is stable."
    },
    {
      "id": "US-006",
      "title": "Run 3x LoCoBench and TAC trials for variance estimation",
      "description": "As an analyst, I need variance data for LoCoBench and TAC benchmarks to assess whether their deltas are reliable.",
      "acceptanceCriteria": [
        "Run 3 trials of LoCoBench (50 tasks) with each config — focus on SG_full vs BL delta",
        "Run 3 trials of TAC (TheAgentCompany) tasks with each config",
        "Store results in runs/trials/locobench_trial_{1,2,3}/ and runs/trials/tac_trial_{1,2,3}/",
        "Compute per-task variance and aggregate CIs",
        "LoCoBench baseline is 0.448, SG_full is 0.452 (delta +0.9%) — determine if this is noise",
        "TAC tasks: verify the high-performing categories maintain their edge across trials"
      ],
      "priority": 6,
      "passes": false,
      "notes": "LoCoBench is the cheapest to trial (lower token usage). TAC may have higher variance due to complex multi-step tasks. Run LoCoBench first as it provides the fastest signal on whether +0.9% is real. If LoCoBench delta is noise, the narrative shifts to SG_full helping only on specific task types, which is actually a stronger and more defensible claim."
    }
  ]
}
