{
  "project": "CodeContextBench",
  "branchName": "ralph/vm-docker-fixes-reruns",
  "description": "VM Operations: Diagnose and fix Docker environments for 29 errored SWE-bench Pro tasks, re-run fixed tasks, then run 3x trials across all configs for variance estimation. Beads: CodeContextBench-yvl, CodeContextBench-lrs, CodeContextBench-c5c.",
  "userStories": [
    {
      "id": "US-001",
      "title": "Diagnose SWE-bench Pro Docker failures by error group",
      "description": "As a benchmark operator, I need to understand exactly why each group of tasks errored so I can write targeted fixes.",
      "acceptanceCriteria": [
        "Examine result.json for each of the 29 errored task runs at ~/evals/custom_agents/agents/claudecode/runs/official/",
        "Group by root cause: protonmail (10 runs: 4 BL + 4 SB + 2 SF), qutebrowser (6 runs: 3 BL + 3 SB), Teleport (7 runs: 4 BL + 3 SB), vuls (3 runs: 1 BL + 2 SB), element-hq (1 SB), tutao (2 runs: 1 BL + 1 SB), nodebb (1 SB)",
        "For each group: extract the exact error message from result.json 'error' field",
        "Check Docker build logs for environment setup failures",
        "Classify each as: (a) Node.js version incompatibility, (b) Docker build timeout, (c) rate limiting, (d) missing system dependency, (e) other",
        "Document findings in a diagnosis file: task_name, config(s), error message, root cause, proposed fix",
        "Note which errors are shared across configs (same Docker issue) vs config-specific"
      ],
      "priority": 1,
      "passes": true,
      "notes": "COMPLETED 2026-02-11. Found 42 infra failures (not 29): 25 rate-limit, 10 Node.js version conflict (protonmail), 4 setup timeout, 3 transcript corruption. Only protonmail needs Docker fix. Others just need re-run with fresh subscription. See docs/swebenchpro_docker_diagnosis.md."
    },
    {
      "id": "US-002",
      "title": "Fix protonmail Docker environment (10 errored runs)",
      "description": "As a benchmark operator, I need the protonmail/webclients Docker environment to build and run successfully so 10 errored runs can be recovered.",
      "acceptanceCriteria": [
        "Identify the Node.js version required by protonmail/webclients (likely Node 18+ based on package.json)",
        "Update the Dockerfile in the protonmail task's environment/ directory",
        "Ensure npm install / yarn install completes without errors",
        "Test the Docker build locally: docker build -f environment/Dockerfile -t protonmail-test .",
        "Run a smoke test: harbor run with nop agent to verify environment starts",
        "Document the fix applied"
      ],
      "priority": 2,
      "passes": true,
      "notes": "COMPLETED 2026-02-11. Fixed Node.js v16→v18 upgrade in all 4 protonmail Dockerfiles (local + Harbor cache = 8 files). Base images have Node 16 at /usr/local/bin/node which shadows apk's Node 18 at /usr/bin/node. Fix: apk del + rm old binaries + apk add fresh. All 8 images build, Node 18.20.1 confirmed, Claude Code installs cleanly. See docs/swebenchpro_docker_diagnosis.md § Fix Applied."
    },
    {
      "id": "US-003",
      "title": "Fix remaining Docker environments (qutebrowser, Teleport, vuls, others)",
      "description": "As a benchmark operator, I need all remaining errored task environments fixed so the full SWE-bench Pro suite can run cleanly.",
      "acceptanceCriteria": [
        "Fix qutebrowser Docker environment (likely Qt/Python dependency issue)",
        "Fix Teleport environment (likely rate limiting — may need auth token or mirror)",
        "Fix vuls environment",
        "Fix element-hq, tutao, nodebb environments",
        "Test each fix with nop agent smoke test via Harbor",
        "For rate-limiting issues (Teleport): document workaround (retry, auth token, or mirror repo)",
        "Track which fixes are environment-only vs require task.yaml changes"
      ],
      "priority": 3,
      "passes": true,
      "notes": "COMPLETED 2026-02-11. No Docker fixes needed for any of the 7 non-protonmail repos. All 32 infra failures are OAuth rate-limit hits (0 output tokens, 0-8s execution, clustered in Feb 8-9 batches). Every repo has passing runs confirming Docker environments work: qutebrowser 6P/0F, teleport 5P/3F, vuls 5P/3F, element-hq 5P/0F, tutanota 1P/0F, nodebb 8P/4F, internetarchive 21P/0F. Fix = re-run with fresh subscription tokens. ~33 task re-runs needed (no code changes). See docs/swebenchpro_docker_diagnosis.md § US-003 Assessment."
    },
    {
      "id": "US-004",
      "title": "Re-run fixed errored tasks with all 3 configs",
      "description": "As a benchmark operator, I need to re-run all previously-errored tasks that now have fixed environments to fill in the missing data.",
      "acceptanceCriteria": [
        "For each fixed task: run with BL (baseline), SG_base (sourcegraph_base), SG_full (strategic_deep_search)",
        "Use the same model: anthropic/claude-haiku-4-5-20251001",
        "Use Daytona environment: --env daytona",
        "Store results in the official runs directory alongside existing results",
        "Verify each run completes (agent_task_seconds > 30s, no error in result.json)",
        "Update analysis.csv with new results",
        "Run scripts/recompute_analysis.py to regenerate corrected aggregations",
        "Harbor command: harbor run --path benchmarks/swebenchpro/<task-id> --agent-import-path agents.mcp_variants:<AgentClass> --model anthropic/claude-haiku-4-5-20251001 --env daytona -n 1"
      ],
      "priority": 4,
      "passes": true,
      "notes": "COMPLETED 2026-02-12. Ran 34 task-config pairs across all 3 configs using opus model via single-account subscription mode. Results: BL 9/13 passed (69%), SG_base 13/16 passed (81%), SG_full 3/5 passed (60%). Zero infra failures — all failures are genuine task difficulty. Two remaining errors: nodebb-eb49a649 SG_base (Docker compose failure) and protonmail-8be4 SG_full (token refresh 403). MANIFEST updated: BL 0.390→0.771, SG_base 0.317→0.765, SG_full 0.694→0.824. Run dir: runs/official/swebenchpro_rerun_opus_20260211_235834/. Script: configs/swebenchpro_rerun_infra_failures.sh."
    },
    {
      "id": "US-005",
      "title": "Run 3x SWE-bench Pro trials for variance estimation",
      "description": "As an analyst, I need 3 independent trials of SWE-bench Pro with each config to compute confidence intervals and assess claim reliability.",
      "acceptanceCriteria": [
        "Run 3 complete trials of all non-errored SWE-bench Pro tasks with each of the 3 configs (BL, SG_base, SG_full)",
        "That's 9 total trial runs (3 configs x 3 trials)",
        "Each trial must use the same model and environment settings as the official run",
        "Store trial results separately from official results (e.g., runs/trials/swebenchpro_trial_{1,2,3}/)",
        "After all trials complete: compute per-task reward variance across trials",
        "Compute bootstrap 95% CIs for aggregate deltas using trial data",
        "Document which tasks show high variance (reward differs across trials) vs stable tasks"
      ],
      "priority": 5,
      "passes": false,
      "notes": "This is the most expensive operation (~9 * 15-30 tasks * $0.50-2.00 per task). Consider starting with a pilot of 5 representative tasks x 3 trials to estimate variance before committing to full runs. SWE-bench Pro has 30 tasks (after error exclusion). Expected total: ~270 task runs. Depends on US-004 completing first so the task set is stable."
    },
    {
      "id": "US-006",
      "title": "Run 3x LoCoBench and TAC trials for variance estimation",
      "description": "As an analyst, I need variance data for LoCoBench and TAC benchmarks to assess whether their deltas are reliable.",
      "acceptanceCriteria": [
        "Run 3 trials of LoCoBench (50 tasks) with each config — focus on SG_full vs BL delta",
        "Run 3 trials of TAC (TheAgentCompany) tasks with each config",
        "Store results in runs/trials/locobench_trial_{1,2,3}/ and runs/trials/tac_trial_{1,2,3}/",
        "Compute per-task variance and aggregate CIs",
        "LoCoBench baseline is 0.448, SG_full is 0.452 (delta +0.9%) — determine if this is noise",
        "TAC tasks: verify the high-performing categories maintain their edge across trials"
      ],
      "priority": 6,
      "passes": false,
      "notes": "LoCoBench is the cheapest to trial (lower token usage). TAC may have higher variance due to complex multi-step tasks. Run LoCoBench first as it provides the fastest signal on whether +0.9% is real. If LoCoBench delta is noise, the narrative shifts to SG_full helping only on specific task types, which is actually a stronger and more defensible claim."
    }
  ]
}
