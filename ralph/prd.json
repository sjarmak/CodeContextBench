{
  "project": "CodeContextBench",
  "branchName": "ralph/benchmark-execution-pipeline",
  "repoPath": "/home/stephanie_jarmak/CodeContextBench",
  "description": "Benchmark Execution Pipeline - Standardize agent configs, create comparison configs, build metrics extraction pipeline with deterministic measurements and LLM judge context generation, and push benchmarks repo to GitHub",
  "userStories": [
    {
      "id": "US-001",
      "title": "Document the 3-config matrix with tool lists",
      "description": "As a researcher, I want a CONFIGS.md documenting exactly which MCP tools are available in each of the 3 agent configurations so the paper methodology is precise.",
      "acceptanceCriteria": [
        "Create docs/CONFIGS.md in the CodeContextBench repo",
        "Document 3 configs: Baseline (BASELINE_MCP_TYPE=none), MCP-NoDeepSearch (sourcegraph_no_deepsearch), MCP-Full (sourcegraph_hybrid)",
        "For each config, list the exact Sourcegraph MCP tools available (e.g., sg_keyword_search, sg_nls_search, sg_deepsearch, sg_read_file) by reading the tool setup logic in ~/evals/custom_agents/agents/claudecode/agents/claude_baseline_agent.py",
        "Include a mapping table: Paper Config Name -> BASELINE_MCP_TYPE value -> Tools Available",
        "Note that sourcegraph_no_deepsearch connects to same endpoint but excludes sg_deepsearch from --allowedTools"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Read claude_baseline_agent.py lines ~100-260 and ~890-950 to extract exact tool lists per mode"
    },
    {
      "id": "US-002",
      "title": "Create LoCoBench 3-config comparison YAML",
      "description": "As a researcher, I want a YAML config that runs all 50 LoCoBench tasks across all 3 agent configs so I can execute the full comparison with one command.",
      "acceptanceCriteria": [
        "Create configs/locobench_3config.yaml by copying configs/locobench_50_tasks_comparison.yaml from ~/evals/custom_agents/agents/claudecode/configs/",
        "Change mcp_modes from [baseline, deepsearch_hybrid] to [baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid]",
        "Set model to anthropic/claude-opus-4-5-20251101 (pinned, not auto-resolving)",
        "Set run_category to official",
        "Keep all 50 task IDs unchanged",
        "Create corresponding configs/locobench_3config.sh shell runner if the original had one, updating MCP mode list to match"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Source config at ~/evals/custom_agents/agents/claudecode/configs/locobench_50_tasks_comparison.yaml and .sh"
    },
    {
      "id": "US-003",
      "title": "Create SWE-Bench Pro 3-config comparison YAML",
      "description": "As a researcher, I want a YAML config for SWE-Bench Pro across all 3 configs.",
      "acceptanceCriteria": [
        "Create configs/swebenchpro_3config.yaml by copying configs/swebenchpro_50_tasks_comparison.yaml from ~/evals/custom_agents/agents/claudecode/configs/",
        "Change mcp_modes from [baseline, sourcegraph_hybrid] to [baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid]",
        "Set model to anthropic/claude-opus-4-5-20251101",
        "Set run_category to official",
        "Keep all task IDs unchanged",
        "Create corresponding .sh runner if original had one"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Source config at ~/evals/custom_agents/agents/claudecode/configs/swebenchpro_50_tasks_comparison.yaml"
    },
    {
      "id": "US-004",
      "title": "Create BigCode MCP 3-config comparison YAML",
      "description": "As a researcher, I want a YAML config for BigCode MCP across all 3 configs.",
      "acceptanceCriteria": [
        "Create configs/bigcode_3config.yaml by copying configs/bigcode_mcp_comparison.yaml from ~/evals/custom_agents/agents/claudecode/configs/",
        "Change mcp_modes from [baseline, sourcegraph_hybrid] to [baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid]",
        "Set model to anthropic/claude-opus-4-5-20251101",
        "Set run_category to official",
        "Set concurrency to 1 (serial execution required for large repos)",
        "Keep all 4 task IDs unchanged",
        "Create corresponding .sh runner if original had one"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Source config at ~/evals/custom_agents/agents/claudecode/configs/bigcode_mcp_comparison.yaml and .sh"
    },
    {
      "id": "US-005",
      "title": "Create K8s Docs 3-config comparison YAML",
      "description": "As a researcher, I want a YAML config for K8s Docs so these tasks run through the standard Harbor pipeline instead of ad-hoc individual jobs.",
      "acceptanceCriteria": [
        "Create configs/k8s_docs_3config.yaml with mcp_modes [baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid]",
        "Use locobench_50_tasks_comparison.yaml as structural template since K8s Docs also uses local task paths (not Harbor registry)",
        "Set benchmark path to /home/stephanie_jarmak/CodeContextBench/benchmarks/kubernetes_docs",
        "Include all 5 task IDs: pkg-doc-001, client-go-doc-001, applyconfig-doc-001, apiserver-doc-001, fairqueuing-doc-001",
        "Set model to anthropic/claude-opus-4-5-20251101",
        "Set run_category to official",
        "Create corresponding .sh shell runner",
        "Set timeout appropriately (900s per task based on task.toml time_limit_sec)"
      ],
      "priority": 5,
      "passes": true,
      "notes": "K8s Docs tasks are at /home/stephanie_jarmak/CodeContextBench/benchmarks/kubernetes_docs/. Each has task.toml, instruction.md, tests/test.sh, ground_truth/. Reference locobench config for local-path benchmark format."
    },
    {
      "id": "US-006",
      "title": "Diagnose BigCode MCP empty results",
      "description": "As a researcher, I want to understand why BigCode MCP runs produce 0 task directories so I can fix it before the official run.",
      "acceptanceCriteria": [
        "Read the BigCode shell runner at ~/evals/custom_agents/agents/claudecode/configs/bigcode_mcp_comparison.sh",
        "Read Harbor logs in runs/official/bigcode_mcp_opus_20260131_130446/baseline/ for error messages",
        "Read the job.log files in each batch directory (e.g., 2026-01-31__13-04-54/)",
        "Check if task.toml files in /home/stephanie_jarmak/CodeContextBench/benchmarks/big_code_mcp/ are being found by Harbor",
        "Write a diagnosis report to docs/BIGCODE_DIAGNOSIS.md documenting: the failure mode, root cause, and recommended fix",
        "If the fix is a config change (not code change), apply it to configs/bigcode_3config.yaml created in US-004"
      ],
      "priority": 6,
      "passes": true,
      "notes": "Runs at ~/evals/custom_agents/agents/claudecode/runs/official/bigcode_mcp_opus_20260131_130446/. All 3 modes (baseline, deepsearch_hybrid, sourcegraph_hybrid) show 0 task dirs in batch directories."
    },
    {
      "id": "US-007",
      "title": "Create run metrics data model",
      "description": "As a researcher, I want a Python data model for per-task and per-run metrics so the extraction pipeline has a clear schema to populate.",
      "acceptanceCriteria": [
        "Create scripts/ccb_metrics/__init__.py and scripts/ccb_metrics/models.py in the CodeContextBench repo",
        "Define TaskMetrics dataclass with fields: task_id (str), benchmark (str), config_name (str), reward (float|None), partial_score (float|None), status (str: passed/failed/error), wall_clock_seconds (float|None), agent_execution_seconds (float|None), environment_setup_seconds (float|None), verifier_seconds (float|None), input_tokens (int|None), output_tokens (int|None), cache_creation_tokens (int|None), cache_read_tokens (int|None), cost_usd (float|None), tool_calls_total (int|None), tool_calls_mcp (int|None), tool_calls_local (int|None), tool_calls_by_name (dict|None), mcp_ratio (float|None), files_modified (int|None), lines_added (int|None), lines_removed (int|None)",
        "Define RunMetrics dataclass with fields: run_id (str), benchmark (str), config_name (str), model (str), timestamp (str), task_count (int), tasks (list[TaskMetrics]), and computed properties: mean_reward, mean_partial_score, pass_rate, mean_tokens, mean_wall_clock, mean_mcp_ratio",
        "Define EvalReport dataclass with fields: report_id (str), generated_at (str), runs (list[RunMetrics]), and a method configs() returning unique config names and benchmarks() returning unique benchmark names",
        "All dataclasses have to_dict() and from_dict() class methods for JSON serialization",
        "Include a to_json(path) method on EvalReport that writes the full report to a JSON file"
      ],
      "priority": 7,
      "passes": false,
      "notes": "This schema unifies what the runner's metrics_extractor.py, metrics_aggregator.py, and harbor_parser.py each extract into a single canonical format. Keep it simple — no inheritance hierarchies. Stdlib only (dataclasses, json, pathlib, typing, datetime, statistics). No pandas, no external deps. The whole ccb_metrics package must be runnable with plain python3 3.10+."
    },
    {
      "id": "US-008",
      "title": "Build core metrics extractor from Harbor result.json",
      "description": "As a researcher, I want to extract token counts, timing, reward, and cost from Harbor result.json files into the TaskMetrics model.",
      "acceptanceCriteria": [
        "Create scripts/ccb_metrics/extractors.py",
        "Implement extract_task_from_result_json(result_json_path) -> TaskMetrics that reads a Harbor result.json and populates: task_id, reward (from verifier_result.rewards.reward), input/output/cache tokens (from agent_result.n_input_tokens etc.), cost_usd (from agent_result.cost_usd), status (passed if reward > 0 else failed), timing fields (from started_at/finished_at timestamps in agent_execution, environment_setup, verifier sections)",
        "Implement extract_task_tokens_from_transcript(claude_code_txt_path) -> dict as fallback when result.json lacks token data — parse the JSONL claude-code.txt, find the final entry with type='result', extract usage.input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens",
        "Implement extract_swebench_partial_score(test_stdout_path) -> float that parses verifier/test-stdout.txt and returns (required_tests_passed / required_tests) as partial_score",
        "Implement extract_reward_from_file(reward_txt_path) -> float as fallback for reward.txt",
        "All extractors handle missing/malformed files gracefully by returning None for missing fields",
        "Test by running on one task from ~/evals/custom_agents/agents/claudecode/runs/official/locobench_50_tasks_20260127_170300/baseline/ and verifying non-None values for reward, tokens, and timing"
      ],
      "priority": 8,
      "passes": false,
      "notes": "Harbor result.json has: verifier_result.rewards.reward, agent_result.{n_input_tokens, n_output_tokens, n_cache_tokens, cost_usd}, timing sections with ISO timestamps. claude-code.txt is JSONL where last 'result' type entry has usage totals. SWE-bench test-stdout.txt has 'Required tests: N' and 'Required tests that passed: M' lines."
    },
    {
      "id": "US-009",
      "title": "Build tool usage extractor from trajectory and transcript",
      "description": "As a researcher, I want tool call counts categorized as MCP vs local extracted from agent traces so I can analyze tool utilization patterns.",
      "acceptanceCriteria": [
        "Add extract_tool_usage_from_trajectory(trajectory_json_path) -> dict to scripts/ccb_metrics/extractors.py",
        "Parse ATIF v1.2 trajectory.json: iterate steps[].tool_calls[].function_name, count each tool name",
        "Categorize tools: MCP tools match pattern 'mcp__*' (e.g., mcp__sourcegraph__sg_keyword_search), local tools are Bash/Read/Edit/Write/Grep/Glob/Task/TaskOutput",
        "Return dict with keys: tool_calls_total, tool_calls_mcp, tool_calls_local, tool_calls_by_name (Counter dict), mcp_ratio (mcp/total)",
        "Add fallback extract_tool_usage_from_transcript(claude_code_txt_path) -> dict that parses JSONL claude-code.txt, finds entries with type='assistant' that have tool_use content blocks, counts tool names",
        "Fallback uses same categorization logic",
        "Both functions return None-valued dict if file is missing or unparseable",
        "Test on one LoCoBench baseline task (should have 0 MCP calls) and verify tool_calls_mcp == 0"
      ],
      "priority": 9,
      "passes": false,
      "notes": "ATIF v1.2 format: {steps: [{tool_calls: [{function_name: 'Bash'}], metrics: {...}}]}. claude-code.txt JSONL format: {type: 'assistant', message: {content: [{type: 'tool_use', name: 'Bash', input: {...}}]}}. MCP tools have mcp__ prefix."
    },
    {
      "id": "US-010",
      "title": "Build run discovery and batch extraction",
      "description": "As a researcher, I want to point the extractor at a runs/official/ directory and have it discover all runs, configs, and tasks automatically.",
      "acceptanceCriteria": [
        "Add discover_runs(runs_dir) -> list[RunMetrics] to scripts/ccb_metrics/discovery.py",
        "Walks the standard Harbor output structure: runs_dir/<run_name>/<config_name>/<batch_timestamp>/<task_id>__<hash>/",
        "Infers benchmark name from run_name (e.g., 'locobench_50_tasks_20260127_170300' -> 'locobench')",
        "Infers config_name from the directory name (e.g., 'baseline', 'sourcegraph_hybrid', 'deepsearch')",
        "For each task dir, calls extract_task_from_result_json() and extract_tool_usage_from_trajectory() to populate TaskMetrics",
        "Falls back to extract_reward_from_file() and extract_task_tokens_from_transcript() when result.json lacks data",
        "For SWE-Bench Pro tasks (detected by 'swebench' in run_name), also calls extract_swebench_partial_score()",
        "Groups tasks into RunMetrics by (benchmark, config_name)",
        "Extracts model from config.json in batch directory if available",
        "Returns list of RunMetrics sorted by (benchmark, config_name)",
        "Test by running on ~/evals/custom_agents/agents/claudecode/runs/official/ and verifying it discovers locobench baseline (50 tasks) and deepsearch (50 tasks)"
      ],
      "priority": 10,
      "passes": false,
      "notes": "Directory structure: runs/official/<run_name>/<config>/<timestamp>/<task_id__hash>/. Some runs have reward.txt in verifier/ subdir, others have it in result.json. Need to handle both."
    },
    {
      "id": "US-011",
      "title": "Build evaluation report generator (deterministic metrics)",
      "description": "As a researcher, I want a CLI that generates a comprehensive evaluation report from all discovered runs, producing markdown tables and a JSON report file.",
      "acceptanceCriteria": [
        "Create scripts/generate_eval_report.py as the CLI entry point using argparse",
        "Accepts --runs-dir (default: ~/evals/custom_agents/agents/claudecode/runs/official/), --output-dir (default: ./eval_reports/), and --csv flag",
        "Script is executable with: python3 scripts/generate_eval_report.py --runs-dir /path/to/runs/official/ --output-dir ./eval_reports/",
        "python3 scripts/generate_eval_report.py --help prints usage with all arguments described",
        "Calls discover_runs() to find all run data",
        "Wraps results in EvalReport and writes eval_report.json to output dir",
        "Generates REPORT.md in output dir with these tables: (1) Run Inventory — benchmark, config, model, task_count, timestamp; (2) Aggregate Performance — per-config mean reward and pass rate across all benchmarks; (3) Per-Benchmark Breakdown — mean reward per benchmark x config matrix; (4) Efficiency — mean input_tokens, output_tokens, cache_tokens, wall_clock_seconds, cost_usd per config x benchmark; (5) Tool Utilization — mean tool_calls_total, tool_calls_mcp, tool_calls_local, mcp_ratio per config x benchmark; (6) SWE-Bench Pro Partial Scores — mean partial_score per config (if SWE-bench data exists)",
        "Each table is a proper markdown table with aligned columns",
        "Also writes tables as CSV files (one per table) in output dir for downstream analysis",
        "Prints summary to stdout: benchmarks found, configs found, total tasks, top-level pass rates",
        "Test by running on existing official runs and verifying REPORT.md is generated with LoCoBench data"
      ],
      "priority": 11,
      "passes": false,
      "notes": "This is the deterministic-only report. LLM judge evaluation is a separate step that consumes context files generated by US-012. CLI invocation: python3 scripts/generate_eval_report.py --runs-dir ~/evals/custom_agents/agents/claudecode/runs/official/ --output-dir ./eval_reports/. No external dependencies — stdlib only (json, dataclasses, pathlib, statistics, csv, datetime, argparse). The script must be runnable from the CodeContextBench repo root with plain python3."
    },
    {
      "id": "US-012",
      "title": "Generate per-task LLM judge context files",
      "description": "As a researcher, I want a JSON context file generated for each task that contains everything an LLM judge needs to evaluate quality — the task instructions, agent output, ground truth (if available), tool usage summary, and run metadata.",
      "acceptanceCriteria": [
        "Add scripts/ccb_metrics/judge_context.py with function generate_judge_contexts(runs_dir, benchmarks_dir, output_dir)",
        "For each task in each run, generates a JSON file at output_dir/<benchmark>/<config>/<task_id>_judge_context.json",
        "Each context file contains: task_id, benchmark, config_name, model, reward, partial_score (if applicable), task_instructions (read from instruction.md in the benchmark task directory), agent_transcript_summary (first 200 + last 100 lines of agent/claude-code.txt), agent_output (the final agent output or solution — from agent/solution.md if it exists, else last assistant message from transcript), ground_truth (read from ground_truth/ directory in benchmark task if it exists, else null), tool_usage_summary (from extractors: total calls, MCP calls, top 5 tools by count), code_changes (list of files modified with lines added/removed, extracted from Edit/Write tool calls in trajectory or transcript), verifier_output (from verifier/test-stdout.txt or verifier/reward.txt), run_metadata (model, config_name, benchmark, timestamp)",
        "The benchmarks_dir parameter points to /home/stephanie_jarmak/CodeContextBench/benchmarks/ and is used to locate instruction.md and ground_truth/ for each task by matching task_id to benchmark directory names",
        "Handles missing files gracefully — fields are null when source data is unavailable",
        "Generates an index file at output_dir/judge_contexts_index.json listing all generated context files with task_id, benchmark, config, and path",
        "Also callable as CLI: python3 -m scripts.ccb_metrics.judge_context --runs-dir <path> --benchmarks-dir ./benchmarks/ --output-dir ./judge_contexts/"
      ],
      "priority": 12,
      "passes": false,
      "notes": "CLI invocation: python3 -m scripts.ccb_metrics.judge_context --runs-dir ~/evals/custom_agents/agents/claudecode/runs/official/ --benchmarks-dir ./benchmarks/ --output-dir ./judge_contexts/. Also importable: from scripts.ccb_metrics.judge_context import generate_judge_contexts. Key mapping: task_id in run dirs maps to benchmark dirs by prefix (e.g., 'pkg-doc-001' matches benchmarks/kubernetes_docs/pkg-doc-001/). For LoCoBench, task dirs under benchmarks/locobench_agent/tasks/ match by full task name. For SWE-bench, instruction comes from Harbor's dataset, not local files."
    },
    {
      "id": "US-013",
      "title": "Add harness configuration capture to report",
      "description": "As a researcher, I want the evaluation report to include the exact harness configuration used for each run — model version, MCP mode, Harbor parameters, agent import path — so results are fully reproducible.",
      "acceptanceCriteria": [
        "Add extract_run_config(batch_dir) -> dict to scripts/ccb_metrics/extractors.py",
        "Reads config.json from the batch directory (e.g., runs/official/<run>/<config>/2026-*/config.json)",
        "Extracts: model_name (from agent.model_name), agent_import_path (from agent.import_path), timeout_multiplier (from config), mcp_mode (from environment variable or inferred from config), task_source (git_url or local path)",
        "Also reads the agent init line from claude-code.txt (first JSONL line with type='system' subtype='init') to capture: claude_code_version, permissionMode, tools list, mcp_servers list, model",
        "Adds a HarnessConfig section to EvalReport model (or as a dict on RunMetrics) with all captured fields",
        "The REPORT.md Run Inventory table includes columns for model and mcp_mode from this config data",
        "Writes a separate harness_configs.json to output dir with the full config per run"
      ],
      "priority": 13,
      "passes": false,
      "notes": "config.json is in the batch timestamp dir. claude-code.txt init line has {type: 'system', subtype: 'init', tools: [...], mcp_servers: [...], model: '...', permissionMode: '...', claude_code_version: '...'}. This is critical for reproducibility."
    },
    {
      "id": "US-014",
      "title": "Update README and push benchmarks repo to GitHub",
      "description": "As a researcher, I want the CodeContextBench benchmarks repo on GitHub with an updated README so it serves as the paper's reproducibility artifact.",
      "acceptanceCriteria": [
        "Update README.md with: project description referencing the paper title, benchmark suite table (name, task count, languages, evaluation method, SDLC phase), directory structure overview, how to use with Harbor runner",
        "Include benchmark table: kubernetes_docs (5 tasks, Go, LLM judge + test), big_code_mcp (4 tasks, Go/Rust/C++/TS, test suite), locobench_agent (50 tasks, multi-lang, semantic similarity), swebench_pro (50 tasks, multi-lang, test suite), github_mined (25 tasks, Python, test suite)",
        "Add section describing the 3-config evaluation matrix (reference docs/CONFIGS.md)",
        "Add section describing the metrics extraction pipeline (scripts/generate_eval_report.py)",
        "Verify no credentials or .env files are tracked (check .gitignore)",
        "Verify all benchmark directories have their task files committed",
        "Push benchmarks-only branch content to main on origin (sjarmak/CodeContextBench)"
      ],
      "priority": 14,
      "passes": false,
      "notes": "Currently only initial commit is on remote. The benchmarks-only branch has all task definitions. Use: git push origin benchmarks-only:main --force"
    }
  ]
}
