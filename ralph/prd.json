{
  "project": "CodeContextBench",
  "branchName": "ralph/qa-audit-fixes",
  "description": "QA Audit Remediation - Fix 28 issues found in benchmark run audit (instruction contamination, unpinned Dockerfiles, missing time limits, Deep Search polling failure, archival, automation)",
  "userStories": [
    {
      "id": "US-001",
      "title": "Strip MCP references from RepoQA instruction.md files",
      "description": "As a benchmark maintainer, I want RepoQA instruction.md files to contain zero MCP/Sourcegraph references so baseline agents get clean instructions.",
      "acceptanceCriteria": [
        "All 10 files in benchmarks/ccb_repoqa/tasks/*/instruction.md have no mention of 'Sourcegraph', 'MCP', 'sg_', 'deep search', 'Deep Search', 'sg_deepsearch', 'sg_nls_search', 'sg_keyword_search'",
        "The task requirements themselves (what the agent should do) are preserved — only MCP-specific guidance text is removed",
        "grep -ri 'Sourcegraph\\|MCP\\|sg_deepsearch\\|sg_nls_search\\|sg_keyword_search' benchmarks/ccb_repoqa/tasks/*/instruction.md returns zero matches"
      ],
      "priority": 1,
      "passes": true,
      "notes": "RepoQA is the worst offender: 'Use Sourcegraph MCP liberally. That's the point of this task.' appears in baseline runs. The SG preamble (prepended by claude_baseline_agent.py for SG configs only) handles MCP instructions."
    },
    {
      "id": "US-002",
      "title": "Strip MCP references from TAC instruction.md files",
      "description": "As a benchmark maintainer, I want TAC instruction.md files to contain zero MCP/Sourcegraph references so baseline agents get clean instructions.",
      "acceptanceCriteria": [
        "All 6 instruction.md files in benchmarks/ccb_tac/*/instruction.md (for the 6 active tasks: tac-buffer-pool-manager, tac-dependency-change, tac-find-in-codebase-1, tac-find-in-codebase-2, tac-implement-hyperloglog, tac-write-unit-test) have no mention of 'Sourcegraph', 'MCP', 'sg_', 'deep search', 'Deep Search'",
        "Remove sections like 'Why this benefits from MCP', 'Strong MCP advantage expected', 'Deep Search understands semantic queries'",
        "Preserve all actual task requirements and context",
        "grep -ri 'Sourcegraph\\|MCP\\|sg_deepsearch\\|Deep.Search' benchmarks/ccb_tac/*/instruction.md returns zero matches"
      ],
      "priority": 2,
      "passes": true,
      "notes": "TAC instructions contain 'Why this benefits from MCP' sections with claims like 'Deep Search understands semantic queries'. Remove these MCP-advocacy sections entirely."
    },
    {
      "id": "US-003",
      "title": "Strip MCP references from LargeRepo instruction.md files",
      "description": "As a benchmark maintainer, I want LargeRepo instruction.md files to contain zero MCP/Sourcegraph references.",
      "acceptanceCriteria": [
        "All 4 files in benchmarks/ccb_largerepo/*/instruction.md have no mention of 'Sourcegraph', 'MCP', 'sg_', 'deep search', 'Deep Search'",
        "Remove text like 'use Sourcegraph MCP for broad search' and 'Why this requires MCP' sections",
        "Preserve all actual task requirements",
        "grep -ri 'Sourcegraph\\|MCP\\|sg_' benchmarks/ccb_largerepo/*/instruction.md returns zero matches"
      ],
      "priority": 3,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-004",
      "title": "Strip MCP references from SWE-bench Pro gap-fill and remaining instruction.md files",
      "description": "As a benchmark maintainer, I want all remaining instruction.md files cleaned of MCP references.",
      "acceptanceCriteria": [
        "All 6 SWE-bench Pro gap-fill instruction.md files (in benchmarks/ccb_swebenchpro/tasks/) have no mention of 'Sourcegraph', 'MCP', 'sg_'",
        "K8s Docs: 2 affected instruction.md files in benchmarks/ccb_k8sdocs/ cleaned of Sourcegraph mentions",
        "PyTorch sgt-001: Single MCP mention in benchmarks/ccb_pytorch/sgt-001/instruction.md removed",
        "Final verification: grep -ri 'Sourcegraph\\|MCP\\|sg_deepsearch\\|sg_nls_search\\|sg_keyword_search' benchmarks/ccb_*/*/instruction.md returns zero matches across ALL benchmarks",
        "Also check nested paths: grep -ri 'Sourcegraph\\|MCP\\|sg_deepsearch' benchmarks/ccb_*/tasks/*/instruction.md returns zero matches"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Gap-fill tasks use conditional language ('If Sourcegraph MCP is configured') which is lower severity but still contaminates baseline."
    },
    {
      "id": "US-005",
      "title": "Pin LargeRepo Dockerfiles to specific commits",
      "description": "As a benchmark maintainer, I want LargeRepo Dockerfiles to checkout specific commits for reproducible builds.",
      "acceptanceCriteria": [
        "big-code-k8s-001/environment/Dockerfile: Replace 'git clone --depth 1 https://github.com/kubernetes/kubernetes.git .' with 'git clone --filter=blob:none --no-checkout https://github.com/kubernetes/kubernetes.git .' followed by 'git checkout' of the pre_fix_rev from task.toml",
        "big-code-servo-001/environment/Dockerfile: Same pattern applied",
        "big-code-trt-001/environment/Dockerfile: Same pattern applied",
        "big-code-vsc-001/environment/Dockerfile: Same pattern applied",
        "Read each task.toml to get the correct pre_fix_rev value and use it in the corresponding Dockerfile",
        "The 'git config user.email' and 'git config user.name' lines are preserved after the checkout"
      ],
      "priority": 5,
      "passes": false,
      "notes": "Current Dockerfiles use 'git clone --depth 1' which gets whatever HEAD is at build time. task.toml has pre_fix_rev values that should be used. Note: pre_fix_rev may be a tag like 'v1.30.0' or 'main' — for 'main', use the actual commit hash from instance_to_mirror.json instead."
    },
    {
      "id": "US-006",
      "title": "Pin LargeRepo SG repo names to specific commits",
      "description": "As a benchmark maintainer, I want LargeRepo SG repo names in largerepo_3config.sh to use pinned commit hashes instead of --latest.",
      "acceptanceCriteria": [
        "In configs/largerepo_3config.sh, the TASK_SG_REPO_NAMES associative array uses commit-hash-suffixed repo names instead of '--latest' suffix",
        "Check configs/instance_to_mirror.json for the correct repo names for each task (big-code-k8s-001, big-code-servo-001, big-code-trt-001, big-code-vsc-001)",
        "Each SG repo name matches what is actually indexed on Sourcegraph"
      ],
      "priority": 6,
      "passes": false,
      "notes": "Current values are like 'sg-benchmarks/kubernetes--latest'. Need to match the commit-specific mirror names from instance_to_mirror.json."
    },
    {
      "id": "US-007",
      "title": "Add missing time_limit_sec for PyTorch sgt-008 and sgt-025",
      "description": "As a benchmark maintainer, I want all PyTorch tasks to have explicit time limits.",
      "acceptanceCriteria": [
        "benchmarks/ccb_pytorch/sgt-008/task.toml has time_limit_sec = 600 in the [task] section",
        "benchmarks/ccb_pytorch/sgt-025/task.toml has time_limit_sec = 600 in the [task] section",
        "grep -L 'time_limit_sec' benchmarks/ccb_pytorch/sgt-*/task.toml returns no files (all 25 have it)"
      ],
      "priority": 7,
      "passes": false,
      "notes": "All other 23 PyTorch tasks have time_limit_sec = 600. These two 'critical' difficulty tasks are the only ones missing it."
    },
    {
      "id": "US-008",
      "title": "Archive ghost runs (protonmail, tutanota, internetarchive, abandoned)",
      "description": "As a benchmark maintainer, I want invalid runs removed from active results.",
      "acceptanceCriteria": [
        "All protonmail/webclients task directories (matching *protonmail__webclients*) with 0 tokens and <5s agent duration are moved into __archived_invalid/ subdirectories within their batch/config/timestamp parent",
        "All tutanota gap-fill runs (matching *tutao-tutanota-f3ffe17a*) with reward=1.0 and 0 tokens moved to __archived_invalid/",
        "All internetarchive/openlibrary-92db runs (matching *internetarchive__openlibrary-92db*) with 0 tokens moved to __archived_invalid/",
        "4 abandoned runs in swebenchpro_selected_opus_20260202_024115/baseline/ (directories with empty trial.log and no result.json) moved to __archived_invalid/",
        "Use 'mv' to move directories — create __archived_invalid/ sibling directory if it doesn't exist"
      ],
      "priority": 8,
      "passes": false,
      "notes": "Total ~36 invalid runs. The __archived_invalid/ directory convention is already used elsewhere in the codebase. Move the entire task directory (e.g., instance_protonmail__webclients-__CpVy44j/) into __archived_invalid/ alongside its siblings."
    },
    {
      "id": "US-009",
      "title": "Archive 12 stale run batches",
      "description": "As a benchmark maintainer, I want superseded run batches archived to runs/official/archive/.",
      "acceptanceCriteria": [
        "mkdir -p runs/official/archive/ if it doesn't exist",
        "Move these 12 batch directories from runs/official/ to runs/official/archive/: bigcode_mcp_opus_20260204_023501, bigcode_mcp_opus_20260204_133210, k8s_docs_opus_20260203_160607, k8s_docs_opus_20260204_133210, locobench_selected_opus_20260203_060731, locobench_selected_opus_20260203_085551, swebenchpro_selected_opus_20260202_024115, swebenchpro_selected_opus_20260203_160607, sweperf_opus_20260203_160835, tac_opus_20260203_160607, tac_opus_20260203_221123, tac_opus_20260204_190539",
        "All 12 directories exist in runs/official/archive/ after move",
        "None of the 12 directories exist in runs/official/ after move"
      ],
      "priority": 9,
      "passes": false,
      "notes": "These are all superseded by newer batches. The archive/ directory already exists and has some content."
    },
    {
      "id": "US-010",
      "title": "Resolve duplicate runs with divergent rewards",
      "description": "As a benchmark maintainer, I want each (task, config) to have one authoritative result.",
      "acceptanceCriteria": [
        "For qutebrowser-394bfaed SG_full: find all result.json files for this task+config, keep the one in the latest timestamp directory, move earlier ones to __archived_invalid/",
        "For qutebrowser-394bfaed SG_base: same treatment",
        "For tutanota-f373ac3 SG_full: same treatment",
        "For tutanota-f373ac3 SG_base: same treatment",
        "Verify by searching: no (task, config) pair has multiple active result.json files with different reward values"
      ],
      "priority": 10,
      "passes": false,
      "notes": "These 4 pairs have runs with divergent rewards (e.g., one says 1.0, another says 0.0). Keep the latest run. Search in swebenchpro_selected_opus_20260204_191918 and swebenchpro_selected_opus_20260204_191937 directories."
    },
    {
      "id": "US-012",
      "title": "Add stale batch detection to check_infra.py",
      "description": "As a benchmark operator, I want check_infra.py to warn about stale batches before new runs.",
      "acceptanceCriteria": [
        "scripts/check_infra.py includes a new check section for 'stale batches'",
        "The check scans runs/official/ for multiple batch directories matching the same benchmark prefix (e.g., two pytorch_opus_* dirs)",
        "When stale batches are found, output a WARNING with the old batch names and the newer batch that supersedes them",
        "The check does NOT auto-archive — it only warns",
        "Existing check_infra.py checks (harbor, docker, disk, tokens) are not modified"
      ],
      "priority": 12,
      "passes": false,
      "notes": "check_infra.py already exists with checks for harbor CLI, Docker, disk space, and OAuth tokens. Add a new check section following the same pattern."
    },
    {
      "id": "US-013",
      "title": "Add auto-stale flag to archive_run.py",
      "description": "As a benchmark operator, I want archive_run.py to identify and archive superseded batches automatically.",
      "acceptanceCriteria": [
        "scripts/archive_run.py supports --auto-stale flag",
        "When --auto-stale is used, it identifies batches superseded by newer batches of the same benchmark (matching by benchmark prefix like 'pytorch_opus_')",
        "Dry-run by default: prints what would be archived without moving anything",
        "--execute flag required to actually move batches to archive/",
        "Output clearly shows which batch supersedes which",
        "Existing archive_run.py functionality (manual batch archival) is not broken"
      ],
      "priority": 13,
      "passes": false,
      "notes": "archive_run.py already exists. Add the --auto-stale mode alongside existing functionality."
    },
    {
      "id": "US-014",
      "title": "Add MANIFEST auto-regeneration to _common.sh",
      "description": "As a benchmark operator, I want MANIFEST.json auto-regenerated after each run completes.",
      "acceptanceCriteria": [
        "configs/_common.sh print_validation_summary function includes a call to 'python3 scripts/generate_manifest.py' after the validation step",
        "The MANIFEST regeneration call is wrapped in '|| true' so it doesn't fail the run if manifest generation has issues",
        "A log message like 'Regenerating MANIFEST.json...' is printed before the call",
        "Manual 'python3 scripts/generate_manifest.py' still works standalone"
      ],
      "priority": 14,
      "passes": false,
      "notes": "print_validation_summary is called at the end of each config run batch. Adding MANIFEST regen here ensures it stays current."
    },
    {
      "id": "US-015",
      "title": "Regenerate MANIFEST.json now",
      "description": "As a benchmark maintainer, I want the current MANIFEST.json to reflect all active runs.",
      "acceptanceCriteria": [
        "Run python3 scripts/generate_manifest.py successfully",
        "MANIFEST.json in runs/official/ contains 400+ entries (up from 5 after archival)",
        "No errors during generation"
      ],
      "priority": 15,
      "passes": false,
      "notes": "Current MANIFEST.json has only 5 entries. Should have 400+ after stale batches are archived (US-009) and ghost runs removed (US-008). Run AFTER US-008 and US-009."
    }
  ]
}
