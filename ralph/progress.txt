# Ralph Progress Log
Started: Wed Feb 11 23:21:05 UTC 2026

## Codebase Patterns
- Result.json has TWO formats: (1) old format with `trials[]` array, (2) new flat format with `agent_execution`, `agent_result`, `verifier_result`, `exception_info` top-level keys
- Agent infra failures = agent_exec < 10s AND 0 output tokens. Genuine failures have tokens > 0.
- Rate limit errors show in claude-code.txt JSONL: `"text":"You've hit your limit · resets 4am (UTC)"` with `"error":"rate_limit"`
- Node.js version conflicts on Alpine: `apk add nodejs` won't upgrade pre-installed Node. Need `apk del nodejs npm` first.
- Exception details live in `exception.txt` (not always present), `trial.log` (often empty), and `agent/claude-code.txt` (JSONL transcript — most reliable for error classification)
- 65536-byte `claude-code.txt` files are corrupted — contain minified JS bundle, not JSONL. Harbor bug when Claude Code crashes on startup.
- Infra failures cluster by batch timing (e.g., Feb 8-9 rate limits) not by task/repo.
- MANIFEST includes infra-fail results as reward=0.0, inflating failure rates.

---

## 2026-02-11 - US-001
- Diagnosed all SWE-bench Pro errored task runs across 8 repos
- Found 42 infra failures (not 29 as PRD estimated) across active non-archived runs
- Root causes classified:
  - 25 rate limit hits ("You've hit your limit · resets 4am (UTC)") — subscription exhausted during Feb 8-9 batch
  - 10 Node.js version conflict (protonmail only) — Alpine base image has Node 16, Claude Code needs 18+
  - 4 agent setup timeout (archived protonmail runs) — chown hangs on large workspace
  - 3 transcript corruption (internetarchive SG_full) — 65KB minified JS dump instead of JSONL
- Key finding: Most failures are NOT Docker build issues. Only protonmail needs a Docker fix.
- Files changed: docs/swebenchpro_docker_diagnosis.md (new), ralph/prd.json, ralph/progress.txt
- **Learnings for future iterations:**
  - Don't assume errored tasks are Docker failures — always check claude-code.txt transcript for the real error
  - Rate limit failures are batch-timing issues, not task-specific. Same tasks pass in different batches.
  - The `result.json` error field is often empty for infra failures — need to read multiple log sources
  - Two result.json formats exist (trials[] vs flat) depending on Harbor version
  - `__archived_invalid` directories contain historical failures but most lack useful error detail
---
