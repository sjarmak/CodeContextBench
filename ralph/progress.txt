# Ralph Progress Log
Started: 2026-02-01

## Codebase Patterns
- Agent config is controlled by `BASELINE_MCP_TYPE` env var in `claude_baseline_agent.py`
- The 3-config comparison uses: `none` (Baseline), `sourcegraph_no_deepsearch` (MCP-NoDeepSearch), `sourcegraph_hybrid` (MCP-Full)
- MCP tool names follow pattern `mcp__sourcegraph__sg_<tool_name>`
- Source configs are at `~/evals/custom_agents/agents/claudecode/configs/`
- Run outputs are at `~/evals/custom_agents/agents/claudecode/runs/official/`
- result.json `agent_result.n_input_tokens` etc. are usually null — use `extract_task_tokens_from_transcript()` fallback for token/cost data
- Transcript `result` entry has `usage.{input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens}` and top-level `total_cost_usd`
- SWE-bench partial score: parse "Required tests:" and "Required tests that passed:" from test-stdout.txt
- Harbor run output structure: `runs/<category>/<run_name>/<config>/<batch_timestamp>/<task_id>__<hash>/`
- Each `harbor run --path` invocation creates a separate batch timestamp dir (one task per batch for BigCode)
- This repo has no build/lint/test CI — quality checks are manual review of created files

---

## 2026-02-01 - US-001
- Created `docs/CONFIGS.md` documenting the 3 agent configurations with exact tool lists
- Files changed: `docs/CONFIGS.md` (new)
- **Learnings for future iterations:**
  - The agent source code at `claude_baseline_agent.py` lines 97-480 contains all config logic
  - `sourcegraph_hybrid` system prompt claims "14 tools" but only 13 distinct tool names are registered in the allowed_tools list (lines 458-472)
  - `sourcegraph_no_deepsearch` uses the same MCP endpoint as `sourcegraph_hybrid` but blocks `sg_deepsearch` and `sg_deepsearch_read` via `--disallowedTools`
  - Hybrid modes (`sourcegraph_hybrid`, `sourcegraph_no_deepsearch`, `deepsearch_hybrid`, `none`) do NOT restrict local tools
  - Non-hybrid modes (`sourcegraph`, `deepsearch`) block local search tools (Grep, Glob, grep, rg, etc.)
- Shell runners use `BASELINE_MCP_TYPE=<mode> harbor run` pattern with `--jobs-dir` to separate output per config
- The original LoCoBench comparison used `deepsearch_hybrid` but the 3-config uses `sourcegraph_no_deepsearch` and `sourcegraph_hybrid`
---

## 2026-02-01 - US-002
- Created `configs/locobench_3config.yaml` with 3 MCP modes: baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid
- Created `configs/locobench_3config.sh` shell runner with per-mode flags (--baseline-only, --no-deepsearch-only, --full-only)
- Files changed: `configs/locobench_3config.yaml` (new), `configs/locobench_3config.sh` (new)
- **Learnings for future iterations:**
  - Original YAML has `deepsearch_hybrid` as the MCP mode; the 3-config replaces this with `sourcegraph_no_deepsearch` + `sourcegraph_hybrid`
  - The YAML has a note that LoCoBench is local and not in Harbor registry — the shell script is the actual execution mechanism
  - Original shell script had 50 task IDs in a flat array (no category comments matching YAML's structure)
  - Shell runners set `CATEGORY` defaulting to `experiment` in original; changed to `official` for 3-config
  - Model is already pinned to `anthropic/claude-opus-4-5-20251101` in the original
---

## 2026-02-01 - US-003
- Created `configs/swebenchpro_3config.yaml` with 3 MCP modes: baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid
- Created `configs/swebenchpro_3config.sh` shell runner with per-mode flags (--baseline-only, --no-deepsearch-only, --full-only)
- Files changed: `configs/swebenchpro_3config.yaml` (new), `configs/swebenchpro_3config.sh` (new)
- **Learnings for future iterations:**
  - SWE-bench Pro uses `--dataset swebenchpro` with `-t <task_id>` args (Harbor registry tasks), unlike LoCoBench which uses `--path` with local dirs
  - Original YAML listed "Flipt (10 tasks)" in comments but actually has 12 Flipt task IDs
  - Original shell runner used `deepsearch` as MCP mode name; 3-config replaces with `sourcegraph_no_deepsearch` + `sourcegraph_hybrid`
  - Original shell runner had `CATEGORY=experiment`; 3-config uses `official`
  - SWE-bench Pro uses 7200s (2hr) timeout vs LoCoBench's 3600s (1hr) due to heavier test suites
---

## 2026-02-01 - US-004
- Created `configs/bigcode_3config.yaml` with 3 MCP modes: baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid
- Created `configs/bigcode_3config.sh` shell runner with per-mode flags (--baseline-only, --no-deepsearch-only, --full-only)
- Set concurrency to 1 (serial execution) and run_category to official
- Files changed: `configs/bigcode_3config.yaml` (new), `configs/bigcode_3config.sh` (new)
- **Learnings for future iterations:**
  - BigCode MCP is a local benchmark (not in Harbor registry), so YAML is reference only — shell script is the execution mechanism
  - Original had only 2 modes (baseline, sourcegraph_hybrid); 3-config adds sourcegraph_no_deepsearch
  - BigCode shell runner uses `declare -A TASK_SG_REPO_NAMES` to map task IDs to Sourcegraph repo names (e.g., `sg-benchmarks/kubernetes--latest`)
  - SOURCEGRAPH_REPO_NAME env var must be set per-task so the agent searches the correct repo
  - BigCode uses TIMEOUT_MULTIPLIER=10 (10x default) due to large codebases
  - Original CATEGORY defaulted to `experiment`; 3-config uses `official`
---

## 2026-02-01 - US-005
- Created `configs/k8s_docs_3config.yaml` with 3 MCP modes: baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid
- Created `configs/k8s_docs_3config.sh` shell runner with per-mode flags (--baseline-only, --no-deepsearch-only, --full-only)
- Set timeout to 900s per task (matching task.toml time_limit_sec), TIMEOUT_MULTIPLIER=3
- All 5 task IDs included: pkg-doc-001, client-go-doc-001, applyconfig-doc-001, apiserver-doc-001, fairqueuing-doc-001
- Files changed: `configs/k8s_docs_3config.yaml` (new), `configs/k8s_docs_3config.sh` (new)
- **Learnings for future iterations:**
  - K8s Docs is a local benchmark like LoCoBench, uses `--path` not `--dataset`
  - task.toml has `time_limit_sec = 900` (15 min) — much shorter than LoCoBench (3600s) or SWE-bench (7200s)
  - K8s Docs tasks include an MCP config setup script in task.toml `environment.setup_scripts.mcp_config` — this is separate from the Harbor agent-level MCP setup
  - All tasks are Go language, category is "package-documentation"
  - TIMEOUT_MULTIPLIER=3 provides adequate headroom for 900s tasks (compared to LoCoBench's 10x for 3600s)
---

## 2026-02-01 - US-006
- Diagnosed BigCode MCP "empty results" issue in `runs/official/bigcode_mcp_opus_20260131_130446/`
- Created `docs/BIGCODE_DIAGNOSIS.md` with full analysis
- Files changed: `docs/BIGCODE_DIAGNOSIS.md` (new)
- **Findings:**
  - Task directories DO exist — 15 tasks completed across 3 configs, all with reward=1.0
  - The "0 task dirs" observation was incorrect; task dirs are nested inside batch timestamp dirs
  - Original `bigcode_mcp_comparison.sh` only ran 2 configs (baseline + sourcegraph_hybrid), not 3
  - `deepsearch_hybrid` had only 1 of 4 tasks run (likely manual/abandoned)
  - `sourcegraph_no_deepsearch` was never run (not in original 2-config script)
  - Multiple invocations to same `--jobs-dir` created duplicate task runs
  - One AgentTimeoutError (servo, sourcegraph_hybrid, 6000s) but task still scored 1.0
- **Fix:** No config changes needed — use `bigcode_3config.sh` (US-004) for a clean re-run with all 3 configs
- **Learnings for future iterations:**
  - Harbor creates one batch timestamp dir per `harbor run` invocation, not one per task
  - BigCode tasks are run individually (one `harbor run` per task), so each task gets its own batch dir
  - Reusing `--jobs-dir` across invocations accumulates results; use fresh timestamps for clean runs
  - Even timed-out tasks can receive rewards if the verifier finds passing tests
---

## 2026-02-01 - US-007
- Created `scripts/ccb_metrics/__init__.py` and `scripts/ccb_metrics/models.py`
- Defined `TaskMetrics` dataclass with 23 fields covering scoring, timing, tokens, cost, tool usage, and code changes
- Defined `RunMetrics` dataclass with computed properties: mean_reward, mean_partial_score, pass_rate, mean_tokens, mean_wall_clock, mean_mcp_ratio
- Defined `EvalReport` dataclass with configs(), benchmarks(), and to_json() methods
- All dataclasses have to_dict() and from_dict() class methods for JSON serialization
- Files changed: `scripts/ccb_metrics/__init__.py` (new), `scripts/ccb_metrics/models.py` (new)
- **Learnings for future iterations:**
  - The package uses stdlib only (dataclasses, json, pathlib, typing, datetime, statistics) — no external deps
  - `from_dict()` filters to known fields so extra keys in serialized JSON are safely ignored
  - RunMetrics computed properties are included in to_dict() output but stripped on from_dict() input
  - `_safe_mean()` helper handles None values gracefully — returns None if all values are None
  - Python 3.10+ is required (uses `dict[str, int]` and `list[TaskMetrics]` type hints with `from __future__ import annotations`)
---

## 2026-02-01 - US-008
- Created `scripts/ccb_metrics/extractors.py` with 4 extraction functions
- `extract_task_from_result_json()` — reads Harbor result.json, populates TaskMetrics with reward, status, timing fields, and token/cost data (when available)
- `extract_task_tokens_from_transcript()` — fallback parser for claude-code.txt JSONL, extracts input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens, total_cost_usd from last `result` type entry
- `extract_swebench_partial_score()` — parses verifier/test-stdout.txt for "Required tests: N" and "Required tests that passed: M", returns M/N
- `extract_reward_from_file()` — reads verifier/reward.txt as a float fallback
- All functions handle missing/malformed files gracefully (return None for missing fields)
- Tested on real data: LoCoBench baseline task (reward=0.5462, timing fields non-None), transcript fallback (tokens non-None), SWE-bench partial score (0.0), reward.txt (0.0)
- Files changed: `scripts/ccb_metrics/extractors.py` (new)
- **Learnings for future iterations:**
  - result.json `agent_result.n_input_tokens` etc. are often null — transcript fallback is essential for token/cost data
  - The transcript's `result` entry has `usage` dict with `input_tokens`, `output_tokens`, `cache_creation_input_tokens`, `cache_read_input_tokens` and top-level `total_cost_usd`
  - The transcript also has `modelUsage` dict keyed by model name, useful for per-model cost breakdown
  - SWE-bench test-stdout.txt uses "Required tests:" and "Required tests that passed:" (not "Passed tests:" for the partial score)
  - result.json timing sections: `environment_setup`, `agent_setup`, `agent_execution`, `verifier` each have `started_at`/`finished_at`
  - result.json `agent_setup` is separate from `agent_execution` — the wall_clock covers everything from top-level `started_at` to `finished_at`
---
