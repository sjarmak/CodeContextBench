# MCP Usage Audit — Paired Rerun Analysis (2026-02-15)

**Scope**: 141 unique tasks from `paired_rerun_*` batches (Feb 14-15, 2026), comparing **Baseline** vs **SG_full** (Sourcegraph MCP with full preamble). These are paired runs where both configs execute concurrently on the same VM, eliminating load as a confound.

**Data Pipeline**: `task_metrics.json` extracted from `result.json` + `claude-code.txt` transcripts via `reextract_all_metrics.py`. Validated 279 task-config records (35 auth-failure/zero-output filtered out).

---

## Executive Summary

| Metric | Value |
|--------|-------|
| Paired tasks analyzed | 141 (126 with complete BL+SF data) |
| Mean reward delta (SG_full - BL) | **+0.026** |
| Median reward delta | **0.0** (neutral for most tasks) |
| Tasks improved | 21 (17%) |
| Tasks degraded | 10 (8%) |
| Tasks neutral | 95 (75%) |
| Positive flips (BL fail -> SF pass) | 1 |
| Negative flips (BL pass -> SF fail) | 0 |
| Mean agent task time delta | **+51%** (trimmed mean: +37%, median: +25%) |
| Mean cost delta | **+87%** (trimmed: +65%, median: +45%) |
| MCP adoption rate | 80% of SG_full tasks used at least 1 MCP tool |

**Key finding**: MCP provides a modest reward improvement (+2.6%) but at a significant time/cost premium (+37-51% agent time, +65-87% cost). The benefit is concentrated in specific task types: dependency editing, feature implementation, and code review. Most tasks see no reward change. MCP predominantly hurts efficiency.

---

## 1. Timing Methodology Verification

**CONFIRMED: All time deltas use `agent_execution_seconds` (task time only).**

| Check | Result |
|-------|--------|
| Task-config pairs with agent_execution_seconds | 255/279 (91%) |
| agent_time < wall_time (timing consistent) | **255/255 (100%)** |
| Timing INCONSISTENT | **0** |

The `agent_execution_seconds` field is derived from Harbor's `agent_execution.started_at` / `agent_execution.finished_at` timestamps, which measure only the coding + tool use phase. Docker build (`environment_setup_seconds`), Claude Code install (`agent_setup`), and test verification (`verifier_seconds`) are excluded.

**Wall time reconstruction gap**: Many tasks show large gaps between `wall_clock = env_setup + agent_exec + verifier` and the actual wall clock. This is expected — `agent_setup_seconds` (Claude Code initialization, npm install, etc.) is not tracked as a separate field. The gap is typically 50-80% of wall time for CodeReview tasks (heavy npm builds) and 15-30% for others.

**Conclusion**: Time deltas are clean. We are NOT measuring Docker build or verifier time.

---

## 2. Token Usage Verification

| Check | Result |
|-------|--------|
| Task-config pairs with token data | 255/279 |
| Zero output tokens | 0 (all filtered as invalid) |
| Suspiciously low (<100 output tokens) | 9 |

The 9 low-output tasks are LargeRepo and PyTorch tasks where the agent hit context limits quickly or the model stopped early. These are legitimate results, not auth failures (those were pre-filtered).

Token extraction uses the final `result` entry in `claude-code.txt` JSONL transcripts, which contains cumulative usage across all turns. This avoids the old bug of using `n_input_tokens` from result.json (which included cache-inflated totals).

**Conclusion**: Token data is accurate. The cost calculation uses Opus 4.6 pricing ($15/M input, $75/M output, $18.75/M cache write, $1.50/M cache read).

---

## 3. MCP Tool Usage Analysis

### Global Usage (SG_full config, 128 tasks)
| Tool | Calls | Category |
|------|-------|----------|
| read_file | 433 | Navigation |
| keyword_search | 233 | Search |
| list_files | 127 | Navigation |
| list_repos | 67 | Navigation |
| nls_search | 62 | Search |
| deepsearch_read | 42 | Search |
| deepsearch | 37 | Search |
| commit_search | 26 | Navigation |
| compare_revisions | 14 | Navigation |
| diff_search | 8 | Navigation |
| find_references | 3 | Navigation |
| **Total** | **1,052** | — |

**Search: 374 calls (36%) | Navigation: 678 calls (64%)**

The agent heavily prefers **`read_file`** (41% of all MCP calls) over search. Keyword search is 2nd most used (22%). Deep Search adoption improved vs earlier runs (37 invocations vs essentially 0 before preamble v3), but remains a minority tool.

### MCP Ratio Distribution
- **Mean MCP ratio**: 23.8% of all tool calls are MCP
- **Median MCP ratio**: 11.6%
- **Zero MCP tasks**: 26/128 (20%) — agent had MCP available but chose not to use it
- **Max MCP ratio**: 94.4%

### Per-Benchmark Adoption
| Benchmark | Tasks | w/MCP | Adoption | Avg Calls |
|-----------|-------|-------|----------|-----------|
| K8s Docs | 5 | 5 | 100% | 17.6 |
| LinuxFLBench | 5 | 5 | 100% | 16.4 |
| PyTorch | 11 | 11 | 100% | 14.3 |
| LoCoBench | 23 | 23 | 100% | 11.7 |
| SWE-bench Pro | 16 | 13 | 81% | 11.4 |
| RepoQA | 10 | 10 | 100% | 8.1 |
| TAC | 7 | 5 | 71% | 7.3 |
| DependEval | 32 | 19 | 59% | 3.4 |
| DIBench | 8 | 7 | 88% | 2.4 |
| LargeRepo | 3 | 2 | 67% | 2.7 |
| CrossRepo | 3 | 1 | 33% | 1.0 |
| CodeReview | 3 | 1 | 33% | 1.0 |
| SWE-Perf | 2 | 0 | 0% | 0.0 |

**Pattern**: Tasks with large, real-world codebases (K8s, Linux kernel, PyTorch) show highest MCP adoption. Synthetic/self-contained tasks (DependEval, SWE-Perf) show lowest adoption.

---

## 4. Reward Analysis

### Overall (SG_full vs Baseline)
- **Mean delta**: +0.026 (positive but small)
- **Median delta**: 0.0 (most tasks unchanged)
- **Improved**: 21 tasks (17%)
- **Degraded**: 10 tasks (8%)
- **Neutral**: 95 tasks (75%)

### By Benchmark
| Benchmark | Mean Delta | N | Improved | Degraded |
|-----------|-----------|---|----------|----------|
| DependEval | **+0.076** | 32 | 8 | 3 |
| TAC | **+0.131** | 7 | 2 | 0 |
| PyTorch | +0.031 | 11 | 1 | 1 |
| CodeReview | +0.020 | 3 | 1 | 0 |
| RepoQA | +0.020 | 10 | 1 | 0 |
| LoCoBench | +0.008 | 23 | 8 | 4 |
| SWE-bench Pro | +0.000 | 15 | 0 | 0 |
| K8s Docs | 0.000 | 5 | 0 | 0 |
| LargeRepo | 0.000 | 3 | 0 | 0 |
| DIBench | 0.000 | 8 | 0 | 0 |
| CrossRepo | 0.000 | 3 | 0 | 0 |
| LinuxFLBench | **-0.075** | 4 | 0 | 1 |
| SWE-Perf | **-0.289** | 2 | 0 | 1 |

**MCP clearly helps DependEval** (+7.6% mean, 8 improved tasks) and **TAC** (+13.1%, 2 improved). It **hurts SWE-Perf** (-28.9%) and **LinuxFLBench** (-7.5%).

### By SDLC Phase
| Phase | Delta | N |
|-------|-------|---|
| Implementation (feature) | **+0.120** | 31 |
| Requirements & Discovery | +0.017 | 12 |
| Implementation (refactoring) | +0.015 | 12 |
| Architecture & Design | +0.000 | 26 |
| Documentation | 0.000 | 5 |
| Implementation (bug fix) | **-0.018** | 18 |
| Testing & QA | **-0.086** | 6 |

**MCP helps most with feature implementation (+12%)** and **hurts on bug fixes (-1.8%) and testing (-8.6%)**.

### By Complexity
| LOC Changed | Delta | N |
|------------|-------|---|
| Small (<50) | +0.044 | 48 |
| Medium (50-200) | +0.002 | 20 |
| Large (200-500) | +0.021 | 20 |
| Very Large (500+) | +0.004 | 22 |

MCP benefits small-change tasks most (+4.4%), suggesting it helps with understanding rather than bulk editing.

### Positive Flips (most impactful)
| Task | Benchmark | BL | SF | MCP Calls |
|------|-----------|-----|-----|-----------|
| multifile_editing-typescript-73e7d1bc | DependEval | 0.08 | 1.00 | 11 (52%) |
| tac-implement-hyperloglog | TAC | 0.17 | 1.00 | 0 (0%) |
| multifile_editing-javascript-beeb2c66 | DependEval | 0.28 | 1.00 | 9 (45%) |
| multifile_editing-java-e1c422ed | DependEval | 0.31 | 1.00 | 13 (72%) |
| sgt-005 | PyTorch | 0.00 | 0.37 | 5 (8%) |

### Negative Flips
| Task | Benchmark | BL | SF | MCP Calls |
|------|-----------|-----|-----|-----------|
| sweperf-001 | SWE-Perf | 0.95 | 0.37 | 0 (0%) |
| lfl-sound-53441 | LinuxFLBench | 1.00 | 0.70 | 36 (65%) |

---

## 5. Efficiency Analysis (Agent Task Time)

### Absolute Times
| Config | Mean | Median | P25 | P75 |
|--------|------|--------|-----|-----|
| Baseline | 370s | 231s | 46s | 499s |
| SG_full | 484s | 321s | 94s | 628s |

### Time Delta (SG_full vs Baseline)
| Statistic | Value |
|-----------|-------|
| Mean | **+51.4%** |
| Trimmed mean (10%) | **+37.4%** |
| Median | **+25.1%** |
| Faster (<-5%) | 25 tasks (20%) |
| Neutral (±5%) | 10 tasks (8%) |
| Slower (>+5%) | **91 tasks (72%)** |

### By Benchmark
| Benchmark | Trimmed Mean | Median | N | Faster | Slower |
|-----------|-------------|--------|---|--------|--------|
| CodeReview | **-18.8%** | -16.1% | 3 | 3 | 0 |
| DIBench | +2.8% | +1.8% | 8 | 3 | 4 |
| LoCoBench | +13.3% | +6.8% | 23 | 4 | 16 |
| LinuxFLBench | +16.8% | +15.2% | 4 | 0 | 4 |
| SWE-bench Pro | +21.3% | +9.7% | 15 | 5 | 9 |
| TAC | +21.7% | +25.3% | 7 | 3 | 4 |
| PyTorch | +23.9% | +8.7% | 11 | 3 | 6 |
| CrossRepo | +44.3% | +35.0% | 3 | 0 | 3 |
| K8s Docs | +53.5% | +55.8% | 5 | 1 | 4 |
| LargeRepo | +63.7% | +32.3% | 3 | 0 | 3 |
| SWE-Perf | +68.8% | +68.8% | 2 | 0 | 1 |
| DependEval | **+69.8%** | +55.9% | 32 | 2 | 28 |
| RepoQA | **+77.0%** | +71.2% | 10 | 1 | 9 |

**Only CodeReview shows a net speed improvement with MCP.** All other benchmarks are slower. DependEval and RepoQA show the worst time inflation despite DependEval having the best reward improvement.

### Cost Delta
- Mean: +87.4% (trimmed: +65.3%, median: +45.3%)
- Output tokens: +788.8% mean (trimmed: +28.8%, median: +17.0%)

The extreme mean output token delta (789%) is driven by a few tasks where the baseline barely ran (auth edge cases that passed the 10s filter). The trimmed mean of +29% is more representative.

### Fastest MCP Tasks (time savings)
| Task | Benchmark | Delta | BL Time | SF Time | MCP Calls |
|------|-----------|-------|---------|---------|-----------|
| instance_navidrome... | SWE-Pro | **-76%** | 941s | 222s | 9 |
| sgt-003 | PyTorch | **-61%** | 1462s | 564s | 50 |
| multifile_editing-java | DependEval | **-57%** | 221s | 94s | 13 |
| instance_teleport... | SWE-Pro | **-53%** | 983s | 464s | 13 |
| tac-buffer-pool-manager | TAC | **-53%** | 1093s | 518s | 23 |

---

## 6. MCP Effectiveness by Usage Level

| MCP Usage Bin | N | MCP Reward | BL Reward | Reward Delta | Time Delta |
|---------------|---|-----------|-----------|-------------|-----------|
| No MCP (0%) | 26 | 0.730 | 0.718 | +0.012 | +31% |
| Light (0-10%) | 36 | 0.408 | 0.391 | +0.017 | +49% |
| Moderate (10-30%) | 28 | 0.372 | 0.368 | +0.004 | +45% |
| Heavy (30%+) | 38 | 0.715 | 0.648 | **+0.067** | +74% |

**Heavy MCP users get the best reward improvement (+6.7%) but also the worst time penalty (+74%).** The "no MCP" group still shows +31% time overhead from MCP preamble injection and initial search attempts before falling back to local tools.

### By SDLC Phase
| Phase | Avg MCP Calls | Reward | Time Delta |
|-------|--------------|--------|------------|
| Documentation | 17.6 | 0.920 | +135% |
| Implementation (bug fix) | 13.7 | 0.365 | +26% |
| Architecture & Design | 9.4 | 0.789 | +40% |
| Requirements & Discovery | 6.8 | 0.833 | +63% |
| Implementation (feature) | 5.3 | 0.502 | +90% |
| Maintenance | 3.0 | 0.000 | -41% |
| Implementation (refactoring) | 1.9 | 0.422 | +14% |
| Testing & QA | 0.8 | 0.695 | +32% |

---

## 7. Key Findings

### Where MCP Helps
1. **DependEval multi-file editing**: +7.6% reward delta, MCP helps agent understand file dependencies
2. **TAC implementation tasks**: +13.1% reward, MCP helps with codebase exploration
3. **Feature implementation SDLC phase**: +12% reward, consistent benefit from code understanding
4. **Heavy MCP users**: +6.7% reward when agent commits to using MCP extensively
5. **Time savings on specific tasks**: Up to -76% on SWE-Pro (Navidrome), -61% on PyTorch (sgt-003)

### Where MCP Hurts
1. **Time cost**: 72% of tasks are slower with MCP (median +25%)
2. **SWE-Perf**: -29% reward, MCP distracts from performance optimization
3. **LinuxFLBench**: -7.5% reward, heavy MCP usage (65% ratio) didn't help fault localization
4. **RepoQA time inflation**: +77% time despite 100% adoption; MCP used for verification but not needed
5. **Bug fix tasks**: -1.8% reward delta; MCP may slow down focused debugging

### MCP Tool Effectiveness
- **read_file is the workhorse** (41% of calls) — agent uses it like `Read` but for remote repos
- **keyword_search is the primary search tool** (22%), nls_search used much less (6%)
- **Deep Search finally adopted** (37 invocations) after preamble v3, but still minority
- **20% of MCP tasks use zero MCP calls** — agent decides local tools are sufficient
- **find_references barely used** (3 calls) — major underutilization for code navigation

### Methodological Findings
- Time deltas confirmed to use `agent_execution_seconds` (task time only, excludes Docker/verifier)
- Token data extracted from transcripts (not inflated result.json `n_input_tokens`)
- 35 invalid tasks filtered (12.5% failure rate — auth failures, zero output)
- Wall time reconstruction gaps are expected (agent_setup phase not separately tracked)

---

## Appendix: Data Quality

| Metric | Count |
|--------|-------|
| Total task-config records | 314 |
| Filtered as invalid | 35 (11%) |
| Valid records | 279 |
| Unique tasks | 141 |
| With complete BL+SF pair | 126 |
| Missing agent_execution_seconds | 24 (9%) |
| Benchmarks covered | 13/13 |

**Script**: `scripts/mcp_audit.py --paired-only` (default)
**Raw data**: `/tmp/mcp_audit_v3.json` (verbose JSON with per-task details)
