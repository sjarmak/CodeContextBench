{
  "project": "CodeContextBench",
  "branchName": "ralph/benchmark-task-fixes",
  "description": "Fix broken task definitions, metadata errors, weak evaluation scripts, flat difficulty labels, and formulaic MCP scores across all 116 benchmark tasks",
  "userStories": [
    {
      "id": "US-001",
      "title": "Fix crossrepo api_upgrade_01 ground truth",
      "description": "As a benchmark evaluator, I want api_upgrade_01's expected_changes.json to match its instruction.md (grpc.Dial migration) so agents are graded on what they were asked to do.",
      "acceptanceCriteria": [
        "expected_changes.json in benchmarks/ccb_crossrepo/api_upgrade_01/ references correct files for grpc.Dial/DialContext to grpc.NewClient migration across etcd, kubernetes, containerd repos",
        "Removed patterns match grpc.Dial() and grpc.DialContext() calls, added patterns match grpc.NewClient() calls",
        "validate_patch.py regex patterns updated to check for grpc.NewClient migration (not pointer.Int32 to ptr.To)",
        "No references to pointer.Int32 or ptr.To remain in expected_changes.json"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Current ground truth checks Kubernetes pointer API changes instead of gRPC migration. Requires researching actual grpc.Dial deprecation pattern in Go ecosystem."
    },
    {
      "id": "US-002",
      "title": "Fix crossrepo bug_localization_01 ground truth",
      "description": "As a benchmark evaluator, I want bug_localization_01's ground truth to describe the NumPy/pandas/scikit-learn dtype compatibility bug stated in instruction.md.",
      "acceptanceCriteria": [
        "expected_changes.json in benchmarks/ccb_crossrepo/bug_localization_01/ references Python files in numpy, pandas, and scikit-learn (not Kubernetes Go code)",
        "Expected content strings mention dtype compatibility, nullable integer, ExtensionArray, or similar pandas/numpy concepts",
        "File paths and line ranges point to actual bug locations in the Python data science stack",
        "No references to pkg/kubelet/pleg/evented.go or EventedPLEG remain"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Current ground truth points to Kubernetes PLEG code. Requires researching the actual NumPy dtype compatibility issue when pandas nullable integers flow into scikit-learn preprocessing."
    },
    {
      "id": "US-003",
      "title": "Fix crossrepo refactor_rename_01 ground truth",
      "description": "As a benchmark evaluator, I want refactor_rename_01's ground truth to match the Django/Flask/requests HTTP Request class renaming described in instruction.md.",
      "acceptanceCriteria": [
        "expected_changes.json in benchmarks/ccb_crossrepo/refactor_rename_01/ references Django, Flask, and requests source files",
        "Removed patterns match HttpRequest (Django) and Request (Flask/requests), added patterns match HTTPRequest",
        "validate_patch.py regex patterns updated for HTTP Request class renaming",
        "No references to pkg/proxy/healthcheck/healthcheck.go or ProxierHealthServer remain"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Current ground truth checks Kubernetes proxy health server renaming. Requires identifying correct file paths in Django/Flask/requests for Request class definitions."
    },
    {
      "id": "US-004",
      "title": "Fix crossrepo repo field metadata in selected_benchmark_tasks.json",
      "description": "As an analyst, I want crossrepo tasks to have correct repo fields so stratification is accurate.",
      "acceptanceCriteria": [
        "In configs/selected_benchmark_tasks.json: api_upgrade_01 repo field lists etcd, kubernetes, containerd",
        "bug_localization_01 repo field lists numpy, pandas, scikit-learn",
        "refactor_rename_01 repo field lists Django, Flask, requests",
        "cross_file_reasoning_01 repo field lists kubernetes, containerd",
        "JSON is valid after changes"
      ],
      "priority": 4,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-005",
      "title": "Remove time limit and relabel sgt-008 as critical",
      "description": "As a benchmark operator, I want sgt-008 to have no time limit and be labeled critical difficulty since it modifies 110 files.",
      "acceptanceCriteria": [
        "benchmarks/ccb_pytorch/sgt-008/task.toml: timeout field removed or set to 0",
        "benchmarks/ccb_pytorch/sgt-008/task.toml: difficulty = 'critical'",
        "configs/selected_benchmark_tasks.json: sgt-008 difficulty updated to 'critical'",
        "TOML and JSON are valid after changes"
      ],
      "priority": 5,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-006",
      "title": "Remove time limit and relabel sgt-025 as critical",
      "description": "As a benchmark operator, I want sgt-025 to have no time limit and be labeled critical difficulty since it modifies 3526 LOC.",
      "acceptanceCriteria": [
        "benchmarks/ccb_pytorch/sgt-025/task.toml: timeout field removed or set to 0",
        "benchmarks/ccb_pytorch/sgt-025/task.toml: difficulty = 'critical'",
        "configs/selected_benchmark_tasks.json: sgt-025 difficulty updated to 'critical'",
        "TOML and JSON are valid after changes"
      ],
      "priority": 6,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-007",
      "title": "Fix placeholder issue numbers in sgt-009 and sgt-016",
      "description": "As a benchmark evaluator, I want real GitHub issue references in sgt-009 and sgt-016 so agents have proper problem context.",
      "acceptanceCriteria": [
        "benchmarks/ccb_pytorch/sgt-009/instruction.md: '#ISSUE_NUMBER' replaced with actual PyTorch issue number (look up from ground truth commit f4e98fdd605ab84b770f3985c2c70fc7478f1165)",
        "benchmarks/ccb_pytorch/sgt-016/instruction.md: '#ISSUE_NUMBER' replaced with actual PyTorch issue number (look up from ground truth commit 3d27d955fd257283b4adfad635a7ed240ff83489)",
        "grep -r 'ISSUE_NUMBER' benchmarks/ returns zero results"
      ],
      "priority": 7,
      "passes": true,
      "notes": "Use gh api or PyTorch GitHub to find issue numbers associated with these commits."
    },
    {
      "id": "US-008",
      "title": "Fix truncated descriptions in sgt-005 and sgt-010",
      "description": "As a benchmark evaluator, I want sgt-005 and sgt-010 to have complete instruction descriptions.",
      "acceptanceCriteria": [
        "benchmarks/ccb_pytorch/sgt-005/instruction.md: truncated sentence 'Add type hint for the new' completed with actual PR description content from ground truth commit 41835855368264b6a44956d722b1c2c2ba924d7e",
        "benchmarks/ccb_pytorch/sgt-010/instruction.md: truncated sentence 'Add type hint for the new' completed with actual PR description content from ground truth commit b002562550577a09038c40c0e04ae18a54c8f12b",
        "No truncated sentences remain in either file"
      ],
      "priority": 8,
      "passes": true,
      "notes": "Look up the actual PR descriptions from PyTorch GitHub using the ground truth commit hashes."
    },
    {
      "id": "US-009",
      "title": "Fix language labels in SWE-bench Pro task.toml files",
      "description": "As an analyst, I want correct language labels in SWE-bench Pro so stratification and reporting are accurate.",
      "acceptanceCriteria": [
        "All element-hq-element-web task.toml files: language changed from 'python' to 'typescript'",
        "All qutebrowser task.toml files: language changed from 'typescript' to 'python'",
        "All nodebb task.toml files: language confirmed as 'javascript'",
        "All navidrome task.toml files: language confirmed as 'go'"
      ],
      "priority": 9,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-010",
      "title": "Fix language labels in selected_benchmark_tasks.json",
      "description": "As an analyst, I want selected_benchmark_tasks.json language fields to match corrected task.toml values.",
      "acceptanceCriteria": [
        "configs/selected_benchmark_tasks.json: all element-hq-element-web entries have language 'typescript'",
        "All qutebrowser entries have language 'python'",
        "All nodebb entries have language 'javascript'",
        "All navidrome entries have language 'go'",
        "Language distribution in docs/TASK_SELECTION.md updated to reflect corrections",
        "JSON is valid after changes"
      ],
      "priority": 10,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-011",
      "title": "Fix copy-paste error in big-code-vsc-001 test script",
      "description": "As a benchmark evaluator, I want the VSCode task test script to reference the correct task.",
      "acceptanceCriteria": [
        "benchmarks/ccb_largerepo/big-code-vsc-001/tests/test.sh: comment changed from 'scrollend event' to 'stale diagnostics after git branch switch'",
        "No other copy-paste artifacts in test scripts across all 4 LargeRepo tasks (verify big-code-k8s-001, big-code-servo-001, big-code-trt-001 test.sh comments match their task)"
      ],
      "priority": 11,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-012",
      "title": "Add compilation checks to LargeRepo big-code-k8s-001",
      "description": "As a benchmark evaluator, I want the Kubernetes task to verify code compiles, not just grep for keywords.",
      "acceptanceCriteria": [
        "benchmarks/ccb_largerepo/big-code-k8s-001/tests/test.sh: adds 'go build ./pkg/...' or similar compilation check",
        "Build failure sets score to 0.0 before keyword scoring runs",
        "Runs 'go test ./pkg/apis/core/taint/...' or relevant unit test package if it exists",
        "Existing keyword-based scoring retained as secondary signal",
        "Dockerfile includes Go build toolchain (verify or add)"
      ],
      "priority": 12,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-013",
      "title": "Add compilation checks to LargeRepo big-code-servo-001",
      "description": "As a benchmark evaluator, I want the Servo task to verify code compiles.",
      "acceptanceCriteria": [
        "benchmarks/ccb_largerepo/big-code-servo-001/tests/test.sh: adds 'cargo check' or 'cargo build' compilation check",
        "Build failure sets score to 0.0 before keyword scoring runs",
        "Runs at least one relevant test (e.g., cargo test for scroll-related modules) if feasible",
        "Existing keyword-based scoring retained as secondary signal",
        "Dockerfile includes Rust build toolchain (verify or add)"
      ],
      "priority": 13,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-014",
      "title": "Add compilation checks to LargeRepo big-code-trt-001",
      "description": "As a benchmark evaluator, I want the TensorRT-LLM task to verify code at least passes syntax checks.",
      "acceptanceCriteria": [
        "benchmarks/ccb_largerepo/big-code-trt-001/tests/test.sh: adds Python syntax check (python -m py_compile) for modified .py files",
        "Adds C++ syntax check for modified .h/.cpp files if compiler is available",
        "Syntax failure sets score to 0.0 before keyword scoring runs",
        "Existing keyword-based scoring retained as secondary signal",
        "Dockerfile includes Python and basic C++ compiler (verify or add)"
      ],
      "priority": 14,
      "passes": true,
      "notes": "Full CUDA compilation is not feasible in Docker; syntax checks are the practical minimum."
    },
    {
      "id": "US-015",
      "title": "Add compilation checks to LargeRepo big-code-vsc-001",
      "description": "As a benchmark evaluator, I want the VSCode task to verify TypeScript type-checks.",
      "acceptanceCriteria": [
        "benchmarks/ccb_largerepo/big-code-vsc-001/tests/test.sh: adds 'npx tsc --noEmit' or equivalent TypeScript check",
        "Type-check failure sets score to 0.0 before keyword scoring runs",
        "Runs at least one relevant test if feasible",
        "Existing keyword-based scoring retained as secondary signal",
        "Dockerfile includes Node.js and TypeScript compiler (verify or add)"
      ],
      "priority": 15,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-016",
      "title": "Expand pkg-doc-001 ground truth",
      "description": "As a benchmark evaluator, I want pkg-doc-001's ground truth to be proportionate to task requirements.",
      "acceptanceCriteria": [
        "benchmarks/ccb_k8sdocs/pkg-doc-001/ground_truth/doc.go expanded to 70-100+ lines",
        "Covers: ContainerManager interface, cgroup management, QoS enforcement, resource allocation",
        "Covers subpackages: cpumanager, memorymanager, topologymanager, devicemanager",
        "Covers platform differences: Linux vs Windows, cgroup v1 vs v2",
        "Follows same Go documentation conventions as other k8sdocs ground truths",
        "Test script scoring still works correctly with expanded ground truth"
      ],
      "priority": 16,
      "passes": true,
      "notes": "Current ground truth is only 21 lines / 889 bytes. Other k8sdocs tasks have 75-150 lines."
    },
    {
      "id": "US-017",
      "title": "Calibrate SWE-bench Pro difficulty labels",
      "description": "As an analyst, I want SWE-bench Pro difficulty to vary based on patch complexity rather than all being 'hard'.",
      "acceptanceCriteria": [
        "For each of 36 SWE-bench Pro tasks: extract files_changed count from config.json patch field",
        "Reclassify: 1-3 files = 'medium', 4-10 files = 'hard', 10+ files = 'very_hard'",
        "configs/selected_benchmark_tasks.json updated with new difficulty values",
        "At least 2 distinct difficulty values present among the 36 tasks",
        "JSON is valid after changes"
      ],
      "priority": 17,
      "passes": true,
      "notes": "Write a script to extract file counts from patches and apply the classification."
    },
    {
      "id": "US-018",
      "title": "Calibrate PyTorch difficulty labels",
      "description": "As an analyst, I want PyTorch difficulty to vary based on LOC changes.",
      "acceptanceCriteria": [
        "For each of 12 PyTorch tasks (excluding sgt-008 and sgt-025 already set to critical): review LOC changes",
        "Reclassify: <50 LOC = 'medium', 50-200 LOC = 'hard', >200 LOC = 'very_hard'",
        "configs/selected_benchmark_tasks.json updated with new difficulty values",
        "At least 2 distinct difficulty values present (excluding the 2 critical tasks)",
        "JSON is valid after changes"
      ],
      "priority": 18,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-019",
      "title": "Calibrate LoCoBench and RepoQA difficulty labels",
      "description": "As an analyst, I want LoCoBench and RepoQA difficulty to vary within each benchmark.",
      "acceptanceCriteria": [
        "LoCoBench: architectural_understanding tasks with >1M tokens context = 'expert', cross_file_refactoring = 'hard', bug_investigation = 'hard'",
        "RepoQA: small repos (<50 source files) = 'medium', large repos (>100 source files) = 'hard'",
        "configs/selected_benchmark_tasks.json updated for all reclassified LoCoBench and RepoQA tasks",
        "At least 2 distinct difficulty values in each benchmark",
        "JSON is valid after changes"
      ],
      "priority": 19,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-020",
      "title": "Document difficulty calibration methodology",
      "description": "As an analyst, I want docs/TASK_SELECTION.md to document how difficulty labels were calibrated.",
      "acceptanceCriteria": [
        "docs/TASK_SELECTION.md has new section 'Difficulty Calibration Methodology'",
        "Documents per-benchmark rules (files_changed thresholds, LOC thresholds, context size thresholds)",
        "Notes the 'critical' difficulty tier for release-engineering tasks",
        "Includes updated difficulty distribution table",
        "Markdown renders correctly"
      ],
      "priority": 20,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-021",
      "title": "Recalculate MCP benefit scores with per-task discrimination",
      "description": "As an analyst, I want MCP benefit scores to vary between tasks within the same benchmark.",
      "acceptanceCriteria": [
        "cross_file_deps component uses actual per-task file counts for SWE-bench Pro (from patch), PyTorch (from ground truth), and LoCoBench (from context_files)",
        "context_complexity uses per-task context_length where available (LoCoBench metadata)",
        "semantic_search_potential varies by task characteristics (number of repos, codebase size)",
        "Scores vary across tasks within each benchmark (standard deviation > 0.05 for benchmarks with 5+ tasks)",
        "configs/selected_benchmark_tasks.json updated with new mcp_benefit_score and mcp_breakdown values",
        "JSON is valid after changes"
      ],
      "priority": 21,
      "passes": true,
      "notes": "May require a Python script to extract per-task features and recalculate scores. Update or create scripts/ccb_metrics/task_selection.py."
    },
    {
      "id": "US-022",
      "title": "Document MCP score methodology update",
      "description": "As an analyst, I want docs/TASK_SELECTION.md to document the updated per-task MCP scoring.",
      "acceptanceCriteria": [
        "docs/TASK_SELECTION.md MCP benefit scoring section updated to describe per-task feature extraction",
        "Documents which features are used per benchmark (file counts, context length, repo count)",
        "Shows example calculation for at least one task",
        "Markdown renders correctly"
      ],
      "priority": 22,
      "passes": true,
      "notes": ""
    }
  ]
}
