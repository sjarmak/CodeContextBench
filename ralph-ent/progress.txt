## Codebase Patterns

### Enterprise Report Pipeline
- `scripts/generate_enterprise_report.py` orchestrates 4 sub-reports: workflow_metrics, economic_analysis, reliability_analysis, failure_analysis
- Each sub-report has a `scan_*()` function to collect data, compute functions, and `build_output()` to produce dict
- Enterprise report outputs: `enterprise_report.json`, `ENTERPRISE_REPORT.md`, `EXECUTIVE_SUMMARY.md`
- `governance_evaluator.py` exists but is a placeholder (ImportError handler in orchestrator)

### Task Structure
- Benchmark tasks live in `benchmarks/ccb_{benchmark}/{task_name}/`
- Each task has: `task.toml`, `instruction.md`, `environment/Dockerfile`, `tests/test.sh`
- Build context is `environment/` dir (NOT task root) — COPY paths in Dockerfile relative to environment/
- Harbor uploads `tests/` to `/tests/` at runtime; test.sh runs from `/tests/test.sh`
- task.toml verification: use `/tests/test.sh`, NOT `/workspace/tests/test.sh`

### Task Registration
- Register in `configs/selected_benchmark_tasks.json` with fields: benchmark, task_name, difficulty, language, mcp_benefit_score, repo
- Add to relevant `configs/*_2config.sh` script's TASKS array
- For SG_full: add repo mapping to TASK_SG_REPO_NAMES associative array

### Governance Tasks
- 6 existing tasks in `benchmarks/ccb_governance/`
- Permission metadata in task.toml: `permitted_paths`, `restricted_paths`, `writable_paths`, `requires_audit_log`
- No OS-level ACLs — agent self-enforces through natural language role assignment
- Compliance checking is post-hoc via trace analysis (not blocking)

### Enterprise Tasks
- 9 existing tasks in `benchmarks/ccb_enterprise/`
- Complexity dimensions: multi-team ownership, conflicting docs, stale artifacts, legacy deps, polyglot, dependency tracing
- Verification types: checklist (7 tasks), ordering with Kendall tau (1), F1 score (1)
- Codebases used: django/django (Python), flipt-io/flipt (Go)

### Trace Analysis
- Agent traces in `agent/claude-code.txt` (JSONL format)
- MCP tool names have `sg_` prefix: `mcp__sourcegraph__sg_keyword_search`
- Batch timestamp dirs: `YYYY-MM-DD__HH-MM-SS` (use regex to distinguish from task dirs)
- runs/official is symlink to real path under ~/evals/

### Run Configs
- Only baseline + SG_full configs (SG_base dropped)
- Paired execution: `run_paired_configs()` launches both simultaneously
- Multi-account OAuth with round-robin distribution

### Key Script Imports
- `ccb_metrics` package: extractors, models, statistics, discovery, task_selection
- `aggregate_status.py`: RUNS_DIR, SKIP_PATTERNS, DIR_PREFIX_TO_SUITE, CONFIGS constants
- `status_fingerprints.py`: error classification with 10 regex patterns

### Instructions for New Tasks
- ZERO implementation hints: no file paths, method names, class names, line numbers
- Describe realistic enterprise scenarios (bug reports, incident reports, feature requests)
- Agent must discover relevant code through exploration
- Keep verifier-required naming if the verifier checks specific keywords in git diffs

## Progress

