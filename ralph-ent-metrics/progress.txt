## Codebase Patterns

### Project Structure
- Benchmark tasks in `benchmarks/ccb_*/{task_name}/` with task.toml, instruction.md, tests/test.sh
- Run results in `runs/official/` (symlink to /home/stephanie_jarmak/evals/custom_agents/agents/claudecode/runs/official)
- Task metadata in `configs/selected_benchmark_tasks.json` (array of objects with task_name, benchmark, repo, language, difficulty, mcp_benefit_score)
- Existing analysis scripts in `scripts/` — follow their patterns for scan logic, output formats, CLI args

### Trace Data Locations
- `result.json` — task outcome (reward, tokens, timing, exceptions)
- `agent/claude-code.txt` — JSONL transcript (tool calls with timing)
- `agent/trajectory.json` — structured tool usage (includes subagent calls)
- `agent/setup/stdout.txt` — Docker/build logs
- `task_metrics.json` — extracted metrics (if exists)

### Directory Layout Patterns
- Two layouts exist: `config/batch_timestamp/task__hash/` and `config/task__hash/`
- Batch timestamp dirs match regex: `^\d{4}-\d{2}-\d{2}__\d{2}-\d{2}-\d{2}`
- Task dirs contain `__` separator: `task_name__hash`
- Configs: "baseline" and "sourcegraph_full" (no more sourcegraph_base)

### MCP Tool Names in Traces
- MCP tools have `sg_` prefix: `mcp__sourcegraph__sg_keyword_search` (NOT `mcp__sourcegraph__keyword_search`)
- Deep Search tool: `mcp__sourcegraph__sg_deepsearch`

### Scan Logic Pattern (from aggregate_status.py)
- Walk runs/official/{config}/{batch_or_task}/
- Skip dirs matching: `__broken_verifier`, `validation_test`, `archive`
- Timestamp-based dedup: for duplicate (suite, config, task_name), keep latest `started_at`
- DIR_PREFIX_TO_SUITE mapping: `swebenchpro_` -> `ccb_swebenchpro`, `pytorch_` -> `ccb_pytorch`, etc.

### Cost Extraction
- CORRECT: read claude-code.txt JSONL for per-message token counts
- WRONG: use cumulative n_input_tokens from result.json (includes cache, inflates 50-100x)
- Pricing: input $15/MTok, output $75/MTok, cache_write $18.75/MTok, cache_read $1.875/MTok

### CLI Convention
- All scripts support: --help, --suite SUITE, --config CONFIG filters
- Output to stdout by default, --output FILE to write to file
- JSON output with --json flag or as default
- Use argparse for CLI parsing

### Existing Extractors (ccb_metrics/)
- `ccb_metrics/extractors.py` has battle-tested extractors for tool counts, search patterns, code changes, cost, conversation analysis
- `extract_task_metrics.py` orchestrates all extractors into a TaskMetrics dataclass
- For new scripts: can import from aggregate_status.py (scan logic) and workflow_taxonomy.py (category mapping)
- `_is_mcp_tool()` checks for `mcp__` prefix — consistent across all scripts

### Cross-Script Imports
- All scripts use `sys.path.insert(0, str(Path(__file__).resolve().parent))` for sibling imports
- Import using bare module names: `from workflow_metrics import ...` NOT `from scripts.workflow_metrics import ...`
- When orchestrating multiple scripts (like generate_enterprise_report.py), use late imports inside functions to avoid `build_output` name collisions
- All sub-scripts expose: `scan_*()` → `compute_*()` → `build_output()` pattern for programmatic use

## Progress

## 2026-02-15 - US-002
- Implemented workflow taxonomy module (`scripts/workflow_taxonomy.py`)
- Created methodology doc (`docs/WORKFLOW_METRICS.md`)
- Files changed: scripts/workflow_taxonomy.py (new), docs/WORKFLOW_METRICS.md (new), ralph-ent-metrics/prd.json (updated passes)
- **Learnings for future iterations:**
  - 14 benchmark suites map to 6 workflow categories — all suites covered in SUITE_TO_CATEGORY
  - Module is importable from project root via `from scripts.workflow_taxonomy import ...`
  - Time multipliers are conservative lower-bounds; cross_repo_navigation has lowest throughput (600 tok/min, 2.5 calls/min) reflecting context-switching overhead
  - The `--json` flag pattern is standard for CLI scripts in this project
---

## 2026-02-15 - US-003
- Implemented workflow metrics extraction engine (`scripts/workflow_metrics.py`)
- Scans runs/official/ for 404 task records across all configs (baseline, sourcegraph_base, sourcegraph_full)
- Extracts 14 per-task metrics: agent_task_time, tool counts, navigation ratio, context switches, unique files
- Computes per-category aggregates across 6 workflow categories with mean/median by config
- Computes category deltas (baseline vs SG_full time savings) and engineer-equivalent minutes
- Navigation summary shows MCP adoption: baseline=0 MCP calls, SG_base=9.6, SG_full=10.4 mean MCP calls
- Files changed: scripts/workflow_metrics.py (new), ralph-ent-metrics/prd.json (updated passes), ralph-ent-metrics/progress.txt
- **Learnings for future iterations:**
  - Reuse aggregate_status.py imports (_iter_task_dirs, detect_suite, etc.) rather than reimplementing scan logic
  - Prefer transcript (claude-code.txt) over trajectory.json for tool counts — transcript includes subagent calls
  - Agent task time comes from result.json `agent_execution.started_at/finished_at` (not wall_clock which includes Docker build)
  - Navigation ratio = (search+read+MCP calls) / total_tool_calls; baseline ~0.37, SG_full ~0.51
  - Context switches require tracking file_path across sequential tool calls — Read/Edit/Write/Glob all have file_path, Bash does not
  - Some categories show negative deltas (SG_full slower): code_comprehension -128.6%, cross_repo_navigation -76.0% — these are real findings, not bugs
  - Timestamp-based dedup is critical: same (suite, config, task_name) can appear in multiple batch dirs
---

## 2026-02-15 - US-005
- Implemented economic analysis engine (`scripts/economic_analysis.py`)
- Scans runs/official/ for 402 task records, computes cost/reward/ROI metrics per config
- Per-config results: BL=$2.27/task (69% pass), SG_base=$2.48/task (66% pass), SG_full=$2.85/task (80% pass)
- Cost comparison: 36 cost_effective, 12 premium (context resolved), 80 waste, 18 no_comparison
- ROI: SG_full costs 25% more per task but passes 10% more tasks; 12 tasks resolved by context
- Files changed: scripts/economic_analysis.py (new), ralph-ent-metrics/prd.json (updated passes), ralph-ent-metrics/progress.txt
- **Learnings for future iterations:**
  - Reuse cost_report.py's `extract_cost_data()` for cost extraction — handles task_metrics.json and result.json fallback
  - Prefer task_metrics.json cost_usd when available (corrected extraction from transcript), fall back to cost_report.py
  - The --compare flag pattern: always compute comparisons internally, only include in output when --compare is set
  - Marginal cost of context (SG_full - baseline) averages $0.73/task — modest overhead
  - "waste" category dominates (80 tasks) because SG_full costs more on most tasks where both configs achieve same outcome
  - `_extract_task_name` from aggregate_status.py strips hash suffix — essential for cross-config matching
---

## 2026-02-15 - US-010
- Implemented reliability analysis pipeline (`scripts/reliability_analysis.py`)
- Scans 392 task records across 13 suites and 3 configs
- Per-suite/config stats: mean_reward, std_dev, min, max, median, n_tasks, 95% CI (bootstrap 1000 samples)
- Cross-suite consistency via CV: baseline=0.43, SG_base=0.60, SG_full=0.37 (lower=more consistent)
- Reliability floor (mean-2*std): all configs floor at 0.0 due to high variance (binary 0/1 rewards)
- 6 failure clusters identified at >2x overall failure rate
- Files changed: scripts/reliability_analysis.py (new), ralph-ent-metrics/prd.json (updated passes), ralph-ent-metrics/progress.txt
- **Learnings for future iterations:**
  - Bootstrap CI uses numpy.random.default_rng(seed) for reproducibility — avoids deprecated np.random.choice
  - Suites with n<5 tasks (codereview, sweperf) report "insufficient data for CI" — CI fields are null
  - "unknown" language/difficulty appears in failure clusters when task_name doesn't match metadata — prefix matching handles most but not all (e.g., dependeval tasks not in selected_benchmark_tasks.json)
  - Coefficient of variation (CV) effectively measures cross-suite consistency — SG_full has lowest CV (0.37), meaning most consistent performance across different benchmark types
  - Pooled std dev across suites gives more accurate reliability floor than per-suite computation
  - Binary rewards (0/1) create high std_dev (~0.4) — reliability floor is always near 0 for any config
---

## 2026-02-15 - US-012
- Implemented failure analysis engine (`scripts/failure_analysis.py`)
- 6 failure modes in taxonomy: context_insufficient, context_misused, implementation_error, verification_mismatch, infrastructure_failure, scope_exceeded
- Scans 119 failed task records across all configs; classifies by failure mode and context attribution
- Context attribution: 12 context_resolved (BL fail→SG_full pass), 76 context_no_impact, 12 context_partial_help, 10 context_made_worse
- Dominant failure modes: context_misused (55), context_insufficient (27), scope_exceeded (20)
- Residual limitations in SG_full: 14 context_misused, 6 scope_exceeded — context alone can't fix these
- `--report` generates failure_analysis.md with executive summary, per-suite breakdown, case studies
- 5 case studies auto-extracted: all LoCoBench tasks where baseline had 0 tool calls but SG_full succeeded with MCP
- Files changed: scripts/failure_analysis.py (new), ralph-ent-metrics/prd.json (updated passes), ralph-ent-metrics/progress.txt
- **Learnings for future iterations:**
  - Cross-config matching uses task_name (hash-stripped) — same as economic_analysis.py pattern
  - Reuse `_extract_tool_counts_from_transcript` from workflow_metrics.py pattern (prefer transcript over trajectory)
  - Error fingerprint severity maps cleanly to failure taxonomy: infra/api/setup→infrastructure_failure, verifier→verification_mismatch
  - Classification heuristic thresholds: <5 tool calls + exception = infra; >100 calls = scope_exceeded; >10 search + 0 edits = context_misused
  - `_is_failed()` checks both `has_exception` and `reward==0.0` — errored tasks are always failed
  - Context attribution requires ALL configs loaded (not just filtered config) for cross-config comparison
  - When --config is filtered, all context_impact labels become "no_comparison" since other configs aren't loaded — this is correct behavior
---

## 2026-02-15 - US-015
- Defined 6 JSON schemas in `schemas/` directory using JSON Schema Draft 7
- Schemas: workflow_metrics_schema.json, economic_metrics_schema.json, reliability_metrics_schema.json, failure_analysis_schema.json, governance_report_schema.json, enterprise_report_schema.json
- Each schema derived from actual script output structure (read all 4 Python scripts to map fields)
- Files changed: schemas/workflow_metrics_schema.json (new), schemas/economic_metrics_schema.json (new), schemas/reliability_metrics_schema.json (new), schemas/failure_analysis_schema.json (new), schemas/governance_report_schema.json (new), schemas/enterprise_report_schema.json (new), ralph-ent-metrics/prd.json (updated passes), ralph-ent-metrics/progress.txt
- **Learnings for future iterations:**
  - Enterprise report schema uses `sections` dict with nullable sub-reports — allows graceful degradation when sub-scripts missing
  - Governance schema is a placeholder defining expected structure (check_id/status/severity pattern) for future governance_evaluator.py
  - All schemas use `type: ["number", "null"]` pattern for optional numeric fields (matching Python's Optional[float] → JSON null)
  - The `examples` field in JSON Schema Draft 7 is an array of complete valid instances — useful for downstream consumers
  - Pre-existing schemas (result.schema.json, judge_result.schema.json, enterprise_metrics_schema.json) already in schemas/ — new schemas complement these
---

## 2026-02-15 - US-016
- Implemented enterprise report generator (`scripts/generate_enterprise_report.py`)
- Orchestrates 4 sub-scripts (workflow_metrics, economic_analysis, reliability_analysis, failure_analysis) via direct Python imports
- Gracefully skips missing governance_evaluator.py (logs warning, sets section to null)
- Outputs 3 files: enterprise_report.json (schema-validated envelope), ENTERPRISE_REPORT.md (4 sections), EXECUTIVE_SUMMARY.md (157 words)
- Executive summary computed from actual data: 14.9% reliability improvement, $2.85/task SG_full cost, 79.7% pass rate, 64.4% weighted CI
- Added `reports/` to .gitignore
- Files changed: scripts/generate_enterprise_report.py (new), .gitignore (updated), ralph-ent-metrics/prd.json (updated passes), ralph-ent-metrics/progress.txt
- **Learnings for future iterations:**
  - Sub-scripts use `sys.path.insert(0, scripts_dir)` — imports must use bare module names (`from workflow_metrics import ...`), NOT `from scripts.workflow_metrics import ...`
  - All sub-scripts follow identical pattern: `scan_*()` → `compute_*()` → `build_output()` — easy to orchestrate programmatically
  - Schema validation uses `jsonschema` library (soft dependency) — validate but don't crash if missing
  - `_count_total_tasks` deduplicates by (task_name, config) tuple across sections to avoid double-counting
  - The `build_output` function in each sub-script has name collisions when importing — use late imports inside functions to avoid conflicts
  - Time savings across categories can be negative (SG_full slower) — report honestly, don't filter
---
