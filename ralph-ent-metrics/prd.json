{
  "project": "CodeContextBench Enterprise Metrics Pipeline",
  "branchName": "ralph/enterprise-metrics",
  "description": "Build the enterprise evaluation metrics pipeline: workflow impact metrics, economic benchmarking, reliability analysis, failure-mode analysis, JSON schemas, and orchestrated report generation. All scripts derive metrics from existing trace data in runs/official/.",
  "userStories": [
    {
      "id": "US-002",
      "title": "Create workflow taxonomy module and methodology doc",
      "description": "As a product marketer, I want engineering workflows categorized and mapped to time estimates so that task results translate to 'hours saved per workflow.'",
      "acceptanceCriteria": [
        "scripts/workflow_taxonomy.py exists and is importable as a Python module",
        "Module defines WORKFLOW_CATEGORIES dict mapping category names to: description, benchmark_suites list, time_multiplier_tokens_per_minute float, time_multiplier_tool_calls_per_minute float",
        "Categories include: code_comprehension, cross_repo_navigation, dependency_analysis, bug_localization, feature_implementation, onboarding",
        "Each category maps to at least one existing benchmark suite (e.g., code_comprehension -> ['ccb_locobench', 'ccb_repoqa'])",
        "Module defines SUITE_TO_CATEGORY dict mapping each benchmark suite to its primary workflow category",
        "docs/WORKFLOW_METRICS.md exists with: methodology section explaining token-to-time conversion, citations to developer productivity research (DORA, Microsoft, McKinsey), clear 'modeled/estimated' disclaimers",
        "python3 -c \"from scripts.workflow_taxonomy import WORKFLOW_CATEGORIES, SUITE_TO_CATEGORY; print(len(WORKFLOW_CATEGORIES))\" prints 6",
        "python3 scripts/workflow_taxonomy.py runs without error and prints category summary table"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Time multipliers should be conservative estimates. Label all projections as modeled. Research sources: Google DORA State of DevOps, Microsoft Developer Velocity Index, McKinsey developer productivity. Target audience is regulated enterprise (fintech, healthcare, defense)."
    },
    {
      "id": "US-003",
      "title": "Build workflow metrics extraction engine",
      "description": "As an analyst, I want to extract workflow-level metrics from existing trace data so that I can compute time savings and navigation reduction per workflow category.",
      "acceptanceCriteria": [
        "scripts/workflow_metrics.py exists and runs: python3 scripts/workflow_metrics.py --help shows usage",
        "Script scans runs/official/ for task results (reuse scan logic pattern from scripts/aggregate_status.py: walk dirs, find result.json, timestamp-based dedup)",
        "For each task, extracts from trajectory.json and claude-code.txt: agent_task_time_seconds, tool_call_count, file_read_count, file_edit_count, search_query_count (grep+glob), mcp_call_count, unique_files_accessed",
        "Computes per-task navigation metrics: navigation_ratio = (search+read calls) / total_tool_calls, context_switch_count = count of transitions between distinct files",
        "Maps each task to workflow category using workflow_taxonomy.SUITE_TO_CATEGORY",
        "Computes per-workflow-category aggregates: mean/median agent_task_time, mean navigation_ratio, mean tool_calls by config",
        "Computes per-category delta: baseline_mean_time - sg_full_mean_time = estimated_time_saved_seconds",
        "Converts time_saved to engineer-equivalent using taxonomy time multipliers",
        "Outputs workflow_metrics.json with sections: per_task (list of task records), per_category (aggregated by workflow), navigation_summary (overall navigation metrics by config), methodology (string noting 'modeled estimates')",
        "Supports --suite SUITE and --config CONFIG filters",
        "Handles missing trajectory.json gracefully (logs warning, skips task, does not crash)",
        "python3 scripts/workflow_metrics.py runs without error on existing runs/official/ data"
      ],
      "priority": 2,
      "passes": true,
      "notes": "runs/official/ is a symlink to /home/stephanie_jarmak/evals/custom_agents/agents/claudecode/runs/official. Trace files: result.json (status/reward/tokens), agent/claude-code.txt (JSONL transcript), agent/trajectory.json (structured tool usage). MCP tool names have sg_ prefix: mcp__sourcegraph__sg_keyword_search. Batch timestamp dirs match regex ^\\d{4}-\\d{2}-\\d{2}__\\d{2}-\\d{2}-\\d{2}. Two dir layouts exist: config/batch_ts/task__hash/ and config/task__hash/."
    },
    {
      "id": "US-005",
      "title": "Build economic analysis engine with ROI and comparative cost",
      "description": "As a VP Engineering, I want cost-per-successful-outcome, productivity-per-dollar, and comparative cost metrics so that I can build a business case for context infrastructure.",
      "acceptanceCriteria": [
        "scripts/economic_analysis.py exists and runs: python3 scripts/economic_analysis.py --help shows usage",
        "Reads cost data from task_metrics.json or result.json (reuse extraction pattern from scripts/cost_report.py)",
        "Computes per-task: cost_usd, reward (0 or 1), cost_per_success (cost / reward for passing tasks, null for failing)",
        "Computes per-config aggregates: total_cost, avg_cost_per_task, avg_cost_per_success, pass_rate, tasks_passed, productivity_per_dollar (tasks_passed / total_cost)",
        "Computes per-task marginal_cost_of_context = sg_full_cost - baseline_cost",
        "Computes token_efficiency per config = total_tokens / tasks_passed",
        "Includes --compare mode that categorizes each task: 'cost_effective' (lower/same cost + same/better outcome), 'premium' (higher cost + better outcome), 'waste' (higher cost + same/worse outcome), 'no_comparison' (task in only one config)",
        "Computes break_even_hours: how many engineer-hours must be saved per dollar of MCP cost for positive ROI",
        "Outputs economic_metrics.json with sections: per_task, per_config, cost_comparison, roi_summary",
        "roi_summary includes: cost_delta_pct, pass_rate_delta, implied_hours_saved_per_dollar (using workflow taxonomy multipliers)",
        "Supports --suite filter",
        "Uses corrected cost extraction (not cumulative n_input_tokens). Pricing: input $15/MTok, output $75/MTok, cache_write $18.75/MTok, cache_read $1.875/MTok",
        "python3 scripts/economic_analysis.py runs without error on existing data"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Cost extraction gotcha: old method used cumulative n_input_tokens which includes cache, inflating costs 50-100x. Use the pattern from scripts/reextract_all_metrics.py which reads claude-code.txt JSONL transcript for accurate token counts. If task_metrics.json exists and has cost_usd field, use that directly. Subscription accounts mean actual cost is $0 but we compute hypothetical API cost for reporting."
    },
    {
      "id": "US-010",
      "title": "Build reliability analysis pipeline with variance and failure clustering",
      "description": "As an enterprise buyer, I want performance variance, confidence intervals, and failure clustering so that I can assess predictability and identify weak spots.",
      "acceptanceCriteria": [
        "scripts/reliability_analysis.py exists and runs: python3 scripts/reliability_analysis.py --help shows usage",
        "Groups tasks by (suite, config) and computes: mean_reward, std_dev, min, max, median, n_tasks",
        "Computes 95% confidence intervals for pass rate per suite/config using bootstrap resampling (1000 samples). Uses numpy for resampling.",
        "Computes cross-suite consistency: coefficient_of_variation of per-suite pass rates within each config",
        "Computes per-config reliability_floor = mean_reward - 2 * std_dev (pessimistic estimate)",
        "Failure clustering: groups failed tasks by language, difficulty, benchmark_suite, and mcp_benefit_score_quartile (from selected_benchmark_tasks.json)",
        "For each grouping dimension, computes failure_rate per group and flags groups with failure_rate > 2x overall average as 'failure_clusters'",
        "Failure cluster descriptions are human-readable: e.g., 'Hard Python tasks fail 3.2x more often than average (45% vs 14%)'",
        "Outputs reliability_metrics.json with sections: per_suite_config (stats + CI), cross_suite_consistency, reliability_floor, failure_clusters (list with dimension, group, failure_rate, relative_rate, description)",
        "Handles single-run data gracefully (reports point estimates, notes 'insufficient data for CI' where applicable)",
        "Supports --suite and --config filters",
        "python3 scripts/reliability_analysis.py runs without error on existing data"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Task metadata (language, difficulty, mcp_benefit_score) is in configs/selected_benchmark_tasks.json. Load it to join with run results. For bootstrap CI: sample with replacement from binary rewards array (0s and 1s), compute mean for each sample, take 2.5th and 97.5th percentiles. numpy is available. scipy may not be — use numpy for bootstrap, avoid scipy dependency if possible."
    },
    {
      "id": "US-012",
      "title": "Build failure analysis engine with taxonomy and context attribution",
      "description": "As a product manager and solutions engineer, I want every failed task classified by failure mode with context attribution so that I can distinguish infrastructure failures from agent limitations and build case studies.",
      "acceptanceCriteria": [
        "scripts/failure_analysis.py exists and runs: python3 scripts/failure_analysis.py --help shows usage",
        "Defines FAILURE_TAXONOMY dict with 6 failure modes: context_insufficient, context_misused, implementation_error, verification_mismatch, infrastructure_failure, scope_exceeded",
        "Each failure mode has: id, label, description, heuristic (how to classify)",
        "For each failed task, classifies failure mode using: error fingerprints from status_fingerprints.py (infrastructure_failure), trajectory analysis (context_insufficient if few/no search calls, context_misused if many searches but wrong files edited), result.json exception info",
        "Context attribution for each failed task: checks if same task passed in other config. Labels: 'context_resolved' (baseline fail + SG_full pass), 'context_partial_help' (both fail but SG_full got further by tool call count), 'context_no_impact' (both fail similarly), 'context_made_worse' (baseline pass + SG_full fail)",
        "Outputs failure_analysis.json with: per_task (list of failed task records with failure_mode, failure_detail, context_impact), aggregate (counts per failure_mode), context_summary (counts per context_impact), residual_limitations (failure modes that persist even in SG_full)",
        "Supports --report flag that also generates failure_analysis.md with: executive summary, context impact summary, per-suite breakdown, 3-5 case study examples of context-resolved failures",
        "Case studies extracted automatically: pick tasks where baseline failed + SG_full passed, sort by MCP call count descending, take top 3-5",
        "Supports --suite and --config filters",
        "python3 scripts/failure_analysis.py runs without error on existing data"
      ],
      "priority": 5,
      "passes": true,
      "notes": "Reuse status_fingerprints.py for infrastructure error classification. For context attribution, you need to match tasks across configs by task_name (strip the hash suffix: task_name is everything before __). Trajectory analysis: count tool calls in trajectory.json — if agent made <5 tool calls before failing, likely infrastructure_failure. If agent searched extensively but edited wrong files, likely context_misused. If agent never searched (in baseline), likely context_insufficient."
    },
    {
      "id": "US-015",
      "title": "Define report JSON schemas",
      "description": "As a dashboard developer, I want well-defined JSON schemas for all enterprise metrics so that the dashboard can reliably validate and consume new data.",
      "acceptanceCriteria": [
        "schemas/ directory exists at project root",
        "schemas/workflow_metrics_schema.json exists with JSON Schema Draft 7 defining the output format of workflow_metrics.py",
        "schemas/economic_metrics_schema.json exists defining output of economic_analysis.py",
        "schemas/governance_report_schema.json exists defining output of governance_evaluator.py (can be placeholder structure since governance tasks don't exist yet)",
        "schemas/reliability_metrics_schema.json exists defining output of reliability_analysis.py",
        "schemas/failure_analysis_schema.json exists defining output of failure_analysis.py",
        "schemas/enterprise_report_schema.json exists as top-level envelope wrapping all sub-reports with metadata (generated_at, ccb_version, task_count, config_list)",
        "Each schema includes: $schema, title, description, required fields, type definitions, property descriptions",
        "Each schema includes an examples section with a minimal valid example",
        "python3 -c \"import json, glob; [json.load(open(f)) for f in glob.glob('schemas/*.json')]; print('All schemas valid JSON')\" succeeds"
      ],
      "priority": 6,
      "passes": true,
      "notes": "Schemas should match the actual output format of the scripts built in earlier stories. Read the scripts to understand their output structure. The governance_report_schema can define the expected structure even though governance_evaluator.py is in a different Ralph instance — it will be built later."
    },
    {
      "id": "US-016",
      "title": "Build enterprise report generator with executive summary",
      "description": "As a benchmark operator, I want a single command that produces all enterprise reports including a 1-page executive summary so that report generation is reproducible and automated.",
      "acceptanceCriteria": [
        "scripts/generate_enterprise_report.py exists and runs: python3 scripts/generate_enterprise_report.py --help shows usage",
        "Orchestrates: workflow_metrics.py, economic_analysis.py, reliability_analysis.py, failure_analysis.py by importing their main functions or running them as subprocesses",
        "Gracefully handles missing sub-scripts (e.g., governance_evaluator.py may not exist yet): logs warning, skips that section, continues with available sections",
        "Aggregates all outputs into enterprise_report.json (top-level envelope per enterprise_report_schema.json) with metadata: generated_at (ISO timestamp), ccb_version (git describe), total_tasks, configs_compared",
        "Generates ENTERPRISE_REPORT.md with four sections: 1) Technical Report (task success rates, MCP tool usage stats), 2) Workflow Report (time saved per category, navigation reduction), 3) Executive Report (productivity impact, economic efficiency, reliability score), 4) Failure Analysis Dossier (recurring breakdowns, context-driven resolutions)",
        "Generates EXECUTIVE_SUMMARY.md (under 500 words) with: headline metric ('Context infrastructure improves agent reliability by X%'), time savings estimate, economic efficiency metric, governance readiness (placeholder until governance tasks exist), reliability CI, top residual limitation",
        "All numbers in executive summary computed from actual benchmark data (no hardcoded values)",
        "Supports --output-dir DIR flag (default: reports/)",
        "Validates output against schemas/enterprise_report_schema.json if jsonschema is available (soft dependency)",
        "python3 scripts/generate_enterprise_report.py --output-dir /tmp/test_report runs without error and produces enterprise_report.json, ENTERPRISE_REPORT.md, EXECUTIVE_SUMMARY.md"
      ],
      "priority": 7,
      "passes": true,
      "notes": "This script ties everything together. It must handle partial data — if only some sub-scripts have been run, generate what it can. The executive summary should clearly distinguish measured vs modeled metrics. Keep the markdown clean and presentation-ready. The reports/ directory should be gitignored (add to .gitignore if not already). FR-9 from the PRD: all reports must distinguish 'measured' (actual benchmark data) vs 'projected' (modeled estimates)."
    },
    {
      "id": "US-ICP-001",
      "title": "Build ICP profile selector module with 5 enterprise profiles",
      "description": "As a marketing/sales team member, I want benchmark results filtered and framed for specific customer profiles so that I can tailor conversations to the prospect's organizational context.",
      "acceptanceCriteria": [
        "scripts/icp_profiles.py exists with 5 ICP profiles: legacy_modernization, platform_saas, security_compliance, ai_forward, platform_consolidation",
        "Each profile defines: benchmark_suites, workflow_categories, success_thresholds, pain_points, objection_responses",
        "compute_profile_metrics() accepts sub-report data and returns profile-filtered metrics with threshold evaluation",
        "python3 scripts/icp_profiles.py lists all profiles",
        "python3 scripts/icp_profiles.py --profile legacy shows profile details",
        "python3 scripts/icp_profiles.py --list-suites shows suite-to-profile mapping",
        "schemas/icp_profile_schema.json defines the output format",
        "docs/ICP_PROFILES.md documents all profiles, usage, and campaign alignment"
      ],
      "priority": 8,
      "passes": true,
      "notes": "ICP profiles are a filter/lens layer — they don't generate new data, they filter existing sub-report data to profile-relevant suites and evaluate profile-specific success thresholds."
    },
    {
      "id": "US-ICP-002",
      "title": "Integrate ICP profile reports into enterprise report generator",
      "description": "As a benchmark operator, I want generate_enterprise_report.py to support --profile flag so that I can produce customer-specific reports in one command.",
      "acceptanceCriteria": [
        "generate_enterprise_report.py accepts --profile PROFILE flag",
        "When --profile is specified, produces PROFILE_REPORT_{PROFILE_ID}.md alongside standard reports",
        "Profile report includes: headline, threshold evaluation, workflow deltas, economic impact, failure analysis (all filtered to profile suites)",
        "All 5 profiles generate reports without error: python3 scripts/generate_enterprise_report.py --profile {profile} --output-dir /tmp/test",
        "Headlines are data-driven and direction-honest (no abs() hiding regressions)"
      ],
      "priority": 9,
      "passes": true,
      "notes": "Profile reports complement the standard enterprise report — they add a profile-specific view, not replace the existing output."
    }
  ]
}
