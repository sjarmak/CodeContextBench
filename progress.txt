## Codebase Patterns
- crossrepo tasks use external `validate_patch.py` from Docker base image; ground truth config is in `tests/expected_changes.json` and `task.yaml`
- crossrepo tasks have sources under `/ccb_crossrepo/src/{repo_name}/` in the Docker container
- Task definitions split across: `task.toml` (Harbor config), `task.yaml` (10Figure config), `instruction.md` (agent prompt), `tests/expected_changes.json` (ground truth patterns)
- `test.sh` scripts delegate to `/ccb_crossrepo/scripts/validate_patch.py` for scoring; patterns come from task.yaml and expected_changes.json
- Config metadata lives in `configs/selected_benchmark_tasks.json`
- LargeRepo Dockerfiles already have language toolchains: golang:1.23-bookworm (k8s), Rust (servo), Python+npm (trt/vsc)
- PyTorch task.toml files use `time_limit_sec` for timeout and `difficulty` under `[task]` section
- PyTorch instruction.md files were auto-generated from PR descriptions; many have template artifacts like `#ISSUE_NUMBER` or truncated text
- Some PyTorch tasks have ground_truth_rev commits that don't match their instruction.md title - always verify by looking up both

---

## 2026-02-04 - US-001
- Fixed expected_changes.json for api_upgrade_01: replaced incorrect pointer.Int32→ptr.To Kubernetes migration with correct grpc.Dial→grpc.NewClient migration
- Updated description, expected_files (etcd, kubernetes, containerd paths), and expected_patterns (grpc.Dial/DialContext removed, grpc.NewClient added)
- Set compilation_required to true
- Files changed: benchmarks/ccb_crossrepo/api_upgrade_01/tests/expected_changes.json
- **Learnings for future iterations:**
  - The crossrepo expected_changes.json had completely wrong content (Kubernetes pointer API instead of gRPC migration) - always verify ground truth matches instruction.md
  - task.yaml already had correct old_api/new_api fields; only expected_changes.json was wrong
  - validate_patch.py is external and not in this repo - changes to scoring logic must go through expected_changes.json and task.yaml
---

## 2026-02-04 - US-002
- Fixed expected_changes.json for bug_localization_01: replaced incorrect Kubernetes EventedPLEG/evented.go ground truth with correct NumPy/pandas/scikit-learn dtype compatibility bug
- Updated description, expected_files (sklearn, pandas, numpy paths), expected_content (dtype, nullable, ExtensionArray, Int64, check_array, StandardScaler), and expected_patterns
- Set compilation_required to false (bug localization task, not code changes)
- Files changed: benchmarks/ccb_crossrepo/bug_localization_01/tests/expected_changes.json
- **Learnings for future iterations:**
  - Bug localization tasks use a different expected_changes.json structure than api_upgrade tasks - they have expected_content keywords rather than removed/added patterns
  - The instruction.md explicitly lists the 4 files to trace through, which should be used as expected_files in the ground truth
  - task.yaml was already correct (repos: numpy, pandas, scikit-learn) - only expected_changes.json had wrong content (same pattern as US-001)
  - For bug localization, expected_patterns can be split into root_cause_files and entry_point_files to help differentiate scoring
---

## 2026-02-04 - US-003
- Fixed expected_changes.json for refactor_rename_01: replaced incorrect Kubernetes ProxierHealthServer→ProxyHealthServer renaming with correct Django/Flask/requests HTTPRequest class renaming
- Updated description, expected_files (django, flask, requests paths), and expected_patterns (HttpRequest/Request removed, HTTPRequest added)
- Included key files from all three libraries: request.py, wrappers.py, models.py, plus imports and test client files
- Files changed: benchmarks/ccb_crossrepo/refactor_rename_01/tests/expected_changes.json
- **Learnings for future iterations:**
  - Same pattern as US-001 and US-002: expected_changes.json had completely wrong content (Kubernetes code), while task.yaml was already correct
  - All three crossrepo tasks had the same issue - expected_changes.json was templated from Kubernetes examples and never updated
  - For refactor/rename tasks, expected_patterns should have both class definition patterns (e.g., "class HttpRequest") and general usage patterns (e.g., "HttpRequest") for comprehensive scoring
  - File paths in expected_files follow the convention: src/{repo_name}/{path_within_repo}
---

## 2026-02-04 - US-004
- Fixed crossrepo repo field metadata in configs/selected_benchmark_tasks.json
- Updated refactor_rename_01 repo from "django,flask,requests" to "Django,Flask,requests" to match proper project name casing
- Fixed trailing comma in metadata section that made JSON invalid
- Verified all 4 crossrepo tasks have correct repo fields: api_upgrade_01 (etcd,kubernetes,containerd), bug_localization_01 (numpy,pandas,scikit-learn), refactor_rename_01 (Django,Flask,requests), cross_file_reasoning_01 (kubernetes,containerd)
- Files changed: configs/selected_benchmark_tasks.json
- **Learnings for future iterations:**
  - The JSON file had a trailing comma in the metadata section making it technically invalid - always validate JSON after changes
  - Most repo fields were already correct from the original generation; only refactor_rename_01 needed casing fix
  - bug_localization_01 and refactor_rename_01 still have language="go" which is incorrect (should be "python") - this is addressed in later stories (US-009/US-010)
---

## 2026-02-04 - US-005
- Removed `time_limit_sec = 600` from sgt-008 task.toml
- Changed difficulty from "hard" to "critical" in both task.toml and selected_benchmark_tasks.json
- Files changed: benchmarks/ccb_pytorch/sgt-008/task.toml, configs/selected_benchmark_tasks.json
- **Learnings for future iterations:**
  - Removing a TOML field is a simple line deletion; no need to set to 0
  - US-006 is the same pattern (sgt-025) - apply identical changes
---

## 2026-02-04 - US-006
- Removed `time_limit_sec = 600` from sgt-025 task.toml
- Changed difficulty from "hard" to "critical" in both task.toml and selected_benchmark_tasks.json
- Files changed: benchmarks/ccb_pytorch/sgt-025/task.toml, configs/selected_benchmark_tasks.json
- **Learnings for future iterations:**
  - Identical pattern to US-005 - simple line deletion for time_limit_sec and string replacement for difficulty
  - Python 3.10 on this system doesn't have `tomllib` (added in 3.11); use `pip._vendor.tomli` as fallback for TOML validation
---

## 2026-02-04 - US-007
- Replaced all `#ISSUE_NUMBER` placeholder strings with actual PyTorch PR/issue numbers across 5 files
- sgt-002: replaced with #169497 (the PR being reverted in runtime_assert pass)
- sgt-004: replaced with #170499 (already referenced in the title, second occurrence was template duplicate)
- sgt-009: replaced with #167368 (the HOP print functionalize PR matching the instruction title); also fixed task.toml description
- sgt-012: replaced with #164048 (the issue being properly fixed, already mentioned in description text)
- Note: sgt-016 did NOT have #ISSUE_NUMBER despite being named in the story title
- Files changed: benchmarks/ccb_pytorch/sgt-002/instruction.md, sgt-004/instruction.md, sgt-009/instruction.md, sgt-009/task.toml, sgt-012/instruction.md
- **Learnings for future iterations:**
  - The `#ISSUE_NUMBER` placeholder came from PyTorch's PR template and was never filled in by original authors - this is a common pattern in auto-generated task descriptions
  - Many tasks have ground_truth_rev commits that don't match their instruction.md title (e.g., sgt-009 instruction is about HOP print but ground truth commit is about NCCL AllToAll) - use the instruction title to look up the correct PR, not the ground truth commit
  - Always grep across ALL benchmarks directories for placeholders, not just the ones named in the story - found 4 additional files beyond the 2 listed in the story
  - sgt-004 had a duplicate "Fixes #" line (one real, one placeholder) - the original PR body likely had both
---

## 2026-02-04 - US-008
- Fixed truncated descriptions in sgt-005 and sgt-010 instruction.md files
- sgt-005: Completed "Add type hint for the new " → "Add type hint for the new property" and added 2 missing change list entries (triton.py and test_triton_heuristics.py) from PR #170190
- sgt-010: Completed "Also partially covered by smoke test PR: https://g" → full URL https://github.com/pytorch/pytorch/pull/165922/files from PR #167327
- Both truncations appeared twice in each file (once in Description, once in Task section) - fixed all occurrences
- Files changed: benchmarks/ccb_pytorch/sgt-005/instruction.md, benchmarks/ccb_pytorch/sgt-010/instruction.md
- **Learnings for future iterations:**
  - Ground truth commits in the PRD don't match the instruction titles (sgt-005 commit 41835855 is for a revert PR, sgt-010 commit b002562550 is for "Add doc for Symmetric Memory") - must search by PR title instead
  - Use `gh api "search/issues?q=..."` to find PRs since `gh search` is not available in this gh version
  - Truncation in sgt-005 wasn't just the type hint text - the entire change list was truncated, missing 2 of 4 files. Always compare against the full PR body to catch all missing content
  - sgt-010 had a typo "convolutino" in the original PR body - left as-is since the instruction should match the actual PR
---

## 2026-02-04 - US-009
- Fixed language labels in SWE-bench Pro task.toml files across 236 files
- element-hq-element-web: changed language from "python" to "typescript" (56 files)
- qutebrowser: changed language from "typescript" to "python" (79 files)
- nodebb: changed language from "python" to "javascript" (44 files)
- navidrome: changed language from "python" to "go" (57 files)
- Also updated tags arrays to match the corrected language in all files
- Files changed: 236 task.toml files across benchmarks/ccb_swebenchpro/tasks/
- **Learnings for future iterations:**
  - The SWE-bench Pro task generation script assigned "python" as default language to all tasks, regardless of the actual project language
  - qutebrowser got "typescript" instead of "python" - a different error from the default, suggesting some partial language detection was attempted
  - Both `language` field and `tags` array need to be updated in sync - the tags contain the language name as the last element
  - The tasks/ directory is in .gitignore (root-level `tasks/` pattern) but files were previously force-added and are tracked by git - use `git status` not `git add` with globs to stage changes
---

## 2026-02-04 - US-010
- Fixed language labels in configs/selected_benchmark_tasks.json to match corrected task.toml values
- SWE-bench Pro entries (element-hq-element-web=typescript, qutebrowser=python, nodebb=javascript, navidrome=go) were already correct
- Fixed crossrepo entries: bug_localization_01 changed from "go" to "python" (repos: numpy, pandas, scikit-learn), refactor_rename_01 changed from "go" to "python" (repos: Django, Flask, requests)
- Updated language distribution table in docs/TASK_SELECTION.md with actual counts from JSON
- Files changed: configs/selected_benchmark_tasks.json, docs/TASK_SELECTION.md
- **Learnings for future iterations:**
  - The SWE-bench Pro language labels in selected_benchmark_tasks.json were already correct even before US-009 fixed the task.toml files - the JSON was generated with correct labels for these repos
  - The crossrepo tasks had "go" as language because the original template was based on Kubernetes (Go) - same root cause as the wrong expected_changes.json content (US-001 through US-003)
  - The original language distribution table in TASK_SELECTION.md had inflated counts (python=33, go=24, javascript=8, java=5) vs actual (python=32, go=22, javascript=5, java=2) - unclear if prior changes weren't reflected or the table was generated from a different source
---

## 2026-02-04 - US-011
- Fixed copy-paste error in big-code-vsc-001/tests/test.sh: comment said "scrollend event implementation" (copied from servo task) but actual task is "stale diagnostics after git branch switch"
- Changed comment header from "VS Code scrollend event implementation" to "VS Code stale diagnostics after git branch switch"
- Updated verification checklist items to match actual task (diagnostics, TypeScript language features, diagnostics pipeline)
- Verified all 3 other LargeRepo test scripts have correct comments matching their tasks:
  - big-code-k8s-001: "Kubernetes NoScheduleNoTraffic taint effect" ✓
  - big-code-servo-001: "Servo scrollend event implementation" ✓
  - big-code-trt-001: "TensorRT-LLM W4A8_MXFP4_INT8 quantization mode" ✓
- Files changed: benchmarks/ccb_largerepo/big-code-vsc-001/tests/test.sh
- **Learnings for future iterations:**
  - The LargeRepo test scripts were likely generated from a template starting with the servo/scrollend task, and the vsc-001 script wasn't updated
  - The test script body (actual grep patterns and scoring logic) was already correct for the diagnostics task - only the comment header was wrong
  - All 4 LargeRepo tasks have similar test.sh structure: git change detection guard → keyword grep scoring → reward calculation
---

## 2026-02-04 - US-012
- Added Go compilation checks to big-code-k8s-001/tests/test.sh
- Compilation check runs `go build` on pkg/apis/core, pkg/scheduler, and pkg/kubelet packages before keyword scoring
- Build failure sets score to 0.0 and exits early
- Added unit test execution for ./pkg/apis/core/taint/... with proper exit code handling
- Rebalanced scoring weights: taint constant=0.3, file changes=0.2, tests added=0.2, unit tests pass=0.3
- Dockerfile already uses golang:1.23-bookworm base image (Go toolchain confirmed present)
- Files changed: benchmarks/ccb_largerepo/big-code-k8s-001/tests/test.sh
- **Learnings for future iterations:**
  - The Dockerfile base image `golang:1.23-bookworm` already provides the Go toolchain - no Dockerfile changes needed
  - When checking exit codes in bash, `$?` is reset by `if` statements - use `cmd && RC=0 || RC=$?` pattern instead
  - Go test exit code 1 = test failure, exit code 2+ = package not found or build error - useful for distinguishing missing packages from actual failures
  - The `set -e` at the top means all compilation checks need explicit error handling (using `!` negation or `|| true`) to prevent the script from exiting on the first build failure
---

## 2026-02-04 - US-013
- Added Rust compilation checks to big-code-servo-001/tests/test.sh
- Compilation check runs `cargo check` before keyword scoring; build failure sets score to 0.0 and exits early
- Added unit test execution for scroll-related tests via `cargo test scroll`/`cargo test scrollend` with proper exit code handling
- Improved commit detection to use origin refs (same robust pattern as k8s task from US-012)
- Rebalanced scoring weights: scrollend keyword=0.3, file changes=0.2, WPT tests=0.2, unit tests pass=0.3
- Dockerfile already uses `rust:bookworm` base image (full Rust/Cargo toolchain confirmed present)
- Files changed: benchmarks/ccb_largerepo/big-code-servo-001/tests/test.sh
- **Learnings for future iterations:**
  - The Dockerfile is at `environment/Dockerfile` not at the task root level - different directory structure than expected
  - `cargo check` is preferred over `cargo build` for compilation verification as it's significantly faster (no codegen)
  - For Rust test filtering, `cargo test <pattern>` runs tests whose names match the pattern - useful for running scroll-related tests without knowing exact test names
  - The servo test script had a simpler commit detection than k8s (using FETCH_HEAD/ORIG_HEAD) - upgraded to the more robust origin ref pattern
---

## 2026-02-04 - US-014
- Added Python syntax checks and C++ syntax checks to big-code-trt-001/tests/test.sh
- Python syntax check uses `python -m py_compile` on all modified .py files; failure sets score to 0.0 and exits early
- C++ syntax check uses `g++ -fsyntax-only -std=c++17` on modified .h/.cpp/.cc files; informational only (doesn't fail) since CUDA/TRT headers aren't available in Docker
- Upgraded commit detection to robust origin ref pattern (consistent with k8s/servo tasks from US-012/US-013)
- Improved file change detection to include committed changes (not just unstaged)
- Existing keyword-based scoring retained as secondary signal
- Dockerfile already has `python:3.11-bookworm` (Python) and `build-essential` (g++ compiler) — no Dockerfile changes needed
- Files changed: benchmarks/ccb_largerepo/big-code-trt-001/tests/test.sh
- **Learnings for future iterations:**
  - For TensorRT-LLM, full CUDA compilation is not feasible in Docker — `py_compile` is the practical minimum for Python files
  - C++ syntax checking with `g++ -fsyntax-only` will fail on most TRT files due to missing CUDA headers — made it informational-only to avoid false positives
  - The `build-essential` apt package in Debian includes g++ — no need for separate C++ compiler installation
  - Used `ORIGIN_REF_GUARD` variable name in the guard section to avoid collision with `ORIGIN_REF` used later in syntax checks — be careful about variable name reuse in long bash scripts
---

## 2026-02-04 - US-015
- Added TypeScript type-check compilation checks to big-code-vsc-001/tests/test.sh
- Type-check uses `npx tsc --noEmit` targeting VS Code's tsconfig files (extensions/typescript-language-features and src/)
- Includes fallback to per-file `--isolatedModules` check if no tsconfig found
- Type-check failure sets score to 0.0 and exits early (same pattern as k8s/servo/trt tasks)
- Added unit test execution via `npm test` in extensions/typescript-language-features (best-effort, reduces score on failure)
- Upgraded commit detection to robust origin ref pattern (consistent with US-012/013/014)
- Improved file change detection to include committed changes
- Rebalanced scoring weights: diagnostics changes=0.2, TS service changes=0.2, type-check=0.3, unit tests=0.3
- Dockerfile already uses `node:22-bookworm` with `npm install --legacy-peer-deps` — Node.js and TypeScript are available
- Files changed: benchmarks/ccb_largerepo/big-code-vsc-001/tests/test.sh
- **Learnings for future iterations:**
  - VS Code has multiple tsconfig.json files across the repo — target the specific extension's tsconfig rather than root
  - The `npm test` command in VS Code extensions may need a display/electron environment that won't be available in Docker — making test failure non-fatal is important
  - All 4 LargeRepo tasks now have consistent structure: origin ref detection → compilation check → unit tests → keyword scoring → weighted reward calculation
  - The `while read` loop runs in a subshell, so variable changes inside it don't propagate — use marker files (touch/test) to signal failure to the parent shell
---

## 2026-02-04 - US-016
- Expanded pkg-doc-001 ground truth doc.go from 22 lines to 113 lines
- Added comprehensive documentation covering all required topics:
  - ContainerManager interface and its methods
  - Cgroup management with v1/v2 support and hierarchy description
  - QoS enforcement for Guaranteed, Burstable, and BestEffort pods
  - Resource allocation coordination across sub-managers
  - All 4 subpackages: cpumanager, memorymanager, topologymanager, devicemanager with descriptions
  - Platform differences: Linux (full cgroup support), Windows (job objects), stub for unsupported
- Used `//` line comment style with `# Heading` sections consistent with apiserver-doc-001 and client-go-doc-001
- All 8 test script checks pass (10/10 score)
- Files changed: benchmarks/ccb_k8sdocs/pkg-doc-001/ground_truth/doc.go
- **Learnings for future iterations:**
  - The k8sdocs ground truth files range from 75-151 lines; pkg-doc-001 was an outlier at 22 lines
  - All k8sdocs test scripts use the same 10-point scoring structure: file exists (2), package declaration (1), domain-specific checks (7)
  - The `//` line comment style with `# Heading` sections is used by 4 of 5 k8sdocs tasks; only applyconfig-doc-001 uses block comments
  - Ground truth should cover all topics that the test script checks for, to serve as a realistic exemplar of what a correct answer looks like
---

## 2026-02-04 - US-017
- Calibrated SWE-bench Pro difficulty labels based on files_changed count from config.json patch field
- Classification thresholds: 1-3 files = 'medium', 4-10 files = 'hard', 10+ files = 'very_hard'
- Result: 13 tasks remain 'hard' (4-10 files), 23 tasks reclassified to 'very_hard' (11+ files), 0 tasks classified as 'medium'
- No SWE-bench Pro tasks have ≤3 files changed, so 'medium' tier is empty for this benchmark
- Files changed: configs/selected_benchmark_tasks.json
- **Learnings for future iterations:**
  - SWE-bench Pro config.json files are at `benchmarks/ccb_swebenchpro/tasks/{dir_name}/tests/config.json`
  - Task directory names use `-` (hyphen) where task_ids use `__` (double underscore) — e.g., `instance_ansible__ansible-xxx` becomes `instance_ansible-ansible-xxx`
  - The `patch` field in config.json contains unified diff; count `diff --git` lines to get file count
  - All 36 SWE-bench Pro tasks have at least 6 files changed — none qualify for 'medium' difficulty with the ≤3 files threshold
  - The protonmail/webclients tasks are particularly large (30-84 files), while qutebrowser tasks are the smallest (6-7 files)
---

## 2026-02-04 - US-018
- Calibrated PyTorch difficulty labels based on LOC changes from instruction.md metadata
- Classification thresholds: <50 LOC = 'medium', 50-200 LOC = 'hard', >200 LOC = 'very_hard'
- Result (12 tasks in selected_benchmark_tasks.json, excluding sgt-008 and sgt-025 already critical):
  - medium: 3 tasks (sgt-003: 38 LOC, sgt-005: 22 LOC, sgt-012: 25 LOC)
  - hard: 6 tasks (sgt-001: 94, sgt-002: 180, sgt-009: 87, sgt-010: 52, sgt-016: 93, sgt-021: 59)
  - very_hard: 1 task (sgt-014: 242 LOC)
  - critical: 2 tasks (sgt-008, sgt-025 — unchanged)
- Files changed: configs/selected_benchmark_tasks.json
- **Learnings for future iterations:**
  - Only 12 of 25 PyTorch tasks are in selected_benchmark_tasks.json — not all tasks in benchmarks/ are selected
  - LOC data is available in each task's instruction.md under "Files Changed" / "Additions" / "Deletions" metadata
  - Total LOC = additions + deletions provides the best proxy for task complexity
  - sgt-003 was previously 'hard' but actually has only 38 LOC — difficulty labels were originally assigned without LOC analysis
---

## 2026-02-04 - US-019
- Calibrated LoCoBench and RepoQA difficulty labels in configs/selected_benchmark_tasks.json
- LoCoBench (25 tasks): architectural_understanding tasks (9) kept as 'expert' (all have >1M tokens context); cross_file_refactoring (13) and bug_investigation (3) changed from 'expert' to 'hard'
- RepoQA (10 tasks): skypjack/uvw (78 source files) and rust-bakery/nom (65 source files) changed from 'hard' to 'medium'; remaining 8 repos all have >100 source files, kept as 'hard'
- Both benchmarks now have 2 distinct difficulty values (LoCoBench: expert/hard, RepoQA: hard/medium)
- Files changed: configs/selected_benchmark_tasks.json
- **Learnings for future iterations:**
  - All selected LoCoBench architectural_understanding tasks happen to have >1M tokens context, so none needed downgrading
  - RepoQA task.toml files don't store source file counts — had to look up repos via GitHub API to count source files
  - For borderline repos (50-100 source files like uvw=78 and nom=65), classified as 'medium' since the intent is to create variation and these are genuinely smaller repos
  - The remaining 8 RepoQA repos are all clearly >100 files (ranging from 141 for express to 559 for logging-log4cxx)
---

## 2026-02-04 - US-020
- Added "Difficulty Calibration Methodology" section to docs/TASK_SELECTION.md
- Documents per-benchmark calibration rules in a summary table (metric + thresholds per benchmark)
- Describes the "critical" difficulty tier for release-engineering-scale PyTorch tasks
- Includes updated difficulty distribution table with counts and percentages across all 116 tasks
- Distribution: easy=1, medium=22, hard=58, very_hard=24, critical=2, expert=9
- Files changed: docs/TASK_SELECTION.md
- **Learnings for future iterations:**
  - The actual difficulty distribution is heavily weighted toward "hard" (50%) — this is expected since most benchmarks default to hard for their baseline tasks
  - crossrepo has 1 "easy" task that doesn't fit the standard medium/hard/very_hard tiers — used a simplified table format (Metric + Thresholds) instead of separate columns per difficulty level
  - All SWE-bench Pro tasks have 6+ files changed, so the "medium" tier (1-3 files) is empty for that benchmark
---

## 2026-02-04 - US-021
- Recalculated MCP benefit scores with per-task discrimination across all 116 tasks
- Updated 67 tasks with new scores (49 tasks unchanged, already had per-task variation)
- Key changes by benchmark:
  - LoCoBench (25 tasks): Used actual context_length (975K-1.16M) and files_count (73-86) from task.toml metadata for context_complexity, cross_file_deps, and semantic_search_potential. Scores now range 0.717-0.931 (sd=0.055) vs all identical before
  - RepoQA (10 tasks): Used per-repo source file counts (65-559) for context_complexity and cross_file_deps, and code_ratio (0.0-0.79) for semantic_search_potential. Scores now range 0.597-0.953 (sd=0.114) vs all 0.85 before
  - k8sdocs (5 tasks): Used per-package source file counts (25-450) for all three variable components. Scores now range 0.378-0.894 (sd=0.219) vs all 0.515 before
  - LargeRepo (4 tasks): Used per-codebase LOC estimates and expected files touched. Scores now range 0.730-0.876 (sd=0.064) vs all 0.895 before
  - DIBench (8 tasks): Used per-repo file counts (20-82). Scores now range 0.534-0.847 (sd=0.125) vs range 0.70-0.74 before
  - TAC (8 tasks): Used per-category resource scaling. Scores now range 0.350-0.603 (sd=0.103) vs range 0.44-0.575 before
  - SWEperf (3 tasks): Used baseline runtime for complexity scaling. Scores now range 0.433-0.525 (sd=0.047) vs all 0.4575 before
  - SWE-bench Pro (36 tasks): No changes — already had per-task variation from file counts
  - PyTorch (12 tasks): No changes — already had per-task variation from file counts
  - CrossRepo (5 tasks): No changes — already had per-task variation
- All benchmarks with 5+ tasks now have stddev > 0.05
- New avg MCP score: 0.6852 (was 0.6753)
- Files changed: configs/selected_benchmark_tasks.json
- **Learnings for future iterations:**
  - LoCoBench tasks have very narrow metadata ranges (context_length 975K-1.16M, files_count 73-86) — requires stretched normalization to create meaningful score variation
  - RepoQA code_ratio from repoqa_instances.jsonl is a good differentiator for semantic_search_potential
  - For small benchmarks (3-4 tasks), achieving stddev > 0.05 is easy; for large uniform benchmarks like LoCoBench (25 tasks with similar metadata), it requires careful normalization
  - The original scoring script used benchmark-level defaults for most components, making all tasks within a benchmark identical — per-task features from task.toml metadata create much better discrimination
---

## 2026-02-04 - US-022
- Updated docs/TASK_SELECTION.md MCP benefit scoring section to document per-task feature extraction methodology
- Added "Per-Task Feature Extraction" subsection with table showing which features drive each component per benchmark (10 benchmarks × 3 variable components)
- Added "Example Calculation" subsection with worked example for big-code-k8s-001 (ccb_largerepo) showing feature extraction → component values → weighted sum = 0.8755
- Added "Score Variation by Benchmark" subsection with table showing score ranges and standard deviations per benchmark
- Updated average MCP benefit score from 0.6753 to 0.6852
- Revised component definitions to describe per-task semantics rather than fixed benchmark-level defaults
- Files changed: docs/TASK_SELECTION.md
- **Learnings for future iterations:**
  - The documentation should reflect the actual data sources, not just the formula — the per-benchmark feature extraction table is the most valuable addition
  - SWE-bench Pro and PyTorch already had per-task variation before US-021 (from file counts), so their stddev is listed as "per-task" rather than a specific number
  - The example calculation is most illustrative when it uses a task with clearly explainable features (LargeRepo k8s is ideal because codebase LOC and file counts are intuitive)
---
