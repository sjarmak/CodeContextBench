## Codebase Patterns

### Harbor Task Format
- Each task lives in `benchmarks/ccb_{benchmark}/{task_name}/`
- Required files: task.toml, instruction.md, environment/Dockerfile, tests/test.sh
- Build context is `environment/` dir (NOT task root) — `docker.py:73: context_dir=str(self.environment_dir)`
- COPY paths in Dockerfile must be relative to `environment/`: `COPY inject_defects.sh` YES, `COPY environment/inject_defects.sh` NO
- Harbor uploads `tests/` directory to `/tests/` in container at runtime — do NOT COPY tests/ in Dockerfile
- task.toml verification command: use `/tests/test.sh` (Harbor upload path), NOT `/workspace/tests/test.sh`

### task.toml Format
```toml
[task]
name = "task-name-here"
language = "python"
difficulty = "medium"
time_limit_sec = 600

[task.metadata]
# Custom fields for governance
permitted_paths = ["service-a/"]
restricted_paths = ["service-b/"]
```

### Dockerfile Pattern (Python tasks)
```dockerfile
FROM python:3.11-slim
WORKDIR /workspace
# Copy workspace files from environment/ build context
COPY workspace/ /workspace/
# Install deps if needed
RUN pip install -r requirements.txt 2>/dev/null || true
```

### test.sh Pattern
```bash
#!/bin/bash
set -e
cd /workspace
# Check for expected output/changes
# Exit 0 = pass, non-zero = fail
```

### selected_benchmark_tasks.json Format
- Array of objects with: task_name, benchmark, repo, language, difficulty, mcp_benefit_score, selected (bool)
- New tasks should follow existing object structure
- benchmark field = "ccb_governance" for governance tasks

### Trajectory Analysis
- trajectory.json contains structured tool usage with tool names and parameters
- File access operations: Read (file_path param), Write (file_path), Edit (file_path), Glob (pattern)
- Extract file_path from tool call parameters to check against permitted/restricted lists

## Progress

## 2026-02-15 - US-007
- Created `docs/GOVERNANCE_BENCHMARK.md` (569 words): Overview, Task Design (permission simulation via workspace layout + task.toml metadata), 5 Governance Scenarios, Evaluation Criteria (3 violation types + compliance_rate scoring), Integration with Harbor
- Created `docs/ENTERPRISE_TASK_DESIGN.md` (563 words): 6 Complexity Dimensions (multi-team ownership, conflicting docs, stale artifacts, partial context, legacy deps, polyglot services) with templates, Integration with Harbor format
- Files changed: docs/GOVERNANCE_BENCHMARK.md (new), docs/ENTERPRISE_TASK_DESIGN.md (new)
- **Learnings for future iterations:**
  - Both docs reference `governance_evaluator.py` and `trajectory.json` analysis — US-009 must implement exactly what's described here
  - task.toml governance_metadata fields: `permitted_paths`, `restricted_paths`, `writable_paths` — future tasks must use these exact field names
  - Enterprise complexity dimensions can be mixed (a task can test both "partial context" and "multi-team ownership") but keep each task focused on one primary dimension for clean evaluation
---
