# Ralph Progress Log
Started: 2026-02-01

## Codebase Patterns
- Agent config is controlled by `BASELINE_MCP_TYPE` env var in `claude_baseline_agent.py`
- The 3-config comparison uses: `none` (Baseline), `sourcegraph_no_deepsearch` (MCP-NoDeepSearch), `sourcegraph_hybrid` (MCP-Full)
- MCP tool names follow pattern `mcp__sourcegraph__sg_<tool_name>`
- Source configs are at `~/evals/custom_agents/agents/claudecode/configs/`
- Run outputs are at `~/evals/custom_agents/agents/claudecode/runs/official/`
- result.json `agent_result.n_input_tokens` etc. are usually null — use `extract_task_tokens_from_transcript()` fallback for token/cost data
- Transcript `result` entry has `usage.{input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens}` and top-level `total_cost_usd`
- SWE-bench partial score: parse "Required tests:" and "Required tests that passed:" from test-stdout.txt
- ATIF v1.2 trajectory.json tool calls are at `steps[].tool_calls[].function_name`; claude-code.txt tool calls are `assistant` entries with `tool_use` content blocks having `name` field
- MCP tools follow naming `mcp__<server>__<tool>` (e.g., `mcp__sourcegraph__sg_keyword_search`); local tools are Bash, Read, Edit, Write, etc.
- Trajectory tool counts < transcript tool counts because trajectory excludes sub-agent (Task) tool calls
- Harbor run output structure: `runs/<category>/<run_name>/<config>/<batch_timestamp>/<task_id>__<hash>/`
- Each `harbor run --path` invocation creates a separate batch timestamp dir (one task per batch for BigCode)
- This repo has no build/lint/test CI — quality checks are manual review of created files
- Batch-level config.json has `agents` (list); task-level has `agent` (dict) — always check both
- claude-code.txt init line (`type: 'system', subtype: 'init'`) contains tools, mcp_servers, model, claude_code_version

---

## 2026-02-01 - US-001
- Created `docs/CONFIGS.md` documenting the 3 agent configurations with exact tool lists
- Files changed: `docs/CONFIGS.md` (new)
- **Learnings for future iterations:**
  - The agent source code at `claude_baseline_agent.py` lines 97-480 contains all config logic
  - `sourcegraph_hybrid` system prompt claims "14 tools" but only 13 distinct tool names are registered in the allowed_tools list (lines 458-472)
  - `sourcegraph_no_deepsearch` uses the same MCP endpoint as `sourcegraph_hybrid` but blocks `sg_deepsearch` and `sg_deepsearch_read` via `--disallowedTools`
  - Hybrid modes (`sourcegraph_hybrid`, `sourcegraph_no_deepsearch`, `deepsearch_hybrid`, `none`) do NOT restrict local tools
  - Non-hybrid modes (`sourcegraph`, `deepsearch`) block local search tools (Grep, Glob, grep, rg, etc.)
- Shell runners use `BASELINE_MCP_TYPE=<mode> harbor run` pattern with `--jobs-dir` to separate output per config
- The original LoCoBench comparison used `deepsearch_hybrid` but the 3-config uses `sourcegraph_no_deepsearch` and `sourcegraph_hybrid`
---

## 2026-02-01 - US-002
- Created `configs/locobench_3config.yaml` with 3 MCP modes: baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid
- Created `configs/locobench_3config.sh` shell runner with per-mode flags (--baseline-only, --no-deepsearch-only, --full-only)
- Files changed: `configs/locobench_3config.yaml` (new), `configs/locobench_3config.sh` (new)
- **Learnings for future iterations:**
  - Original YAML has `deepsearch_hybrid` as the MCP mode; the 3-config replaces this with `sourcegraph_no_deepsearch` + `sourcegraph_hybrid`
  - The YAML has a note that LoCoBench is local and not in Harbor registry — the shell script is the actual execution mechanism
  - Original shell script had 50 task IDs in a flat array (no category comments matching YAML's structure)
  - Shell runners set `CATEGORY` defaulting to `experiment` in original; changed to `official` for 3-config
  - Model is already pinned to `anthropic/claude-opus-4-5-20251101` in the original
---

## 2026-02-01 - US-003
- Created `configs/swebenchpro_3config.yaml` with 3 MCP modes: baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid
- Created `configs/swebenchpro_3config.sh` shell runner with per-mode flags (--baseline-only, --no-deepsearch-only, --full-only)
- Files changed: `configs/swebenchpro_3config.yaml` (new), `configs/swebenchpro_3config.sh` (new)
- **Learnings for future iterations:**
  - SWE-bench Pro uses `--dataset swebenchpro` with `-t <task_id>` args (Harbor registry tasks), unlike LoCoBench which uses `--path` with local dirs
  - Original YAML listed "Flipt (10 tasks)" in comments but actually has 12 Flipt task IDs
  - Original shell runner used `deepsearch` as MCP mode name; 3-config replaces with `sourcegraph_no_deepsearch` + `sourcegraph_hybrid`
  - Original shell runner had `CATEGORY=experiment`; 3-config uses `official`
  - SWE-bench Pro uses 7200s (2hr) timeout vs LoCoBench's 3600s (1hr) due to heavier test suites
---

## 2026-02-01 - US-004
- Created `configs/bigcode_3config.yaml` with 3 MCP modes: baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid
- Created `configs/bigcode_3config.sh` shell runner with per-mode flags (--baseline-only, --no-deepsearch-only, --full-only)
- Set concurrency to 1 (serial execution) and run_category to official
- Files changed: `configs/bigcode_3config.yaml` (new), `configs/bigcode_3config.sh` (new)
- **Learnings for future iterations:**
  - BigCode MCP is a local benchmark (not in Harbor registry), so YAML is reference only — shell script is the execution mechanism
  - Original had only 2 modes (baseline, sourcegraph_hybrid); 3-config adds sourcegraph_no_deepsearch
  - BigCode shell runner uses `declare -A TASK_SG_REPO_NAMES` to map task IDs to Sourcegraph repo names (e.g., `sg-benchmarks/kubernetes--latest`)
  - SOURCEGRAPH_REPO_NAME env var must be set per-task so the agent searches the correct repo
  - BigCode uses TIMEOUT_MULTIPLIER=10 (10x default) due to large codebases
  - Original CATEGORY defaulted to `experiment`; 3-config uses `official`
---

## 2026-02-01 - US-005
- Created `configs/k8s_docs_3config.yaml` with 3 MCP modes: baseline, sourcegraph_no_deepsearch, sourcegraph_hybrid
- Created `configs/k8s_docs_3config.sh` shell runner with per-mode flags (--baseline-only, --no-deepsearch-only, --full-only)
- Set timeout to 900s per task (matching task.toml time_limit_sec), TIMEOUT_MULTIPLIER=3
- All 5 task IDs included: pkg-doc-001, client-go-doc-001, applyconfig-doc-001, apiserver-doc-001, fairqueuing-doc-001
- Files changed: `configs/k8s_docs_3config.yaml` (new), `configs/k8s_docs_3config.sh` (new)
- **Learnings for future iterations:**
  - K8s Docs is a local benchmark like LoCoBench, uses `--path` not `--dataset`
  - task.toml has `time_limit_sec = 900` (15 min) — much shorter than LoCoBench (3600s) or SWE-bench (7200s)
  - K8s Docs tasks include an MCP config setup script in task.toml `environment.setup_scripts.mcp_config` — this is separate from the Harbor agent-level MCP setup
  - All tasks are Go language, category is "package-documentation"
  - TIMEOUT_MULTIPLIER=3 provides adequate headroom for 900s tasks (compared to LoCoBench's 10x for 3600s)
---

## 2026-02-01 - US-006
- Diagnosed BigCode MCP "empty results" issue in `runs/official/bigcode_mcp_opus_20260131_130446/`
- Created `docs/BIGCODE_DIAGNOSIS.md` with full analysis
- Files changed: `docs/BIGCODE_DIAGNOSIS.md` (new)
- **Findings:**
  - Task directories DO exist — 15 tasks completed across 3 configs, all with reward=1.0
  - The "0 task dirs" observation was incorrect; task dirs are nested inside batch timestamp dirs
  - Original `bigcode_mcp_comparison.sh` only ran 2 configs (baseline + sourcegraph_hybrid), not 3
  - `deepsearch_hybrid` had only 1 of 4 tasks run (likely manual/abandoned)
  - `sourcegraph_no_deepsearch` was never run (not in original 2-config script)
  - Multiple invocations to same `--jobs-dir` created duplicate task runs
  - One AgentTimeoutError (servo, sourcegraph_hybrid, 6000s) but task still scored 1.0
- **Fix:** No config changes needed — use `bigcode_3config.sh` (US-004) for a clean re-run with all 3 configs
- **Learnings for future iterations:**
  - Harbor creates one batch timestamp dir per `harbor run` invocation, not one per task
  - BigCode tasks are run individually (one `harbor run` per task), so each task gets its own batch dir
  - Reusing `--jobs-dir` across invocations accumulates results; use fresh timestamps for clean runs
  - Even timed-out tasks can receive rewards if the verifier finds passing tests
---

## 2026-02-01 - US-007
- Created `scripts/ccb_metrics/__init__.py` and `scripts/ccb_metrics/models.py`
- Defined `TaskMetrics` dataclass with 23 fields covering scoring, timing, tokens, cost, tool usage, and code changes
- Defined `RunMetrics` dataclass with computed properties: mean_reward, mean_partial_score, pass_rate, mean_tokens, mean_wall_clock, mean_mcp_ratio
- Defined `EvalReport` dataclass with configs(), benchmarks(), and to_json() methods
- All dataclasses have to_dict() and from_dict() class methods for JSON serialization
- Files changed: `scripts/ccb_metrics/__init__.py` (new), `scripts/ccb_metrics/models.py` (new)
- **Learnings for future iterations:**
  - The package uses stdlib only (dataclasses, json, pathlib, typing, datetime, statistics) — no external deps
  - `from_dict()` filters to known fields so extra keys in serialized JSON are safely ignored
  - RunMetrics computed properties are included in to_dict() output but stripped on from_dict() input
  - `_safe_mean()` helper handles None values gracefully — returns None if all values are None
  - Python 3.10+ is required (uses `dict[str, int]` and `list[TaskMetrics]` type hints with `from __future__ import annotations`)
---

## 2026-02-01 - US-008
- Created `scripts/ccb_metrics/extractors.py` with 4 extraction functions
- `extract_task_from_result_json()` — reads Harbor result.json, populates TaskMetrics with reward, status, timing fields, and token/cost data (when available)
- `extract_task_tokens_from_transcript()` — fallback parser for claude-code.txt JSONL, extracts input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens, total_cost_usd from last `result` type entry
- `extract_swebench_partial_score()` — parses verifier/test-stdout.txt for "Required tests: N" and "Required tests that passed: M", returns M/N
- `extract_reward_from_file()` — reads verifier/reward.txt as a float fallback
- All functions handle missing/malformed files gracefully (return None for missing fields)
- Tested on real data: LoCoBench baseline task (reward=0.5462, timing fields non-None), transcript fallback (tokens non-None), SWE-bench partial score (0.0), reward.txt (0.0)
- Files changed: `scripts/ccb_metrics/extractors.py` (new)
- **Learnings for future iterations:**
  - result.json `agent_result.n_input_tokens` etc. are often null — transcript fallback is essential for token/cost data
  - The transcript's `result` entry has `usage` dict with `input_tokens`, `output_tokens`, `cache_creation_input_tokens`, `cache_read_input_tokens` and top-level `total_cost_usd`
  - The transcript also has `modelUsage` dict keyed by model name, useful for per-model cost breakdown
  - SWE-bench test-stdout.txt uses "Required tests:" and "Required tests that passed:" (not "Passed tests:" for the partial score)
  - result.json timing sections: `environment_setup`, `agent_setup`, `agent_execution`, `verifier` each have `started_at`/`finished_at`
  - result.json `agent_setup` is separate from `agent_execution` — the wall_clock covers everything from top-level `started_at` to `finished_at`
---

## 2026-02-01 - US-009
- Added `extract_tool_usage_from_trajectory()` and `extract_tool_usage_from_transcript()` to `scripts/ccb_metrics/extractors.py`
- `extract_tool_usage_from_trajectory()` — parses ATIF v1.2 trajectory.json `steps[].tool_calls[].function_name`, counts each tool, categorizes as MCP vs local
- `extract_tool_usage_from_transcript()` — fallback parser for claude-code.txt JSONL, finds `type='assistant'` entries with `tool_use` content blocks
- Both return dict with: tool_calls_total, tool_calls_mcp, tool_calls_local, tool_calls_by_name, mcp_ratio
- Shared helper `_build_tool_usage_dict()` handles categorization and ratio calculation
- MCP tools identified by `mcp__` prefix; local tools matched against known set (Bash, Read, Edit, Write, Grep, Glob, Task, etc.)
- Tested on real data: baseline trajectory (35 calls, 0 MCP), deepsearch trajectory (28 calls, 2 MCP), baseline transcript (61 calls, 0 MCP), missing file (all None)
- Files changed: `scripts/ccb_metrics/extractors.py` (modified)
- **Learnings for future iterations:**
  - ATIF v1.2 trajectory.json `steps[].tool_calls[].function_name` is the canonical tool name — same as claude-code.txt `tool_use.name`
  - Trajectory counts differ from transcript counts because trajectory only includes top-level agent steps, while transcript includes sub-agent tool calls too (e.g., Task agent's Bash calls appear in transcript but not parent trajectory)
  - MCP tool naming convention: `mcp__<server>__<tool>` (e.g., `mcp__deepsearch__deepsearch`, `mcp__sourcegraph__sg_keyword_search`)
  - `TodoWrite` is a local tool (built into Claude Code), not MCP
- Task dir names are truncated: `{task_id_prefix}__{random_hash}` — the hash is the last `__`-separated segment
- Batch dir format: `YYYY-MM-DD__HH-MM-SS`; BigCode has multiple batches per config (one per task), LoCoBench/SWE-bench have one batch per config
- Deduplication needed: same task_id can appear in multiple batches within one config
---

## 2026-02-01 - US-010
- Created `scripts/ccb_metrics/discovery.py` with `discover_runs()` function
- Walks `runs_dir/<run_name>/<config_name>/<batch_timestamp>/<task_id__hash>/`
- Infers benchmark from run_name (locobench, swebenchpro, bigcode, k8s_docs)
- For each task dir, calls `extract_task_from_result_json()` + token fallback + tool usage extraction
- Falls back to `extract_reward_from_file()` and `extract_task_tokens_from_transcript()` when result.json lacks data
- For SWE-bench tasks, also calls `extract_swebench_partial_score()`
- Deduplicates tasks across multiple batches (keeps latest)
- Groups tasks into `RunMetrics` by (benchmark, config_name), sorted output
- Extracts model from batch-level `config.json`
- Updated `__init__.py` to export `discover_runs`
- Files changed: `scripts/ccb_metrics/discovery.py` (new), `scripts/ccb_metrics/__init__.py` (modified)
- **Tested on real data:**
  - locobench baseline: 50 tasks, mean_reward=0.498
  - locobench deepsearch: 50 tasks, mean_reward=0.499
  - swebenchpro baseline: 48 tasks, partial_score=0.196
  - swebenchpro deepsearch: 48 tasks, partial_score=0.196
  - bigcode baseline: 4 tasks, reward=1.0
  - bigcode sourcegraph_hybrid: 4 tasks, reward=1.0
  - bigcode deepsearch_hybrid: 1 task, reward=1.0
- **Learnings for future iterations:**
  - Task directory names are truncated for filesystem limits; the hash suffix is after the last `__`
  - BigCode runs have one batch timestamp per `harbor run` invocation (one task each), while LoCoBench/SWE-bench have one batch with all tasks
  - result.json `task_name` field contains the full task ID (not truncated like the directory name)
  - SWE-bench tasks all have reward=0.0 but partial_score varies (0.0 to 0.9) — partial_score is essential for meaningful comparison
  - Trajectory files may not exist in all runs (missing in BigCode); transcript fallback is critical for tool usage
---

## 2026-02-01 - US-011
- Created `scripts/generate_eval_report.py` — CLI entry point for deterministic evaluation report generation
- Accepts `--runs-dir`, `--output-dir`, `--csv`/`--no-csv` arguments; `--help` prints full usage
- Calls `discover_runs()` and wraps results in `EvalReport`, writes `eval_report.json`
- Generates `REPORT.md` with 6 tables: Run Inventory, Aggregate Performance, Per-Benchmark Breakdown, Efficiency, Tool Utilization, SWE-Bench Pro Partial Scores
- Writes CSV files (one per table) for downstream analysis
- Prints summary to stdout: benchmarks, configs, total tasks, pass rates per config
- Tested on real data: discovered 3 benchmarks, 4 configs, 205 tasks, 7 runs
- Files changed: `scripts/generate_eval_report.py` (new)
- **Learnings for future iterations:**
  - The `eval_reports/` directory is generated output — consider adding to .gitignore
  - SWE-bench partial scores table is conditional — only generated when SWE-bench data exists
  - deepsearch_hybrid and sourcegraph_hybrid have very few tasks (1 and 4) in current data — aggregates are unreliable for these
  - The `_safe_mean()` helper in models.py is also needed in the report generator — duplicated as local helper to avoid modifying models.py
---

## 2026-02-01 - US-012
- Created `scripts/ccb_metrics/judge_context.py` with `generate_judge_contexts()` function and CLI entry point
- Created `scripts/__init__.py` to enable `python3 -m scripts.ccb_metrics.judge_context` invocation
- For each task in each run, generates a JSON file at `output_dir/<benchmark>/<config>/<task_id>_judge_context.json`
- Each context file contains: task_id, benchmark, config_name, model, reward, partial_score, task_instructions (from benchmark instruction.md), agent_transcript_summary (first 200 + last 100 lines), agent_output (solution.md or last assistant message), ground_truth, tool_usage_summary (with top 5 tools), code_changes (from Edit/Write tool calls), verifier_output, run_metadata
- Generates `judge_contexts_index.json` listing all generated context files
- CLI: `python3 -m scripts.ccb_metrics.judge_context --runs-dir <path> --benchmarks-dir ./benchmarks/ --output-dir ./judge_contexts/`
- Also importable: `from scripts.ccb_metrics.judge_context import generate_judge_contexts`
- Handles missing files gracefully — fields are null when source data unavailable
- Deduplicates tasks across multiple batches (keeps latest)
- Files changed: `scripts/ccb_metrics/judge_context.py` (new), `scripts/__init__.py` (new)
- **Tested on real data:** 205 judge context files generated across 3 benchmarks and 4 configs
- **Learnings for future iterations:**
  - LoCoBench task_ids map directly to `benchmarks/locobench_agent/tasks/<task_id>/`
  - BigCode task_ids map directly to `benchmarks/big_code_mcp/<task_id>/`
  - K8s Docs task_ids map to `benchmarks/kubernetes_docs/<task_id>/`
  - SWE-bench Pro has no local instruction.md — instructions come from Harbor dataset
  - LoCoBench "ground truth" is in `solution/` subdirectory (not `ground_truth/`)
  - BigCode also uses `solution/` for ground truth
  - K8s Docs uses `ground_truth/` for ground truth
  - Agent output is in `agent/solution.md` when available, else last assistant text from transcript
  - `scripts/__init__.py` is needed for `python3 -m scripts.ccb_metrics.judge_context` to work
---

## 2026-02-01 - US-013
- Added `extract_run_config(batch_dir, transcript_path)` to `scripts/ccb_metrics/extractors.py`
- Reads batch-level `config.json` for model_name, agent_import_path, timeout_multiplier, task_source
- Parses claude-code.txt system init line for claude_code_version, permissionMode, tools list, mcp_servers, model
- Infers mcp_mode from mcp_servers names (none/deepsearch/sourcegraph_no_deepsearch/sourcegraph_hybrid)
- Added `harness_config` optional dict field to `RunMetrics` in `models.py`
- Updated `discovery.py` to extract harness config from each batch dir + first task's transcript
- Updated `generate_eval_report.py`: Run Inventory table now includes MCP Mode column; writes `harness_configs.json` to output dir
- Updated `__init__.py` to export `extract_run_config`
- Files changed: `scripts/ccb_metrics/extractors.py` (modified), `scripts/ccb_metrics/models.py` (modified), `scripts/ccb_metrics/discovery.py` (modified), `scripts/ccb_metrics/__init__.py` (modified), `scripts/generate_eval_report.py` (modified)
- **Learnings for future iterations:**
  - Batch-level config.json has `agents` (list), task-level config.json has `agent` (dict) — handle both
  - Batch-level config.json also lacks `task` key; it uses `tasks` (list) when present
  - claude-code.txt init line is `{type: 'system', subtype: 'init', ...}` — always the first JSONL entry
  - mcp_servers is a list of `{name, status}` dicts; server name determines the MCP mode
  - `sourcegraph_hybrid` in Harbor config dir name actually had only `sourcegraph` server (no deepsearch), so mcp_mode resolves to `sourcegraph_no_deepsearch`
  - `deepsearch_hybrid` in Harbor config dir name had `deepsearch` server only
---

## 2026-02-01 - US-014
- Updated `README.md` with: project description referencing the paper, benchmark suite table (5 suites with task counts, languages, evaluation methods, SDLC phases), directory structure overview, 3-config evaluation matrix section (referencing docs/CONFIGS.md), metrics extraction pipeline section, Running with Harbor section
- Verified no credentials or .env files tracked (.gitignore covers .env, .key, .pem, .claude/)
- Verified all benchmark directories have task files committed (10,729 files across 13 benchmark dirs)
- Pushed `ralph/benchmark-execution-pipeline` branch to origin (sjarmak/CodeContextBench)
- Files changed: `README.md` (modified)
- **Learnings for future iterations:**
  - configs/ and scripts/ created by earlier stories live under `ralph/` subdirectory, not repo root — README paths must reflect this
  - `benchmarks-only` branch has only initial benchmark definitions; the current working branch has all US-001 through US-014 work
  - SWE-bench Pro has 731 tasks in `tasks/` dir but only 50 are selected for the 3-config comparison
  - github_mined has 25 task directories (sgt-001 through sgt-025), not 30 as previously listed in old README
  - big_code_mcp has 4 task directories, not 12 as previously listed
---
