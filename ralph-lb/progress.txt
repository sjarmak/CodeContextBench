## Codebase Patterns

### Directory Structure
- Benchmark tasks: `benchmarks/ccb_{benchmark}/{task_name}/` containing task.toml, instruction.md, environment/, tests/test.sh
- Run results: `runs/official/{prefix}_{config}_{timestamp}/config/{batch_ts}/{task_name}__{hash}/result.json`
- Scripts: `scripts/` — all Python, use pathlib, PROJECT_ROOT = Path(__file__).resolve().parent.parent
- Docs: `docs/` — markdown files
- Schemas: `schemas/` — contains result.schema.json
- Selected tasks registry: `configs/selected_benchmark_tasks.json`

### Reward Output Contract
- ALL test.sh write to `/logs/verifier/reward.txt` as plain decimal float (0.0-1.0)
- EXCEPTION: RepoQA currently writes only `/logs/verifier/reward.json` — needs fix (US-003)
- All test.sh exit 0 always (Harbor interprets reward value, not exit code)
- Harbor reads reward.txt, stores in result.json as `verifier_result.rewards.reward` (or `.score` for some older runs)

### Result.json Structure (Harbor output)
- `task_name`: string
- `verifier_result.rewards.reward`: float 0.0-1.0 (CANONICAL — some old use `.score`)
- `exception_info`: null or object
- `agent_info.name`: string (e.g., "claude-code")
- `agent_info.model_info.name`: string (e.g., "anthropic/claude-opus-4-5-20251101")
- `agent_result.n_input_tokens`: int
- `agent_result.n_output_tokens`: int
- `started_at`, `finished_at`: ISO 8601 timestamps

### Manifest Generation (generate_manifest.py)
- Scans runs/official/ with DIR_PREFIX_TO_SUITE mapping
- Skips: __broken_verifier, validation_test, archive
- Latest batch wins for duplicate task names (sorted order)
- Reward extraction line 116: `rewards.get("reward") if rewards.get("reward") is not None else rewards.get("score")`
- Status: errored (exception), passed (reward > 0), failed (else)

### task.toml Sections (INCONSISTENT — needs normalization)
- Standard: `[verification]` with type, command
- CrossRepo/DIBench/LoCoBench: `[verifier]` with timeout_sec, command
- DependEval: `[evaluation]` with output_file, reward_file
- ALL should get `[verification]` section with reward_type + description (US-005)
- Do NOT remove existing [verifier] sections — Harbor reads them for timeouts

### Benchmark Reward Types
- ccb_swebenchpro: test_ratio (% tests passing)
- ccb_pytorch: diff_similarity (diff match to expected)
- ccb_locobench: semantic_similarity (embedding distance)
- ccb_k8sdocs: checklist (weighted file/pattern checks)
- ccb_largerepo: checklist (weighted structure checks)
- ccb_codereview: checklist (F1 detection + fix score)
- ccb_linuxflbench: checklist (10-point rubric)
- ccb_tac: checklist (task-specific eval.py)
- ccb_crossrepo: semantic_similarity (content + file refs + patterns)
- ccb_dibench: binary (dependency install pass/fail)
- ccb_repoqa: semantic_similarity (correct function retrieval)
- ccb_sweperf: test_ratio (performance test pass)
- ccb_dependeval: binary (dependency ordering correctness)

### Script Conventions
- Use argparse for CLI args
- Use pathlib for file operations
- PROJECT_ROOT pattern: `Path(__file__).resolve().parent.parent`
- RUNS_DIR = PROJECT_ROOT / "runs" / "official"
- Print errors to stderr: `print(..., file=sys.stderr)`
- Exit 0 on success, exit 1 on error

### File Counts
- ~893 task.toml files across 13 benchmarks
- ~900 test.sh files across 13 benchmarks
- Most benchmarks have many tasks with identical test.sh structure (templated)

## Progress

## 2026-02-06 - US-001
- Implemented JSON Schema (draft 2020-12) for result.json at schemas/result.schema.json
- Files changed: schemas/result.schema.json (new)
- **Learnings for future iterations:**
  - jsonschema 3.2.0 is already installed in the system Python (no pip install needed)
  - Real result.json files have `additionalProperties` beyond the core fields (id, trial_name, task_id, etc.) — schema must use `additionalProperties: true`
  - Token fields (n_input_tokens, n_output_tokens) are often null due to Harbor token logging bug — schema uses `["integer", "null"]` union types
  - agent_info and agent_result can be null — schema uses `["object", "null"]` union types
  - 11 real result.json files validated against schema with 0 errors
---

## 2026-02-06 - US-002
- Created scripts/validate_submission.py — validates submission directories against result.schema.json
- Files changed: scripts/validate_submission.py (new)
- **Learnings for future iterations:**
  - jsonschema 3.2.0 uses Draft7Validator (not Draft202012Validator) — works fine for our schema
  - Batch-level result.json files (at config/batch_ts/result.json, not inside task__hash dirs) exist and fail schema validation — this is expected since they lack task_name, verifier_result, etc.
  - For "required" validation errors, jsonschema puts the missing field name in error.message as `'field' is a required property` — extract with split("'")[1]
  - The absolute_path on "required" errors gives the parent path, not the missing field itself — must combine parent path + extracted field name for full path like `verifier_result.rewards.reward`
---
