## Codebase Patterns

### Directory Structure
- Benchmark tasks: `benchmarks/ccb_{benchmark}/{task_name}/` containing task.toml, instruction.md, environment/, tests/test.sh
- Run results: `runs/official/{prefix}_{config}_{timestamp}/config/{batch_ts}/{task_name}__{hash}/result.json`
- Scripts: `scripts/` — all Python, use pathlib, PROJECT_ROOT = Path(__file__).resolve().parent.parent
- Docs: `docs/` — markdown files
- Schemas: `schemas/` — contains result.schema.json
- Selected tasks registry: `configs/selected_benchmark_tasks.json`

### Reward Output Contract
- ALL test.sh write to `/logs/verifier/reward.txt` as plain decimal float (0.0-1.0)
- EXCEPTION: RepoQA currently writes only `/logs/verifier/reward.json` — needs fix (US-003)
- All test.sh exit 0 always (Harbor interprets reward value, not exit code)
- Harbor reads reward.txt, stores in result.json as `verifier_result.rewards.reward` (or `.score` for some older runs)

### Result.json Structure (Harbor output)
- `task_name`: string
- `verifier_result.rewards.reward`: float 0.0-1.0 (CANONICAL — some old use `.score`)
- `exception_info`: null or object
- `agent_info.name`: string (e.g., "claude-code")
- `agent_info.model_info.name`: string (e.g., "anthropic/claude-opus-4-5-20251101")
- `agent_result.n_input_tokens`: int
- `agent_result.n_output_tokens`: int
- `started_at`, `finished_at`: ISO 8601 timestamps

### Manifest Generation (generate_manifest.py)
- Scans runs/official/ with DIR_PREFIX_TO_SUITE mapping
- Skips: __broken_verifier, validation_test, archive
- Latest batch wins for duplicate task names (sorted order)
- Reward extraction line 116: `rewards.get("reward") if rewards.get("reward") is not None else rewards.get("score")`
- Status: errored (exception), passed (reward > 0), failed (else)

### task.toml Sections (INCONSISTENT — needs normalization)
- Standard: `[verification]` with type, command
- CrossRepo/DIBench/LoCoBench: `[verifier]` with timeout_sec, command
- DependEval: `[evaluation]` with output_file, reward_file
- ALL now have `[verification]` section with reward_type + description (US-005 DONE)
- Do NOT remove existing [verifier] sections — Harbor reads them for timeouts
- Total non-template task.toml count: 890 (not 893 as sometimes cited)

### Benchmark Reward Types
- ccb_swebenchpro: test_ratio (% tests passing)
- ccb_pytorch: diff_similarity (diff match to expected)
- ccb_locobench: semantic_similarity (embedding distance)
- ccb_k8sdocs: checklist (weighted file/pattern checks)
- ccb_largerepo: checklist (weighted structure checks)
- ccb_codereview: checklist (F1 detection + fix score)
- ccb_linuxflbench: checklist (10-point rubric)
- ccb_tac: checklist (task-specific eval.py)
- ccb_crossrepo: semantic_similarity (content + file refs + patterns)
- ccb_dibench: binary (dependency install pass/fail)
- ccb_repoqa: semantic_similarity (correct function retrieval)
- ccb_sweperf: test_ratio (performance test pass)
- ccb_dependeval: binary (dependency ordering correctness)

### Script Conventions
- Use argparse for CLI args
- Use pathlib for file operations
- PROJECT_ROOT pattern: `Path(__file__).resolve().parent.parent`
- RUNS_DIR = PROJECT_ROOT / "runs" / "official"
- Print errors to stderr: `print(..., file=sys.stderr)`
- Exit 0 on success, exit 1 on error

### File Counts
- ~893 task.toml files across 13 benchmarks
- ~900 test.sh files across 13 benchmarks
- Most benchmarks have many tasks with identical test.sh structure (templated)

## Progress

## 2026-02-06 - US-001
- Implemented JSON Schema (draft 2020-12) for result.json at schemas/result.schema.json
- Files changed: schemas/result.schema.json (new)
- **Learnings for future iterations:**
  - jsonschema 3.2.0 is already installed in the system Python (no pip install needed)
  - Real result.json files have `additionalProperties` beyond the core fields (id, trial_name, task_id, etc.) — schema must use `additionalProperties: true`
  - Token fields (n_input_tokens, n_output_tokens) are often null due to Harbor token logging bug — schema uses `["integer", "null"]` union types
  - agent_info and agent_result can be null — schema uses `["object", "null"]` union types
  - 11 real result.json files validated against schema with 0 errors
---

## 2026-02-06 - US-002
- Created scripts/validate_submission.py — validates submission directories against result.schema.json
- Files changed: scripts/validate_submission.py (new)
- **Learnings for future iterations:**
  - jsonschema 3.2.0 uses Draft7Validator (not Draft202012Validator) — works fine for our schema
  - Batch-level result.json files (at config/batch_ts/result.json, not inside task__hash dirs) exist and fail schema validation — this is expected since they lack task_name, verifier_result, etc.
  - For "required" validation errors, jsonschema puts the missing field name in error.message as `'field' is a required property` — extract with split("'")[1]
  - The absolute_path on "required" errors gives the parent path, not the missing field itself — must combine parent path + extracted field name for full path like `verifier_result.rewards.reward`
---

## 2026-02-06 - US-003
- Fixed all 10 RepoQA test.sh files + template to write /logs/verifier/reward.txt alongside reward.json
- Added comment header: `# Reward: semantic_similarity (0.0-1.0) — correct function retrieval score`
- reward.txt written in all 3 code paths: missing ground_truth.json, missing solution.json, and Python verifier (success + exception)
- Files changed: benchmarks/ccb_repoqa/templates/tests/test.sh, all 10 benchmarks/ccb_repoqa/tasks/*/tests/test.sh
- **Learnings for future iterations:**
  - All 10 RepoQA task test.sh files are identical copies of templates/tests/test.sh with `{task_variant}` replaced by `sr-qa`
  - Most efficient edit: update template then `sed 's/{task_variant}/sr-qa/g'` to regenerate all 10 task files
  - reward.txt writes must happen in BOTH bash error paths AND inside the embedded Python heredoc (3 total locations in the Python: success path, exception path, and 2 bash early-exit paths)
  - Backward compatibility: keep reward.json writes intact, just add reward.txt alongside
---

## 2026-02-06 - US-004
- Added `# Reward: {reward_type} (0.0-1.0) — {description}` comment header to all 879 test.sh files (10 RepoQA already had headers from US-003)
- Files changed: 879 test.sh files across 13 benchmarks (ccb_codereview, ccb_crossrepo, ccb_dependeval, ccb_dibench, ccb_k8sdocs, ccb_largerepo, ccb_linuxflbench, ccb_locobench, ccb_pytorch, ccb_swebenchpro, ccb_sweperf, ccb_tac)
- **Learnings for future iterations:**
  - All test.sh files use `#!/bin/bash` shebang (no `#!/usr/bin/env bash` found)
  - Python glob.glob with `recursive=True` and `**` is needed for benchmarks with `tasks/` subdirectory nesting (locobench, repoqa, swebenchpro, sweperf)
  - The AC glob `benchmarks/ccb_*/*/tests/test.sh` doesn't match nested `tasks/*/tests/test.sh` paths — recursive find is required for accurate counts
  - Template files (in `templates/` dirs) should be skipped — they're not actual task test files
  - Total non-template test.sh count is 889 (879 modified + 10 already had headers)
---

## 2026-02-06 - US-005
- Added `[verification]` section with `reward_type` and `description` to all 890 non-template task.toml files across 13 benchmarks
- For 6 benchmarks with existing `[verification]` (PyTorch, CodeReview, K8s Docs, LargeRepo, LinuxFLBench, TAC): added reward_type + description fields to existing section
- For 6 benchmarks with `[verifier]` (SWE-bench Pro, LoCoBench, RepoQA, DIBench, CrossRepo, SWE-Perf): appended new `[verification]` section, preserved existing `[verifier]` for Harbor
- For DependEval with `[evaluation]`: appended new `[verification]` section, preserved existing `[evaluation]`
- Fixed K8s Docs verification command: `/workspace/tests/test.sh` → `/tests/test.sh` in all 5 task.toml files
- Created scripts/add_verification_metadata.py for reproducible migration
- Files changed: 890 task.toml files, scripts/add_verification_metadata.py (new)
- **Learnings for future iterations:**
  - 890 non-template task.toml files (not 893 as noted in PRD). Some benchmarks have template dirs not caught by `/templates/` filter — e.g. `ccb_swebenchpro/template/` (singular)
  - Python `toml` library is installed in system Python — use it to validate TOML parse correctness
  - When appending `[verification]` to files with `[verifier]`, insert BEFORE `[environment]` for consistent ordering
  - Template files with TOML placeholders like `{difficulty}` cause parse errors — filter them out from validation
  - The `sync_task_metadata.py` only checks `task.language` and `task.difficulty` — adding new sections doesn't affect it
  - Pre-existing mismatch: sgt-008/sgt-025 have `difficulty='critical'` in task.toml but `'hard'` in selected_benchmark_tasks.json
---

## 2026-02-06 - US-006
- Created docs/AGENT_INTERFACE.md documenting the agent interface specification
- Documents: workspace paths (/workspace, /app, /testbed), instruction.md delivery, time limits
- Documents output: agent modifies workspace files, verifier runs test.sh, reward.txt written
- Documents all 3 MCP configs: baseline (no MCP), SG_base (keyword+NLS, no Deep Search), SG_full (+Deep Search)
- Documents environment variables: TASK_NAME, TIME_LIMIT_SEC
- Includes minimal 5-line bash agent example
- Documents constraints: network access, resource limits (build_timeout_sec), Docker container environment
- Files changed: docs/AGENT_INTERFACE.md (new)
- **Learnings for future iterations:**
  - The actual agent code is at `~/evals/custom_agents/agents/claudecode/agents/claude_baseline_agent.py` — NOT in the CodeContextBench repo
  - BASELINE_MCP_TYPE env var controls config: "none", "sourcegraph_base", "sourcegraph_full" (also "sourcegraph", "deepsearch", "deepsearch_hybrid" deprecated)
  - Workspace detection priority: /workspace > /app > /testbed > / (runtime detection in both setup() and create_run_agent_commands)
  - For SG_base, Deep Search tools are blocked via `--disallowedTools` CLI flag, not via system prompt
  - MCP config is uploaded to `/logs/agent/sessions/.mcp.json` (CLAUDE_CONFIG_DIR) in the container
  - The system prompt (EVALUATION_CONTEXT_PROMPT) is delivered via `--append-system-prompt` as a base64-encoded file to avoid ARG_MAX limits
---

## 2026-02-06 - US-007
- Created docs/LEADERBOARD.md documenting leaderboard scoring specification
- Documents primary metric: mean_reward per benchmark
- Documents aggregate scoring: unweighted macro-average of per-benchmark mean_rewards (equal benchmark weight)
- Documents error handling: errored tasks count as reward=0.0 (never excluded)
- Documents per-benchmark completeness: must run ALL tasks to qualify
- Documents partial submissions: appear only on qualifying per-benchmark views, not aggregate
- Documents tie-breaking: pass_rate > median_reward > token_efficiency
- Includes "Interpreting Scores" section with reward_type explanations and what 0.8 means for each type
- Includes worked calculation example using realistic scores
- Files changed: docs/LEADERBOARD.md (new)
- **Learnings for future iterations:**
  - MANIFEST.json groups runs by "suite/config" key (e.g., "ccb_pytorch/baseline") with per-run stats
  - 13 benchmarks with 156 total tasks in selected_benchmark_tasks.json
  - Current MANIFEST has 201 tasks across 25 runs (incomplete coverage — many benchmarks missing SG_full)
  - Binary benchmarks (DIBench, DependEval) only produce 0.0 or 1.0 — "what 0.8 means" is N/A for them
---

## 2026-02-06 - US-008
- Created docs/SUBMISSION.md documenting submission format for external researchers
- Documents directory structure: one subdir per task containing result.json + trajectory file
- Documents that trajectory logs are REQUIRED — submissions without them are rejected
- Documents trajectory format: JSON array of {role, content, tool_calls, timestamp} OR plain text transcript
- Includes example directory tree showing 6 tasks across 3 benchmarks
- References schemas/result.schema.json for result format and scripts/validate_submission.py for validation
- References scripts/package_submission.py for packaging (US-010, not yet implemented)
- Documents completeness requirements (links to LEADERBOARD.md)
- Files changed: docs/SUBMISSION.md (new)
- **Learnings for future iterations:**
  - The schema requires `verifier_result.rewards.reward` (not `.score`) — this is the most common validation error to document
  - Keep submission docs focused on external users — avoid leaking internal Harbor details
  - Cross-reference other docs (LEADERBOARD.md, AGENT_INTERFACE.md) rather than duplicating content
---
